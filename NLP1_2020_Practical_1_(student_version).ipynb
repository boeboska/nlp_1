{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-aRiOgl4nHg"
   },
   "source": [
    "------\n",
    "**You cannot apply any changes to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
    "\n",
    "------\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIZrAUx57vsM"
   },
   "source": [
    "Practical 1: Sentiment Detection in Movie Reviews\n",
    "========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4kXPMhyngZW"
   },
   "source": [
    "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
    "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
    "Each review is a **document** and consists of one or more sentences.\n",
    "\n",
    "To prepare yourself for this practical, you should\n",
    "have a look at a few of these texts to understand the difficulties of\n",
    "the task: how might one go about classifying the texts? You will write\n",
    "code that decides whether a movie review conveys positive or\n",
    "negative sentiment.\n",
    "\n",
    "Please make sure you have read the following paper:\n",
    "\n",
    ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
    "(2002). \n",
    "[Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
    "\n",
    "Bo Pang et al. were the \"inventors\" of the movie review sentiment\n",
    "classification task, and the above paper was one of the first papers on\n",
    "the topic. The first version of your sentiment classifier will do\n",
    "something similar to Pang et al.'s system. If you have questions about it,\n",
    "you should resolve as soon as possible with your TA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb7errgRASzZ"
   },
   "source": [
    "**Advice**\n",
    "\n",
    "Please read through the entire practical and familiarise\n",
    "yourself with all requirements before you start coding or otherwise\n",
    "solving the tasks. Writing clean and concise code can make the difference\n",
    "between solving the assignment in a matter of hours, and taking days to\n",
    "run all experiments.\n",
    "\n",
    "## Environment\n",
    "\n",
    "All code should be written in **Python 3**. \n",
    "This is the default in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "SaZnxptMJiD7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYZyIF7lJnGn"
   },
   "source": [
    "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
    "The easiest way to\n",
    "install Python is through downloading\n",
    "[Anaconda](https://www.anaconda.com/download). \n",
    "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
    "You can also use an IDE\n",
    "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
    "coding and debugging easier. It is good practice to create a [virtual\n",
    "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
    "project, so that any Python packages don’t interfere with other\n",
    "projects. \n",
    " \n",
    "\n",
    "**Learning Python 3**\n",
    "\n",
    "If you are new to Python 3, you may want to check out a few of these resources:\n",
    "- https://learnxinyminutes.com/docs/python3/\n",
    "- https://www.learnpython.org/\n",
    "- https://docs.python.org/3/tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "hok-BFu9lGoK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from subprocess import call\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sklearn as sk\n",
    "#from google.colab import drive\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "#import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWyGHwE-ieQ"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "**Download the sentiment lexicon and the movie reviews dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "lm-rakqtlMOT"
   },
   "outputs": [],
   "source": [
    "# download sentiment lexicon\n",
    "#!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
    "# download review data\n",
    "#!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkPwuHp5LSuQ"
   },
   "source": [
    "**Load the movie reviews.**\n",
    "\n",
    "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "careEKj-mRpl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 2000 \n",
      "\n",
      "0 NEG 29\n",
      "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
      "1 NEG 11\n",
      "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
      "2 NEG 24\n",
      "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
      "3 NEG 19\n",
      "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
      "4 NEG 38\n",
      "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
      "\n",
      "Number of word types: 47743\n",
      "Number of word tokens: 1512359\n",
      "\n",
      "Most common tokens:\n",
      "         , :    77842\n",
      "       the :    75948\n",
      "         . :    59027\n",
      "         a :    37583\n",
      "       and :    35235\n",
      "        of :    33864\n",
      "        to :    31601\n",
      "        is :    25972\n",
      "        in :    21563\n",
      "        's :    18043\n",
      "        it :    15904\n",
      "      that :    15820\n",
      "     -rrb- :    11768\n",
      "     -lrb- :    11670\n",
      "        as :    11312\n",
      "      with :    10739\n",
      "       for :     9816\n",
      "       his :     9542\n",
      "      this :     9497\n",
      "      film :     9404\n"
     ]
    }
   ],
   "source": [
    "# file structure:\n",
    "# [\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
    "#   ..\n",
    "# ]\n",
    "# where `content` is a list of sentences, \n",
    "# with a sentence being a list of (token, pos_tag) pairs.\n",
    "\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  reviews = json.load(f)\n",
    "  \n",
    "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
    "\n",
    "def print_sentence_with_pos(s):\n",
    "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
    "\n",
    "for i, r in enumerate(reviews):\n",
    "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
    "  print_sentence_with_pos(r[\"content\"][0])\n",
    "  if i == 4: \n",
    "    break\n",
    "    \n",
    "c = Counter()\n",
    "for review in reviews:\n",
    "  for sentence in review[\"content\"]:\n",
    "    for token, pos_tag in sentence:\n",
    "      c[token.lower()] += 1\n",
    "    \n",
    "#Aantal verschillende woorden     \n",
    "print(\"\\nNumber of word types:\", len(c))\n",
    "#Aantal woorden in totaal\n",
    "print(\"Number of word tokens:\", sum(c.values()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in c.most_common(20):\n",
    "  print(\"%10s : %8d\" % (token, count))\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6PWaEoh8B34"
   },
   "source": [
    "# Lexicon-based approach (2+1pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsTSMb6ma4E8"
   },
   "source": [
    "A traditional approach to automatically classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative) or a sentiment score (e.g., a score from 0 to 5).\n",
    "\n",
    "In this practical, you will use the sentiment\n",
    "lexicon released by Wilson et al. (2005). The path of the loaded lexicon is `\"sent_lexicon\"`.\n",
    "\n",
    "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
    "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
    "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
    "\n",
    "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCHUMmb0SZxy"
   },
   "source": [
    "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Ogq0Eq2hQglh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
     ]
    }
   ],
   "source": [
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  line_cnt = 0\n",
    "  for line in f:\n",
    "    print(line.strip())\n",
    "    line_cnt += 1\n",
    "    if line_cnt > 4:\n",
    "      break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mml4nOtIUBhn"
   },
   "source": [
    "Given such a sentiment lexicon, there are ways to solve\n",
    "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
    "$S_{binary}$ by counting how many words have a positive or a\n",
    "negative label in the sentiment lexicon $SLex$.\n",
    "\n",
    "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
    "\n",
    "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
    "\n",
    "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
    "\n",
    "$$\n",
    "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
    "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
    "        \\text{negative} & \\text{otherwise}\n",
    "        \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOFnMvbeeZrc"
   },
   "source": [
    "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Preprocess Sent_lex\n",
    "\n",
    "sent_lex = {}\n",
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  for line in f:\n",
    "    lijst = line.split('\\n')\n",
    "    for x in lijst:\n",
    "        lijstje = x.split(' ')\n",
    "        if lijstje[0] == '':\n",
    "            continue\n",
    "        word = lijstje[2].split('=')[1]\n",
    "        sent_lex[word] = x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ED2aTEYutW1-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  67.95 %\n"
     ]
    }
   ],
   "source": [
    "NoCorrect = 0\n",
    "classifications = []\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  reviews = json.load(f)\n",
    "\n",
    "\n",
    "threshold = 8\n",
    "\n",
    "\n",
    "for r in reviews:\n",
    "    label = r[\"sentiment\"]\n",
    "    #print(label)\n",
    "    text = r[\"content\"] \n",
    "    words = []\n",
    "    value = 0\n",
    "    for x in text:\n",
    "        #print(x)\n",
    "        for a in x:\n",
    "            word = a[0]\n",
    "            if word in sent_lex:          \n",
    "                sentiment = sent_lex[word].split(' ')[5].split('=')[1]\n",
    "                #print(sentiment)\n",
    "                if sentiment == 'positive':\n",
    "                    value += 1\n",
    "                if sentiment == 'negative':\n",
    "                    value -=1\n",
    "\n",
    "    if value > threshold:\n",
    "        classification = 1\n",
    "        \n",
    "    else:\n",
    "        classification = 0\n",
    "        \n",
    "    if classification == 1 and label == 'POS':\n",
    "        classifications.append(1)\n",
    "        NoCorrect +=1\n",
    "\n",
    "    elif classification == 0 and label == 'NEG':\n",
    "        NoCorrect +=1\n",
    "        classifications.append(1)\n",
    "    else:\n",
    "        classifications.append(0)\n",
    "\n",
    "\n",
    "accuracy = (NoCorrect / 2000) * 100\n",
    "\n",
    "print(\"Accuracy: \", accuracy, \"%\")\n",
    "        \n",
    "                    \n",
    "                \n",
    "                                 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "iy528EUTphz5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.95\n"
     ]
    }
   ],
   "source": [
    "# token_results should be a list of binary indicators; for example [1, 0, 1, ...] \n",
    "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
    "token_results = classifications\n",
    "token_accuracy = (sum(classifications)/2000 ) * 100\n",
    "print(\"Accuracy: %0.2f\" % token_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twox0s_3eS0V"
   },
   "source": [
    "As the sentiment lexicon also has information about the **magnitude** of\n",
    "sentiment (e.g., *“excellent\"* would have higher magnitude than\n",
    "*“good\"*), we can take a more fine-grained approach by adding up all\n",
    "sentiment scores, and deciding the polarity of the movie review using\n",
    "the sign of the weighted score $S_{weighted}$.\n",
    "\n",
    "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
    "\n",
    "\n",
    "Make sure you define an appropriate threshold for this approach.\n",
    "\n",
    "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qG3hUDnPtkhS",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  69.0 %\n"
     ]
    }
   ],
   "source": [
    "NoCorrect = 0\n",
    "classificationsmag = []\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  reviews = json.load(f)\n",
    "\n",
    "\n",
    "threshold = 12\n",
    "\n",
    "\n",
    "for r in reviews:\n",
    "    label = r[\"sentiment\"]\n",
    "    #print(label)\n",
    "    text = r[\"content\"] \n",
    "    words = []\n",
    "    value = 0\n",
    "    for x in text:\n",
    "        #print(x)\n",
    "        for a in x:\n",
    "            word = a[0]\n",
    "            if word in sent_lex:          \n",
    "                sentiment = sent_lex[word].split(' ')[5].split('=')[1]\n",
    "                magnitude = sent_lex[word].split(' ')[0].split('=')[1]\n",
    "                if sentiment == 'positive' and magnitude == 'strongsubj':\n",
    "                    value += 2\n",
    "                if sentiment == 'positive' and magnitude == 'weaksubj':\n",
    "                    value += 1\n",
    "                if sentiment == 'negative' and magnitude == 'strongsubj':\n",
    "                    value -=2\n",
    "                if sentiment == 'negative' and magnitude == 'weaksubj':\n",
    "                    value -=1\n",
    "\n",
    "\n",
    "    if value > threshold:\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = 0\n",
    "        \n",
    "    if classification == 1 and label == 'POS':\n",
    "        classificationsmag.append(1)\n",
    "        NoCorrect +=1\n",
    "\n",
    "    elif classification == 0 and label == 'NEG':\n",
    "        NoCorrect +=1\n",
    "        classificationsmag.append(1)\n",
    "    else:\n",
    "        classificationsmag.append(0)\n",
    "\n",
    "\n",
    "accuracy = (NoCorrect / 2000) * 100\n",
    "\n",
    "print(\"Accuracy: \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9vVk7CvDpyka"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.00\n"
     ]
    }
   ],
   "source": [
    "magnitude_results = classificationsmag\n",
    "magnitude_accuracy = (sum(classificationsmag)/2000 ) * 100\n",
    "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9SHoGPfsAHV"
   },
   "source": [
    "#### Make a barplot of the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "8LgBcYcXsEk3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = [token_accuracy, magnitude_accuracy]\n",
    "x = ['Tokencounter', 'Magnitude']\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([60,75])\n",
    "plt.bar(x,accuracies)\n",
    "axes.set_ylabel('Accuracy')\n",
    "axes.set_xlabel('Method used')\n",
    "plt.title('Lexicon based approach')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNhS8OCVxMHd"
   },
   "source": [
    "## (Q1.3 Optional) A better threshold (1pt)\n",
    "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
    "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo7gk1I-omLI"
   },
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwt0B8h8aKjr"
   },
   "source": [
    "The problem with the threshold being length-invariant is the fact that shorter reviews will automatically get classified as negative, because the probabilty that the document has 8 positive sentiment words is very small. And conversely, it also a problem for very long reviws, since the probability that there are more than 8 positive sentiment words is higher, even though there still could be high number of negative words present. \n",
    "\n",
    "An alternative way to compute the treshold could be:\n",
    "\n",
    "First compute the average document length and assume that the threshold for the average length is equal to 8.\n",
    "\n",
    "To compute the threshold: \n",
    "\n",
    "    -Take a document\n",
    "    -Divide its length by the average length\n",
    "    -Multiply with 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MFrz8Jink0D"
   },
   "source": [
    "# Significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxkxrldT9Ymc"
   },
   "source": [
    "Does using the magnitude improve the results? Oftentimes, answering such questions by simply comparing accuracy\n",
    "scores is not enough. When dealing with natural language and human ratings, it is safe to assume that there are infinitely many possible\n",
    "instances that could be used for training and testing, of which the ones\n",
    "we actually train and test on are a tiny sample. Thus, it is possible\n",
    "that observed differences in the reported performance are due to mere chance.\n",
    "\n",
    "There exist statistical methods which can be used to check for\n",
    "consistency (*statistical significance*) in the results, and one of the\n",
    "simplest such tests is the **sign test**. \n",
    "\n",
    "The sign test is based on the binomial distribution. Count all cases when System 1 is better than System 2, when System 2 is better than System 1, and when they are the same. Call these numbers $Plus$, $Minus$ and $Null$ respectively. \n",
    "\n",
    "The sign test returns the probability that the null hypothesis is true. \n",
    "\n",
    "This probability is called the $p$-value and it can be calculated for the two-sided sign test using the following formula (we multiply by two because this is a two-sided sign test and tests for the significance of differences in either direction):\n",
    "\n",
    "$$2 \\, \\sum\\limits_{i=0}^{k} \\binom{N}{i} \\, q^i \\, (1-q)^{N-i}$$\n",
    "\n",
    "where $$N = 2 \\Big\\lceil \\frac{Null}{2}\\Big\\rceil + Plus + Minus$$ is the total\n",
    "number of cases, and\n",
    "$$k = \\Big\\lceil \\frac{Null}{2}\\Big\\rceil + \\min\\{Plus,Minus\\}$$ is the number of\n",
    "cases with the less common sign. \n",
    "\n",
    "Here, we\n",
    "treat ties by adding half a point to either side, rounding up to the\n",
    "nearest integer if necessary. \n",
    "\n",
    "In this experiment, $q = 0.5$, so the formula simplifies to:\n",
    "$$2\\times 0.5^N\\, \\sum\\limits_{i=0}^{k} \\binom{N}{i}$$\n",
    "\n",
    "\n",
    "We use the `comb` function from `scipy` and the `decimal` package for the stable adding of numbers in the final summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "de5l4oPkE-BS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1] [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value = 0.6548123008131539579058732112\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "from scipy.special import comb\n",
    "\n",
    "\n",
    "def sign_test(results_1, results_2):\n",
    "  \"\"\"test for significance\"\"\"\n",
    "  ties, plus, minus = 0, 0, 0\n",
    "\n",
    "  for i in range(0, len(results_1)):\n",
    "    if results_1[i]==results_2[i]:\n",
    "      ties += 1\n",
    "    elif results_1[i]==0: \n",
    "      plus += 1\n",
    "    elif results_2[i]==0: \n",
    "      minus += 1\n",
    "\n",
    "  n = (2 * math.ceil(ties/2.0)) + plus + minus\n",
    "  k = math.ceil(ties/2.0) + min(plus,minus)\n",
    "\n",
    "  summation = Decimal(0.0)\n",
    "  for i in range(0,int(k)+1):\n",
    "      summation += (Decimal(comb(n,i,exact=True)))\n",
    "\n",
    "  # use two-tailed version of test\n",
    "  summation *= 2\n",
    "  summation *= (Decimal(0.5)**Decimal(n))\n",
    "  \n",
    "  print(\"the difference is\", \n",
    "        \"not significant\" if summation >= 0.05 else \"significant\")\n",
    "  \n",
    "  return summation\n",
    "\n",
    "p_value = sign_test(token_results, magnitude_results)\n",
    "print(token_results, magnitude_results)\n",
    "print(\"p_value =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhU_tk-BOaXb"
   },
   "source": [
    "### Using the Sign test\n",
    "\n",
    "**From now on, report all differences between systems, as well as between system configurations, using the\n",
    "sign test.**\n",
    "    \n",
    "You should report statistical test\n",
    "results in an appropriate form. If there are several different methods\n",
    "(i.e., systems) to compare, the Sign test can only be applied to pairs of them\n",
    "at a time. When reporting these pair-wise differences, you should\n",
    "summarise trends to avoid redundancy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LibV4nR89BXb"
   },
   "source": [
    "# Naive Bayes (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnF9adQnuwia"
   },
   "source": [
    "\n",
    "Your second task is to program a simple Machine Learning approach that operates\n",
    "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
    "described by Pang et al. (2002). In this approach, the only features we\n",
    "will consider are the words in the text themselves, without bringing in\n",
    "external sources of information. The BoW model is a popular way of\n",
    "representing texts as vectors, making it\n",
    "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
    "However, the BoW representation is also very crude, since it discards\n",
    "all information related to word order and grammatical structure in the\n",
    "original text—as the name suggests.\n",
    "\n",
    "## Writing your own classifier (4pts)\n",
    "\n",
    "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
    "a reminder, the Naive Bayes classifier works according to the following\n",
    "equation:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
    "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
    "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
    "vector. Remember that we use the log of these probabilities when making\n",
    "a prediction:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
    "\n",
    "You can find more details about Naive Bayes in [Jurafsky &\n",
    "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
    "this helpful\n",
    "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
    "\n",
    "*Note: this section and the next aim to put you in a position to replicate\n",
    "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
    "    will differ from theirs, as they used different data.*\n",
    "\n",
    "**You must write the Naive Bayes training and prediction code from\n",
    "scratch.** You will not be given credit for using off-the-shelf Machine\n",
    "Learning libraries.\n",
    "\n",
    "The data contains the text of the reviews, where each document consists\n",
    "of the sentences in the review, the sentiment of the review and an index\n",
    "(cv) that you will later use for cross-validation. The\n",
    "text has already been tokenised and POS-tagged for you. Your algorithm\n",
    "should read in the text, **lowercase it**, store the words and their\n",
    "frequencies in an appropriate data structure that allows for easy\n",
    "computation of the probabilities used in the Naive Bayes algorithm, and\n",
    "then make predictions for new instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEpyQSBSkb33"
   },
   "source": [
    "#### (Q3.1) Unseen words (1pt)\n",
    "The presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
    "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes**. \n",
    "\n",
    "In case 2, you could also set $P(c|\\bar{f}) = 0$ for the class $c$ within which $f_i \\in \\bar{f}$ was not seen. One way to implement this in log space is to set $P(c|\\bar{f}) = -\\infty$. What would be the problem instead with skipping words only for one class in case 2? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BanFiYYnoxDW"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "If the word that is unseen in one of two classes, would be skipped for only that class, there would be an implicit present in our model. This way there is unfair comparison between the two classes when trying to classify them, because the class for which the word in question, simply has less features than the other class. If this happens often, the implicit bias will keep increasing, making it even more unfair. The possible solution that is given above, would imply that instead of adding 0, a very small number would be added to the probabilty of the class in which the word is not present, however this essentially gives the same problem, as the added amount is very close to zero. Therefore there is still an unfair advantage, hence it is better to skip the word for both classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsZRhaI3WvzC"
   },
   "source": [
    "#### (Q3.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "G7zaJYGFvIJ3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### NB (train)\n",
    "import copy\n",
    "### First make vocabulary\n",
    "vocabulary = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabulary[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "#             print(a)\n",
    "            vocabulary[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "CondProbDict = vocabulary\n",
    "CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "wordcounterPos = vocabulary\n",
    "wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "wordcounterNeg = vocabulary\n",
    "wordcounterNeg = copy.deepcopy(wordcounterNeg)\n",
    "\n",
    "\n",
    "# count for each word its occurence\n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            wordcounterPos[word] += 1\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "# count for each word its occurence\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            \n",
    "            wordcounterNeg[word] += 1\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "totalcountPos = sum(wordcounterPos.values())\n",
    "totalcountNeg = sum(wordcounterNeg.values())\n",
    "\n",
    "# print(totalcountNeg, totalcountPos)\n",
    "\n",
    "wordcounterPos = {k: v for k, v in wordcounterPos.items() if v != 0}\n",
    "wordcounterNeg = {k: v for k, v in wordcounterNeg.items() if v != 0}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key,value in CondProbDict.items():\n",
    "    \n",
    "#   if word in neg and pos\n",
    "    if key in wordcounterPos and key in wordcounterNeg:\n",
    " \n",
    "        CondProbDict[key] =  [wordcounterPos[key]/totalcountPos, wordcounterNeg[key]/totalcountNeg]\n",
    "        \n",
    "# print(CondProbDict)\n",
    "\n",
    "\n",
    "        \n",
    "negPrior = len(negativeReviews) / (len(positiveReviews) + len(negativeReviews))\n",
    "posPrior = len(positiveReviews) / (len(positiveReviews) + len(negativeReviews))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.50\n"
     ]
    }
   ],
   "source": [
    "### NB (test)\n",
    "# load in test data\n",
    "\n",
    "results = []\n",
    "labels = []\n",
    "\n",
    "for x in range(2):\n",
    "    i = x\n",
    "    for y in range(100):\n",
    "        labels.append(i)\n",
    "\n",
    "negativeReviewsTest = reviews[900:1000]\n",
    "positiveReviewsTest = reviews[1900:]\n",
    "testData = negativeReviewsTest + positiveReviewsTest  \n",
    "\n",
    "# loop door elke review heen in test data\n",
    "# per review bepaal de class prob voor neg and pos\n",
    "# pak de hoogste hooste prob \n",
    "\n",
    "for i, r in enumerate(testData):\n",
    "    text = r[\"content\"] \n",
    "    \n",
    "    posProb = 0\n",
    "    negProb = 0\n",
    "    for x in text:\n",
    "        \n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            \n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word in CondProbDict and CondProbDict[word] != 0:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0])\n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            \n",
    "    posProb += np.log(posPrior)\n",
    "    negProb += np.log(negPrior)\n",
    "    if posProb > negProb:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)\n",
    "                \n",
    "significance_list_nb = []\n",
    "amount_of_correct = 0\n",
    "for r, l in zip(results, labels):\n",
    "    if r == l:\n",
    "        amount_of_correct += 1\n",
    "        significance_list_nb.append(1)\n",
    "    else:\n",
    "        significance_list_nb.append(0)\n",
    "\n",
    "\n",
    "nb_accuracy = (amount_of_correct / len(labels) * 100)\n",
    "print(\"Accuracy: %0.2f\" % nb_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0INK-PBoM6CB"
   },
   "source": [
    "#### (Q3.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
    "\n",
    "Simulate this scenario by keeping the positive reviews\n",
    "data unchanged, but only using negative reviews cv000–cv089 for\n",
    "training, and cv900–cv909 for testing. Calculate the classification\n",
    "accuracy, and explain what changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "GWDkt5ZrrFGp"
   },
   "outputs": [],
   "source": [
    "### NB with imbalanced data (train)\n",
    "import copy\n",
    "### First make vocabulary\n",
    "vocabulary = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabulary[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabulary[word] = 0\n",
    "    if i == 89:\n",
    "        break\n",
    "\n",
    "CondProbDict = vocabulary\n",
    "CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "wordcounterPos = vocabulary\n",
    "wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "wordcounterNeg = vocabulary\n",
    "wordcounterNeg = copy.deepcopy(wordcounterNeg)\n",
    "\n",
    "\n",
    "# count for each word its occurence\n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            wordcounterPos[word] += 1\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "# count for each word its occurence\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            \n",
    "            wordcounterNeg[word] += 1\n",
    "    if i == 89:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "totalcountPos = sum(wordcounterPos.values())\n",
    "totalcountNeg = sum(wordcounterNeg.values())\n",
    "\n",
    "wordcounterPos = {k: v for k, v in wordcounterPos.items() if v != 0}\n",
    "wordcounterNeg = {k: v for k, v in wordcounterNeg.items() if v != 0}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key,value in CondProbDict.items():\n",
    "    \n",
    "#   if word in neg and pos\n",
    "    if key in wordcounterPos and key in wordcounterNeg:\n",
    " \n",
    "        CondProbDict[key] =  [wordcounterPos[key]/totalcountPos, wordcounterNeg[key]/totalcountNeg]\n",
    "        \n",
    "\n",
    "\n",
    "negPrior = 90 / (len(positiveReviews) + len(negativeReviews))\n",
    "posPrior = len(positiveReviews) / (len(positiveReviews) + len(negativeReviews))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.18\n"
     ]
    }
   ],
   "source": [
    "### NB with imbalanced data (test)\n",
    "# load in test data\n",
    "\n",
    "resultsImb = []\n",
    "labels = []\n",
    "\n",
    "for x in range(10):\n",
    "    labels.append(0)\n",
    "for x in range(100):\n",
    "    labels.append(1)\n",
    "\n",
    "negativeReviewsTest = reviews[900:909]\n",
    "positiveReviewsTest = reviews[1900:]\n",
    "testData = negativeReviewsTest + positiveReviewsTest  \n",
    "\n",
    "# loop door elke review heen in test data\n",
    "# per review bepaal de class prob voor neg and pos\n",
    "# pak de hoogste hooste prob \n",
    "\n",
    "for i, r in enumerate(testData):\n",
    "    text = r[\"content\"] \n",
    "    \n",
    "    posProb = 0\n",
    "    negProb = 0\n",
    "    for x in text:\n",
    "        \n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            \n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word in CondProbDict and CondProbDict[word] != 0:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "                \n",
    "#     print(\"PRIORR\", np.log(posPrior))\n",
    "#     print(posProb)\n",
    "    posProb += np.log(posPrior)\n",
    "    negProb += np.log(negPrior)\n",
    "    if posProb > negProb:\n",
    "        resultsImb.append(1)\n",
    "    else:\n",
    "        resultsImb.append(0)\n",
    "        \n",
    "amount_of_correct = 0\n",
    "for r, l in zip(resultsImb, labels):\n",
    "    if r == l:\n",
    "        amount_of_correct += 1\n",
    "\n",
    "\n",
    "nb_accuracy = (amount_of_correct / len(labels) * 100)\n",
    "print(\"Accuracy: %0.2f\" % nb_accuracy)\n",
    "        \n",
    "# nb_accuracy = (sum(results)/sum(labels) ) * 100\n",
    "# print(\"Accuracy: %0.2f\" % nb_accuracy)\n",
    "\n",
    "# # print(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# door de deling zijn de negatieve probs per woord veel hoger in verhouding van de positieve probs \n",
    "\n",
    "# daardoor zal hij gemiddeld gezien veel vaker een review als negatief classificeren\n",
    "\n",
    "# terwijl je veel meer traint op positief ... maar dit komt omdat de probabilities van een woord in de positieve en negatieve \n",
    "# classe worden gedeeld door de som van hun eigen aantal. dus het aantal keer dat \"dog\" voorkomt in neg / aantal worden in neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Q3.3\n",
    "No, accuracy is not a measurement for a imbalanced dataset. This is because a classifier can easily get a high accuracy without actually doing a good job in classifying datapoints. It can simply all the datapoints for the class that is more present in the trainig data.\n",
    "\n",
    "This models (naive bayes) makes a mistake in calculating the probabilties for a word given a class. This is because you count the amount of times a word occurs in a negative review but you also divide by the total number of words in negative reviews. Because the total amount of words in negative reviews is low, you simply always divide by a relative low number and thus ends up with a high probability. But this is not a fair comparison for the positive class where you divide by a much higher total amount of positive words and thus end up in a lower probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wJzcHX3WUDm"
   },
   "source": [
    "## Smoothing (1.5pts)\n",
    "\n",
    "As mentioned above, the presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive\n",
    "Bayes classifier to be $0$, thus making that particular test instance\n",
    "undecidable. The standard way to mitigate this effect (as well as to\n",
    "give more clout to rare words) is to use smoothing, in which the\n",
    "probability fraction\n",
    "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
    "$w_i$ becomes\n",
    "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBNIcbwUWphC"
   },
   "source": [
    "#### (Q3.4) Implement Laplace feature smoothing (1pt)\n",
    "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
    "Bayes classifier’s code, and report the impact on performance. \n",
    "Use $\\kappa = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "g03yflCc9kpW"
   },
   "outputs": [],
   "source": [
    "### NB with smoothing (train)\n",
    "\n",
    "\n",
    "import copy\n",
    "k = 1\n",
    "### First make vocabulary\n",
    "vocabulary = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabulary[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabulary[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "CondProbDict = vocabulary\n",
    "CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "\n",
    "wordcounterPos = vocabulary\n",
    "wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "wordcounterNeg = vocabulary\n",
    "wordcounterNeg = copy.deepcopy(wordcounterNeg)\n",
    "\n",
    "\n",
    "# count for each word its occurence\n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            wordcounterPos[word] += 1\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "# count for each word its occurence\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            \n",
    "            wordcounterNeg[word] += 1\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "totalcountPos = sum(wordcounterPos.values())\n",
    "totalcountNeg = sum(wordcounterNeg.values())\n",
    "\n",
    "\n",
    "for key,value in CondProbDict.items():\n",
    "    CondProbDict[key] =  [( wordcounterPos[key] + k) / (totalcountPos + len(CondProbDict)), \n",
    "                          ( wordcounterNeg[key] + k) / (totalcountNeg + len(CondProbDict))]\n",
    "        \n",
    "negPrior = 900 / 1800\n",
    "posPrior = 900 / 1800\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.50\n"
     ]
    }
   ],
   "source": [
    "### NB with smoothing (test)\n",
    "# load in test data\n",
    "\n",
    "resultsSmooth = []\n",
    "labels = []\n",
    "\n",
    "for x in range(2):\n",
    "    i = x\n",
    "    for y in range(100):\n",
    "        labels.append(i)\n",
    "\n",
    "negativeReviewsTest = reviews[900:1000]\n",
    "positiveReviewsTest = reviews[1900:]\n",
    "testData = negativeReviewsTest + positiveReviewsTest  \n",
    "\n",
    "# loop door elke review heen in test data\n",
    "# per review bepaal de class prob voor neg and pos\n",
    "# pak de hoogste hooste prob \n",
    "\n",
    "for i, r in enumerate(testData):\n",
    "    text = r[\"content\"] \n",
    "    \n",
    "    posProb = 0\n",
    "    negProb = 0\n",
    "    for x in text:\n",
    "        \n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            \n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "#             if word in CondProbDict and CondProbDict[word] != 0:\n",
    "            if word in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0])\n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "            \n",
    "    posProb += np.log(posPrior)\n",
    "    negProb += np.log(negPrior)\n",
    "\n",
    "    if posProb > negProb:\n",
    "        resultsSmooth.append(1)\n",
    "    else:\n",
    "        resultsSmooth.append(0)\n",
    "\n",
    "sign_nb_smooth = []\n",
    "amount_of_correct = 0\n",
    "for r, l in zip(resultsSmooth, labels):\n",
    "    if r == l:\n",
    "        amount_of_correct += 1\n",
    "        sign_nb_smooth.append(1)\n",
    "    else:\n",
    "        sign_nb_smooth.append(0)\n",
    "\n",
    "\n",
    "nb_accuracy = (amount_of_correct / len(labels) * 100)\n",
    "print(\"Accuracy: %0.2f\" % nb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-conSBddWWyN"
   },
   "source": [
    "#### (Q3.5) Is the difference between non smoothed (Q3.2) and smoothed (Q3.4) statistically significant? (0.5pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "CCvSNGlHMUPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "1.056348479009256422247245265\n"
     ]
    }
   ],
   "source": [
    "p_value = sign_test(significance_list_nb, sign_nb_smooth)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the smoothed and non smoothed methods is not significant. This is because the p-value from the given function is much higher than 0.05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiGcgwba87D5"
   },
   "source": [
    "## Cross-Validation (1.5pts)\n",
    "\n",
    "A serious danger in using Machine Learning on small datasets, with many\n",
    "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
    "suggested by the data” errors. This type of error occurs when we make\n",
    "repeated improvements to our classifiers by playing with features and\n",
    "their processing, but we don’t get a fresh, never-before seen test\n",
    "dataset every time. Thus, we risk developing a classifier that gets better\n",
    "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
    "\n",
    "A simple method to guard against Type III errors is to use\n",
    "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
    "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
    "time holding out one of the folds for testing, training our classifier\n",
    "on the remaining N - 1 data folds, and reporting performance on the\n",
    "held-out fold. We can use different strategies for dividing the data:\n",
    "\n",
    "-   Consecutive splitting:\n",
    "  - cv000–cv099 = Split 1\n",
    "  - cv100–cv199 = Split 2\n",
    "  - etc.\n",
    "  \n",
    "-   Round-robin splitting (mod 10):\n",
    "  - cv000, cv010, cv020, … = Split 1\n",
    "  - cv001, cv011, cv021, … = Split 2\n",
    "  - etc.\n",
    "\n",
    "-   Random sampling/splitting\n",
    "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OeLcbSauGtR"
   },
   "source": [
    "#### (Q3.6) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q3.2 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "\n",
    "\n",
    "### Create the folds\n",
    "def make_folds(positiveReviews,negativeReviews, stemming, pos):\n",
    "    folds_dict_pos = {}\n",
    "    \n",
    "    if stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "    \n",
    "    for x in range(10):\n",
    "        folds_dict_pos[x] = []\n",
    "\n",
    "    for i, r in enumerate(positiveReviews):\n",
    "\n",
    "        text = r[\"content\"] \n",
    "        words_in_review = []\n",
    "        for x in text:\n",
    "            for a in x:\n",
    "                word = a[0].lower()\n",
    "                if stemming:\n",
    "                    word= stemmer.stem(word)\n",
    "                    \n",
    "                if pos:\n",
    "                    pos = a[1]\n",
    "                    words_in_review.append((word, pos))\n",
    "                    \n",
    "                 \n",
    "                if pos == False:    \n",
    "                    words_in_review.append(word)\n",
    "\n",
    "        current_list = folds_dict_pos[i % 10]\n",
    "        new_list = current_list + [words_in_review]\n",
    "\n",
    "        folds_dict_pos[i % 10] = new_list\n",
    "\n",
    "    # # fill vocab with all unique words \n",
    "    folds_dict_neg = {}\n",
    "    for x in range(10):\n",
    "        folds_dict_neg[x] = []\n",
    "\n",
    "    for i, r in enumerate(negativeReviews):\n",
    "\n",
    "        text = r[\"content\"] \n",
    "        words_in_review = []\n",
    "        for x in text:\n",
    "            for a in x:\n",
    "                word = a[0].lower()\n",
    "                if stemming:\n",
    "                    word= stemmer.stem(word)\n",
    "                    \n",
    "                if pos:\n",
    "                    pos = a[1]\n",
    "                    words_in_review.append((word, pos))\n",
    "                    \n",
    "                if pos == False:    \n",
    "                    words_in_review.append(word)\n",
    "                \n",
    "\n",
    "        current_list = folds_dict_neg[i % 10]\n",
    "        new_list = current_list + [words_in_review]\n",
    "\n",
    "        folds_dict_neg[i % 10] = new_list\n",
    "    return folds_dict_pos, folds_dict_neg\n",
    "\n",
    "\n",
    "###Get the training data given the test fold \n",
    "def pick_training_boiz(test_fold, fold_dict):\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for fold in range(10):\n",
    "        if fold != test_fold:\n",
    "            data_list.append(fold_dict[fold])\n",
    "            \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B:\n",
    "Since we had to reimplement the Naive Bayes from question 3.2, we did not implement smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "3KeCGPa7Nuzx",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45329\n",
      "45329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█████████████████▌                                                                                                                                                              | 1/10 [00:01<00:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45392\n",
      "45392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████████████████████████████████▏                                                                                                                                            | 2/10 [00:02<00:10,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45438\n",
      "45438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████████████████████████████████████████████▊                                                                                                                           | 3/10 [00:03<00:09,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45389\n",
      "45389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|██████████████████████████████████████████████████████████████████████▍                                                                                                         | 4/10 [00:05<00:07,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45648\n",
      "45648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████████████████████████████████████████████████████████████████████████████████████████                                                                                        | 5/10 [00:06<00:06,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45296\n",
      "45296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                      | 6/10 [00:07<00:05,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45365\n",
      "45365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 7/10 [00:08<00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45409\n",
      "45409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 8/10 [00:10<00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45557\n",
      "45557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 9/10 [00:11<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45642\n",
      "45642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "############# NB with round robin CV\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "k = 1\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds(positiveReviews, negativeReviews, False, False)\n",
    "\n",
    "accuracies_nb_smooting = []\n",
    "all_folds_nb_smooting = []\n",
    "\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(10)):\n",
    "    \n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "                \n",
    "    \n",
    "    CondProbDict = fold_vocab\n",
    "    CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "    wordcounterPos = fold_vocab\n",
    "    wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "    wordcounterNeg = fold_vocab\n",
    "    wordcounterNeg = copy.deepcopy(wordcounterNeg)    \n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                if word in wordcounterPos:\n",
    "                    wordcounterPos[word.lower()] += 1\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                if word in wordcounterNeg:\n",
    "                    wordcounterNeg[word.lower()] += 1\n",
    "    \n",
    "    totalcountPos = sum(wordcounterPos.values())\n",
    "    totalcountNeg = sum(wordcounterNeg.values())\n",
    "    \n",
    "    print(len(wordcounterPos))\n",
    "    print(len(wordcounterNeg))\n",
    "\n",
    "#     wordcounterPos = {k: v for k, v in wordcounterPos.items() if v != 0}\n",
    "#     wordcounterNeg = {k: v for k, v in wordcounterNeg.items() if v != 0}\n",
    "\n",
    "#     for key,value in CondProbDict.items():\n",
    "#         if key in wordcounterPos and key in wordcounterNeg:\n",
    "#             CondProbDict[key] =  [wordcounterPos[key]/totalcountPos, wordcounterNeg[key]/totalcountNeg]\n",
    "            \n",
    "    for key,value in CondProbDict.items():\n",
    "        CondProbDict[key] =  [( wordcounterPos[key] + k) / (totalcountPos + len(CondProbDict)), \n",
    "                          ( wordcounterNeg[key] + k) / (totalcountNeg + len(CondProbDict))]\n",
    "\n",
    "    posPrior = len(train_pos) / (len(train_pos) + len(train_neg))\n",
    "    negPrior = len(train_neg) / (len(train_pos) + len(train_neg))\n",
    "    \n",
    "    results = []\n",
    "    labels = []\n",
    "    \n",
    "    for review in test_pos:\n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        for word in review:\n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word.lower() in CondProbDict and CondProbDict[word.lower()] != 0:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word.lower()][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word.lower()][1])\n",
    "\n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "\n",
    "        labels.append(1)\n",
    "        \n",
    "    for review in test_neg:\n",
    "                \n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        for word in review:\n",
    "            \n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word.lower() in CondProbDict and CondProbDict[word.lower()] != 0:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word.lower()][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word.lower()][1])\n",
    "\n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "        \n",
    "        labels.append(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_folds_nb_smooting.append(1)\n",
    "            \n",
    "        else:\n",
    "            all_folds_nb_smooting.append(0)\n",
    "            \n",
    "\n",
    "    accuracies_nb_smooting.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracy_nb_smooting = sum(accuracies)/len(accuracies)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.55"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otdlsDXBNyOa"
   },
   "source": [
    "#### (Q3.7) Write code to calculate and report the variance of the 10 accuracy scores, in addition to the final performance. You must not use a library, such as numpy. (0.5pt)\n",
    "\n",
    "**Please report all future results using 10-fold cross-validation now\n",
    "(unless told to use the held-out test set).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZoBQm1KuNzNR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of accuracies in Cross Validation:  6.0225\n",
      "Accuracy of Naive Bayes with Cross Validation:  81.55\n"
     ]
    }
   ],
   "source": [
    "def calc_variance(accuracies):\n",
    "    mean = sum(accuracies)/len(accuracies)\n",
    "    \n",
    "    summ = 0\n",
    "    for acc in accuracies:\n",
    "        summ += (acc - mean)**2\n",
    "    \n",
    "    return (1/len(accuracies))*summ\n",
    "\n",
    "variance = calc_variance(accuracies)\n",
    "\n",
    "print(\"Variance of accuracies in Cross Validation: \", variance)\n",
    "print(\"Accuracy of Naive Bayes with Cross Validation: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6A2zX9_BRKm"
   },
   "source": [
    "## Features, overfitting, and the curse of dimensionality\n",
    "\n",
    "In the Bag-of-Words model, ideally we would like each distinct word in\n",
    "the text to be mapped to its own dimension in the output vector\n",
    "representation. However, real world text is messy, and we need to decide\n",
    "on what we consider to be a word. For example, is “`word`\" different\n",
    "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
    "definition, and the number of features explodes, while our algorithm\n",
    "fails to learn anything generalisable. Too lax, and we risk destroying\n",
    "our learning signal. In the following section, you will learn about\n",
    "confronting the feature sparsity and the overfitting problems as they\n",
    "occur in NLP classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKK8FNt8VtcZ"
   },
   "source": [
    "### Stemming (1.5pts)\n",
    "\n",
    "To make your algorithm more robust, use stemming and\n",
    "hash different inflections of a word to the same feature in the BoW\n",
    "vector space. How does the performance of your classifier change when\n",
    "you use stemming on your training and test datasets? Please use the [Porter stemming\n",
    "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
    "\n",
    "You should also perform cross-validation. Concatenate the predictions from all folds to compute the significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "NxtCul1IrBi_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32371\n",
      "32371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█████████████████▌                                                                                                                                                              | 1/10 [00:01<00:09,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32482\n",
      "32482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████████████████████████████████▏                                                                                                                                            | 2/10 [00:01<00:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32545\n",
      "32545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████████████████████████████████████████████▊                                                                                                                           | 3/10 [00:02<00:06,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32498\n",
      "32498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|██████████████████████████████████████████████████████████████████████▍                                                                                                         | 4/10 [00:03<00:05,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32699\n",
      "32699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████████████████████████████████████████████████████████████████████████████████████████                                                                                        | 5/10 [00:04<00:04,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32439\n",
      "32439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                      | 6/10 [00:05<00:03,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32440\n",
      "32440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 7/10 [00:06<00:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32493\n",
      "32493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 8/10 [00:07<00:01,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32576\n",
      "32576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 9/10 [00:07<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HALLOO\n",
      "32667\n",
      "32667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "############### NB with stemming and CV\n",
    "\n",
    "\n",
    "from nltk.stem import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds(positiveReviews, negativeReviews, True, False)\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "\n",
    "#### Set K for smoothing\n",
    "\n",
    "k = 1 \n",
    "    \n",
    "accuraciesStem = []\n",
    "all_fold_predictions_stem = []\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(10)):\n",
    "    \n",
    "    print(\"HALLOO\")\n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word] = 0\n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word] = 0\n",
    "                \n",
    "    \n",
    "    CondProbDict = fold_vocab\n",
    "    CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "    wordcounterPos = fold_vocab\n",
    "    wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "    wordcounterNeg = fold_vocab\n",
    "    wordcounterNeg = copy.deepcopy(wordcounterNeg)    \n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                if word in wordcounterPos:\n",
    "                    wordcounterPos[word] += 1\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                if word in wordcounterNeg:\n",
    "                    wordcounterNeg[word] += 1\n",
    "    \n",
    "    totalcountPos = sum(wordcounterPos.values())\n",
    "    totalcountNeg = sum(wordcounterNeg.values())\n",
    "    print(len(wordcounterPos))\n",
    "    print(len(wordcounterNeg))    \n",
    "\n",
    "    for key,value in CondProbDict.items():\n",
    "        CondProbDict[key] =  [(wordcounterPos[key]+k)/(totalcountPos+len(CondProbDict)), \n",
    "                              (wordcounterNeg[key]+k)/(totalcountNeg+len(CondProbDict))]\n",
    "\n",
    "    posPrior = len(train_pos) / (len(train_pos) + len(train_neg))\n",
    "    negPrior = len(train_neg) / (len(train_pos) + len(train_neg))\n",
    "    \n",
    "    results = []\n",
    "    labels = []\n",
    "    \n",
    "    for review in test_pos:\n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        for word in review:\n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "\n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "            all_fold_predictions_stem.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "            all_fold_predictions_stem.append(0)\n",
    "\n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for review in test_neg:\n",
    "                \n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        for word in review:\n",
    "            \n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "\n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "            \n",
    "        else:\n",
    "            results.append(0)\n",
    "            \n",
    "        \n",
    "        labels.append(0)\n",
    "    \n",
    "    \n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_fold_predictions_stem.append(1)\n",
    "        else:\n",
    "            all_fold_predictions_stem.append(0)\n",
    "\n",
    "    accuraciesStem.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracyStem = sum(accuraciesStem)/len(accuraciesStem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in Q3.6 we were asked to reimplement the NB from 3.2, we did not implement smoothing. We will implement smoothing with cross validation in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45329\n",
      "45329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█████████████████▌                                                                                                                                                              | 1/10 [00:01<00:11,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45392\n",
      "45392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████████████████████████████████▏                                                                                                                                            | 2/10 [00:02<00:09,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45438\n",
      "45438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████████████████████████████████████████████▊                                                                                                                           | 3/10 [00:03<00:09,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45389\n",
      "45389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|██████████████████████████████████████████████████████████████████████▍                                                                                                         | 4/10 [00:05<00:07,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45648\n",
      "45648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████████████████████████████████████████████████████████████████████████████████████████                                                                                        | 5/10 [00:06<00:06,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45296\n",
      "45296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                      | 6/10 [00:07<00:05,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45365\n",
      "45365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 7/10 [00:08<00:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45409\n",
      "45409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 8/10 [00:10<00:02,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45557\n",
      "45557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 9/10 [00:11<00:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45642\n",
      "45642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "############### NB without stemming and with CV\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set k for smoothing\n",
    "\n",
    "k = 1\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds(positiveReviews, negativeReviews, False, False)\n",
    "    \n",
    "accuraciesNostem = []\n",
    "\n",
    "all_folds_predictions_nostem = []\n",
    "\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(10)):\n",
    "    \n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "                \n",
    "    \n",
    "    CondProbDict = fold_vocab\n",
    "    CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "    wordcounterPos = fold_vocab\n",
    "    wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "    wordcounterNeg = fold_vocab\n",
    "    wordcounterNeg = copy.deepcopy(wordcounterNeg)    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                if word in wordcounterPos:\n",
    "                    wordcounterPos[word.lower()] += 1\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                if word in wordcounterNeg:\n",
    "                    wordcounterNeg[word.lower()] += 1\n",
    "    \n",
    "    totalcountPos = sum(wordcounterPos.values())\n",
    "    totalcountNeg = sum(wordcounterNeg.values())\n",
    "    \n",
    "    print(len(wordcounterPos))\n",
    "    print(len(wordcounterNeg))\n",
    "\n",
    "    #wordcounterPos = {k: v for k, v in wordcounterPos.items() if v != 0}\n",
    "    #wordcounterNeg = {k: v for k, v in wordcounterNeg.items() if v != 0}\n",
    "\n",
    "    for key,value in CondProbDict.items():\n",
    "\n",
    "    #   if word in neg and pos\n",
    "        if key in wordcounterPos and key in wordcounterNeg:\n",
    "            CondProbDict[key] =  [(wordcounterPos[key]+k)/(totalcountPos+len(CondProbDict))\n",
    "                                   , (wordcounterNeg[key]+k)/(totalcountNeg+len(CondProbDict))]\n",
    "\n",
    "    posPrior = len(train_pos) / (len(train_pos) + len(train_neg))\n",
    "    negPrior = len(train_neg) / (len(train_pos) + len(train_neg))\n",
    "    \n",
    "    results = []\n",
    "    labels = []\n",
    "    \n",
    "    for review in test_pos:\n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        for word in review:\n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word.lower() in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "\n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "            all_folds_predictions_nostem.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "            all_folds_predictions_nostem.append(0)\n",
    "\n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for review in test_neg:\n",
    "                \n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        for word in review:\n",
    "            \n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word.lower() in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "                \n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "                         \n",
    "        else:\n",
    "            results.append(0)\n",
    "            \n",
    "        \n",
    "        labels.append(0)\n",
    "    \n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_folds_predictions_nostem.append(1)  \n",
    "        else:\n",
    "            all_folds_predictions_nostem.append(0)\n",
    "            \n",
    "\n",
    "    accuraciesNostem.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracyNostem = sum(accuraciesNostem)/len(accuraciesNostem)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SrJ1BeLXTnk"
   },
   "source": [
    "#### (Q3.9): Is the difference between NB with smoothing and NB with smoothing+stemming significant? (1pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "gYqKBOiIrInT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy NB with stemming and CV:  81.45\n",
      "Accuracy NB without stemming and with CV:  81.7\n",
      "-------------------------------------------\n",
      "Variance of NB with stemming and CV:  7.5725\n",
      "Variance of NB without stemming and with CV:  6.51\n",
      "-------------------------------------------\n",
      "the difference is not significant\n",
      "1.000000000000000000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy NB with stemming and CV: \", accuracyStem)\n",
    "print(\"Accuracy NB without stemming and with CV: \", accuracyNostem)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Variance of NB with stemming and CV: \", calc_variance(accuraciesStem))\n",
    "print(\"Variance of NB without stemming and with CV: \", calc_variance(accuraciesNostem))\n",
    "print(\"-------------------------------------------\")\n",
    "p_value = sign_test(all_fold_predictions_stem, all_folds_predictions_nostem)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a significant difference between the predictions using stemming and the ones without. Even though our accuracy is a bit lower, we think that the classifier is more robust making it generalize better. This is because all the words are stemmed, reducing the number of outliers which will reduce the amount of overfitting on the training data, caused by noise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkDHVq_1XUVP"
   },
   "source": [
    "#### (Q3.10) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q3.4)? (0.5pt)\n",
    "Give actual numbers. You can use the held-out training set to determine these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MA3vee5-rJyy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary without stemming:  45348\n",
      "Length of vocabulary with stemming:  32561\n"
     ]
    }
   ],
   "source": [
    "### First calculate vocabulary using only .lower()\n",
    "\n",
    "import copy\n",
    "k = 1\n",
    "### First make vocabulary\n",
    "vocabularyNostem = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabularyNostem[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabularyNostem[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "print(\"Length of vocabulary without stemming: \",len(vocabularyNostem))\n",
    "\n",
    "###Calculate vocabulary with stemming\n",
    "from nltk.stem import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "k = 1\n",
    "### First make vocabulary\n",
    "vocabularyStem = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = stemmer.stem(a[0])\n",
    "            vocabularyStem[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = stemmer.stem(a[0])\n",
    "            vocabularyStem[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "print(\"Length of vocabulary with stemming: \",len(vocabularyStem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is clear that when using stemming, the amount of features decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoazfxbNV5Lq"
   },
   "source": [
    "### N-grams (1.5pts)\n",
    "\n",
    "A simple way of retaining some of the word\n",
    "order information when using bag-of-words representations is to add **n-gram** features. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHjy3I7-qWiu"
   },
   "source": [
    "#### (Q3.11) Retrain your classifier from (Q3.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
    "Report accuracy and statistical significances (in comparison to the experiment at (Q3.6) for all 10 folds, and between the new systems). You are allowed to use NLTK to build n-grams from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "eYuKMTOpq9jz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-99bccec5c275>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_pos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mbigrams_in_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mfold_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "############### NB with unigram+bigram features and with CV\n",
    "# fill vocab with all unique words \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set k for smoothing\n",
    "\n",
    "k = 1\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds(positiveReviews, negativeReviews, False, False)\n",
    "    \n",
    "accuraciesBigram = []\n",
    "\n",
    "all_folds_predictions_Bigram = []\n",
    "\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(10)):\n",
    "    \n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "            for bigram in bigrams_in_review :\n",
    "                fold_vocab[bigram] = 0\n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "            for bigram in bigrams_in_review :\n",
    "                fold_vocab[bigram] = 0\n",
    "\n",
    "                \n",
    "    \n",
    "    CondProbDict = fold_vocab\n",
    "    CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "    wordcounterPos = fold_vocab\n",
    "    wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "    wordcounterNeg = fold_vocab\n",
    "    wordcounterNeg = copy.deepcopy(wordcounterNeg)    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            for word in review:\n",
    "                if word in wordcounterPos:\n",
    "                    wordcounterPos[word.lower()] += 1\n",
    "            for bigram in bigrams_in_review :\n",
    "                if bigram in wordcounterPos:\n",
    "                    wordcounterPos[bigram] += 1\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            for word in review:\n",
    "                if word in wordcounterNeg:\n",
    "                    wordcounterNeg[word.lower()] += 1\n",
    "            for bigram in bigrams_in_review :\n",
    "                if bigram in wordcounterNeg:\n",
    "                    wordcounterNeg[bigram] += 1\n",
    "    \n",
    "    totalcountPos = sum(wordcounterPos.values())\n",
    "    totalcountNeg = sum(wordcounterNeg.values())\n",
    "    \n",
    "    print(len(wordcounterPos))\n",
    "    print(len(wordcounterNeg))\n",
    "\n",
    "    for key,value in CondProbDict.items():\n",
    "\n",
    "        if key in wordcounterPos and key in wordcounterNeg:\n",
    "            CondProbDict[key] =  [(wordcounterPos[key]+k)/(totalcountPos+len(CondProbDict))\n",
    "                                   , (wordcounterNeg[key]+k)/(totalcountNeg+len(CondProbDict))]\n",
    "\n",
    "    posPrior = len(train_pos) / (len(train_pos) + len(train_neg))\n",
    "    negPrior = len(train_neg) / (len(train_pos) + len(train_neg))\n",
    "    \n",
    "    results = []\n",
    "    labels = []\n",
    "    \n",
    "    for review in test_pos:\n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        bigrams_in_review = list(nltk.bigrams(review))\n",
    "        for word in review:\n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "        for bigram in bigrams_in_review:\n",
    "            if bigram in CondProbDict:\n",
    "                #print(\"BIGRAM IN POS\", bigram)\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[bigram][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[bigram][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "        #print(posProb)\n",
    "        #print(negProb)\n",
    "\n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "            all_folds_predictions_Bigram.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "            all_folds_predictions_Bigram.append(0)\n",
    "\n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for review in test_neg:\n",
    "                \n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        bigrams_in_review = list(nltk.bigrams(review))\n",
    "        for word in review:\n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word.lower() in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "        for bigram in bigrams_in_review:\n",
    "            if bigram in CondProbDict:\n",
    "                #print(\"BIGRAM IN neg\", bigram)\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[bigram][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[bigram][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))   \n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "                           \n",
    "        else:\n",
    "            results.append(0)\n",
    "            \n",
    "        \n",
    "        labels.append(0)\n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_folds_predictions_Bigram.append(1)\n",
    "        else:\n",
    "            all_folds_predictions_Bigram.append(0)\n",
    "\n",
    "    accuraciesBigram.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracyBigram = sum(accuraciesBigram)/len(accuraciesBigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### NB with unigram+bigram+trigram features and with CV\n",
    "# fill vocab with all unique words \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set k for smoothing\n",
    "\n",
    "k = 1\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds(positiveReviews, negativeReviews, False, False)\n",
    "    \n",
    "accuraciesBiTrigram = []\n",
    "\n",
    "all_folds_predictions_BiTrigram = []\n",
    "\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(10)):\n",
    "    \n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            trigrams_in_review = list(nltk.trigrams(review))\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "            for bigram in bigrams_in_review :\n",
    "                fold_vocab[bigram] = 0\n",
    "            for trigram in trigrams_in_review :\n",
    "                fold_vocab[trigram] = 0\n",
    "\n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            trigrams_in_review = list(nltk.trigrams(review))\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "            for bigram in bigrams_in_review :\n",
    "                fold_vocab[bigram] = 0\n",
    "            for trigram in trigrams_in_review :\n",
    "                fold_vocab[trigram] = 0\n",
    "                \n",
    "    \n",
    "    CondProbDict = fold_vocab\n",
    "    CondProbDict = copy.deepcopy(CondProbDict)\n",
    "\n",
    "    wordcounterPos = fold_vocab\n",
    "    wordcounterPos = copy.deepcopy(wordcounterPos)\n",
    "\n",
    "    wordcounterNeg = fold_vocab\n",
    "    wordcounterNeg = copy.deepcopy(wordcounterNeg)    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            trigrams_in_review = list(nltk.trigrams(review))\n",
    "            for word in review:\n",
    "                if word in wordcounterPos:\n",
    "                    wordcounterPos[word.lower()] += 1\n",
    "            for bigram in bigrams_in_review :\n",
    "                if bigram in wordcounterPos:\n",
    "                    wordcounterPos[bigram] += 1\n",
    "            for trigram in trigrams_in_review :\n",
    "                if trigram in wordcounterPos:\n",
    "                    wordcounterPos[trigram] += 1\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            bigrams_in_review = list(nltk.bigrams(review))\n",
    "            trigrams_in_review = list(nltk.trigrams(review))\n",
    "            for word in review:\n",
    "                if word in wordcounterNeg:\n",
    "                    wordcounterNeg[word.lower()] += 1\n",
    "            for bigram in bigrams_in_review :\n",
    "                if bigram in wordcounterNeg:\n",
    "                    wordcounterNeg[bigram] += 1\n",
    "            for trigram in trigrams_in_review :\n",
    "                if trigram in wordcounterNeg:\n",
    "                    wordcounterNeg[trigram] += 1            \n",
    "    \n",
    "    totalcountPos = sum(wordcounterPos.values())\n",
    "    totalcountNeg = sum(wordcounterNeg.values())\n",
    "    \n",
    "    print(len(wordcounterPos))\n",
    "    print(len(wordcounterNeg))\n",
    "\n",
    "    for key,value in CondProbDict.items():\n",
    "\n",
    "        if key in wordcounterPos and key in wordcounterNeg:\n",
    "            CondProbDict[key] =  [(wordcounterPos[key]+k)/(totalcountPos+len(CondProbDict))\n",
    "                                   , (wordcounterNeg[key]+k)/(totalcountNeg+len(CondProbDict))]\n",
    "\n",
    "    posPrior = len(train_pos) / (len(train_pos) + len(train_neg))\n",
    "    negPrior = len(train_neg) / (len(train_pos) + len(train_neg))\n",
    "    \n",
    "    results = []\n",
    "    labels = []\n",
    "    \n",
    "    for review in test_pos:\n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        bigrams_in_review = list(nltk.bigrams(review))\n",
    "        trigrams_in_review = list(nltk.trigrams(review))\n",
    "        for word in review:\n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "        for bigram in bigrams_in_review:\n",
    "            if bigram in CondProbDict:\n",
    "                #print(\"BIGRAM IN POS\", bigram)\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[bigram][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[bigram][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "        for trigram in trigrams_in_review:\n",
    "            if trigram in CondProbDict:\n",
    "                #print(\"BIGRAM IN POS\", bigram)\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[trigram][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[trigram][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "\n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "            all_folds_predictions_BiTrigram.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "            all_folds_predictions_BiTrigram.append(0)\n",
    "\n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for review in test_neg:\n",
    "                \n",
    "        posProb = 0\n",
    "        negProb = 0\n",
    "        bigrams_in_review = list(nltk.bigrams(review))\n",
    "        trigrams_in_review = list(nltk.trigrams(review))\n",
    "        for word in review:\n",
    "            # make sure that test word voorkomt in both dictionaries\n",
    "            if word.lower() in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[word][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[word][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "        for bigram in bigrams_in_review:\n",
    "            if bigram in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[bigram][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[bigram][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))   \n",
    "        for trigram in trigrams_in_review:\n",
    "            if trigram in CondProbDict:\n",
    "                # bepaal pos prob\n",
    "                posProb += np.log(CondProbDict[trigram][0]) \n",
    "                # bepaal neg prob\n",
    "                negProb += np.log(CondProbDict[trigram][1])\n",
    "            else:\n",
    "                posProb += np.log(1/(totalcountPos + len(CondProbDict)))\n",
    "                negProb += np.log(1/(totalcountNeg + len(CondProbDict)))\n",
    "\n",
    "        \n",
    "        posProb += np.log(posPrior)\n",
    "        negProb += np.log(negPrior)\n",
    "        if posProb > negProb:\n",
    "            results.append(1)\n",
    "                          \n",
    "        else:\n",
    "            results.append(0)\n",
    "            \n",
    "        \n",
    "        labels.append(0)\n",
    "        \n",
    "        \n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_folds_predictions_BiTrigram.append(1) \n",
    "        else:\n",
    "            all_folds_predictions_BiTrigram.append(0)\n",
    "\n",
    "    accuraciesBiTrigram.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracyBiTrigram = sum(accuraciesBiTrigram)/len(accuraciesBiTrigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy NB with bigrams and CV: \", accuracyBigram)\n",
    "print(\"Accuracy NB with bigrams and trigrams and CV: \", accuracyBiTrigram)\n",
    "print(\"-------------------------------------------\")\n",
    "print(accuraciesBigram)\n",
    "print(accuraciesBiTrigram)\n",
    "print(\"Variance of NB with Bigrams and CV: \", calc_variance(accuraciesBigram))\n",
    "print(\"Variance of NB with Bigrams and Trigrams and with CV: \", calc_variance(accuraciesBiTrigram))\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Significance test Bigram- Trigram\")\n",
    "p_value = sign_test(all_folds_predictions_Bigram, all_folds_predictions_BiTrigram)\n",
    "print(p_value)\n",
    "\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Significance test Bigram- 3.6\")\n",
    "p_value = sign_test(all_folds_predictions_Bigram, all_folds_predictions_nostem)\n",
    "print(p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrGGArkrWoL"
   },
   "source": [
    "\n",
    "#### Q3.12: How many features does the BoW model have to take into account now? (0.5pt)\n",
    "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q3.10)?\n",
    "\n",
    "Use the held-out training set once again for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEGZ9SV8pPaa"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "Theoretically, we will expect that the size of the vocabulary will be squared when comparing unigrams and bigrams, since every words could be combined with every other word, which is not the case in practice, while rules for grammar and a finite number of documents will result in a lower number of combinations. \n",
    "\n",
    "From bigrams to trigrams, the same argument can be given. However the difference is even bigger, because with three words even more limitations regarding grammar and observations in the training data will result in way less number of tokens than the size of unigrams cubed. \n",
    "\n",
    "Comparing with question 3.10, it is clear that when using stemming, the size of the vocabulary will decrease, in contrast to using bigrams and/or trigrams. When comparing unigrams, bigrams and trigrams. We observe that, in practice, the size when using bigrams is 10 times the size of the unigram-vocabulary. The size when using trigrams is almost 3 times the size of the bigram-vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_z8sAJeUrdtM"
   },
   "outputs": [],
   "source": [
    "### First calculate vocabulary using only unigrams\n",
    "\n",
    "\n",
    "vocabularyNostem = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabularyNostem[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0].lower()\n",
    "            vocabularyNostem[word] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "print(\"Length of vocabulary with only Unigrams: \",len(vocabularyNostem))\n",
    "\n",
    "###Calculate vocabulary with unigrams+bigrams\n",
    "\n",
    "vocabularyBigram = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    words_in_review = []\n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0]\n",
    "            vocabularyBigram[word] = 0\n",
    "            words_in_review.append(word)\n",
    "    bigrams_in_review = list(nltk.bigrams(words_in_review))\n",
    "    for bigram in bigrams_in_review:\n",
    "        vocabularyBigram[bigram] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    words_in_review = []\n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0]\n",
    "            vocabularyBigram[word] = 0\n",
    "            words_in_review.append(word)\n",
    "    bigrams_in_review = list(nltk.bigrams(words_in_review))\n",
    "    for bigram in bigrams_in_review:\n",
    "        vocabularyBigram[bigram] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "print(\"Length of vocabulary with Uni+Bigrams: \",len(vocabularyBigram))\n",
    "\n",
    "\n",
    "###Calculate vocabulary with unigrams+bigrams+trigrams\n",
    "\n",
    "vocabularyBiTrigram = {}\n",
    "\n",
    "\n",
    "negativeReviews = reviews[0:1000]\n",
    "positiveReviews = reviews[1000:2000]\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "for i, r in enumerate(positiveReviews):\n",
    "    text = r[\"content\"] \n",
    "    words_in_review = []\n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0]\n",
    "            vocabularyBigram[word] = 0\n",
    "            words_in_review.append(word)\n",
    "    bigrams_in_review = list(nltk.bigrams(words_in_review))\n",
    "    trigrams_in_review =list(nltk.trigrams(words_in_review))\n",
    "    for bigram in bigrams_in_review:\n",
    "        vocabularyBiTrigram[bigram] = 0\n",
    "    for trigram in  trigrams_in_review:\n",
    "        vocabularyBiTrigram[trigram] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "\n",
    "# fill vocab with all unique words so this is a dict with all the words in the reviews\n",
    "for i, r in enumerate(negativeReviews):\n",
    "    text = r[\"content\"] \n",
    "    words_in_review = []\n",
    "    for x in text:\n",
    "        for a in x:\n",
    "            word = a[0]\n",
    "            vocabularyBigram[word] = 0\n",
    "            words_in_review.append(word)\n",
    "    bigrams_in_review = list(nltk.bigrams(words_in_review))\n",
    "    trigrams_in_review =list(nltk.trigrams(words_in_review))\n",
    "    for bigram in bigrams_in_review:\n",
    "        vocabularyBiTrigram[bigram] = 0\n",
    "    for trigram in  trigrams_in_review:\n",
    "        vocabularyBiTrigram[trigram] = 0\n",
    "    if i == 899:\n",
    "        break\n",
    "        \n",
    "print(\"Length of vocabulary with Uni+Bi+Trigrams: \",len(vocabularyBiTrigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHWKDL3YV6vh"
   },
   "source": [
    "# Support Vector Machines (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJSYhcVaoJGt"
   },
   "source": [
    "Though simple to understand, implement, and debug, one\n",
    "major problem with the Naive Bayes classifier is that its performance\n",
    "deteriorates (becomes skewed) when it is being used with features which\n",
    "are not independent (i.e., are correlated). Another popular classifier\n",
    "that doesn’t scale as well to big data, and is not as simple to debug as\n",
    "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
    "Vector Machine (SVM) classifier.\n",
    "\n",
    "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
    "Other sources for learning SVM:\n",
    "* http://web.mit.edu/zoya/www/SVM.pdf\n",
    "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
    "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Use the scikit-learn implementation of \n",
    "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LnzNtQBV8gr"
   },
   "source": [
    "#### (Q4.1): Train SVM and compare to Naive Bayes (2pts)\n",
    "\n",
    "Train an SVM classifier (sklearn.svm.LinearSVC) using your features. Compare the\n",
    "classification performance of the SVM classifier to that of the Naive\n",
    "Bayes classifier with smoothing from (Q3.6) and report the numbers.\n",
    "Perform cross-validation and concatenate the predictions from all folds to compute the significance.  Are the results significantly better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "JBscui8Mvoz0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N FOLDDDDD 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                         | 0/9 [00:01<?, ?it/s]\n",
      "  0%|                                                                                                                                                                                        | 0/10 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-7ae4e0faf6d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mbag_of_words_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_nil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_nil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############### NB without stemming and with CV\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "\n",
    "# Set k for smoothing\n",
    "\n",
    "k = 1\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds(positiveReviews, negativeReviews, False, False)\n",
    "    \n",
    "accuracies_svm = []\n",
    "\n",
    "all_fold_predictions_svm = []\n",
    "\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(10)):\n",
    "    print(\"N FOLDDDDD\", n_fold)\n",
    "    \n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            \n",
    "        \n",
    "            \n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "        \n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    " \n",
    "    features_train_data = []\n",
    "    features_train_labels = []\n",
    "\n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in tqdm(train_pos):\n",
    "        \n",
    "#       maak per review de bag of words weer leeg\n",
    "        for review in fold:\n",
    "            \n",
    "            bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "            \n",
    "            for word in review:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "                \n",
    "            features_train_data.append(list(bag_of_words_dict.values()))\n",
    "            features_train_labels.append(1)\n",
    "       \n",
    "            \n",
    "            \n",
    "    for fold in tqdm(train_neg):\n",
    "        for review in fold:\n",
    "            \n",
    "            \n",
    "            bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "            \n",
    "            for word in review:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "                \n",
    "            features_train_data.append(list(bag_of_words_dict.values()))\n",
    "            features_train_labels.append(0)      \n",
    "        \n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(features_train_data, features_train_labels)\n",
    "\n",
    "    results = []\n",
    "    labels = []  \n",
    "    \n",
    "    for review in tqdm(test_pos):     \n",
    "            \n",
    "        bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "\n",
    "        for word in review:\n",
    "            \n",
    "            if word.lower() in bag_of_words_dict:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "        \n",
    "        prediction = clf.predict(np.array(list(bag_of_words_dict.values())) .reshape(1, -1)   )\n",
    "        results.append(prediction[0])\n",
    "        \n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for review in tqdm(test_neg):     \n",
    "            \n",
    "        bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "\n",
    "        for word in review:\n",
    "            \n",
    "            if word.lower() in bag_of_words_dict:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "        \n",
    "        \n",
    "        prediction = clf.predict(np.array( list(bag_of_words_dict.values())) .reshape(1, -1))\n",
    "        results.append(prediction[0])\n",
    "        \n",
    "        labels.append(0)\n",
    "\n",
    "    \n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_fold_predictions_svm.append(1)\n",
    "        else:\n",
    "            all_fold_predictions_svm.append(0)\n",
    "\n",
    "    accuracies_svm.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracySVM = sum(accuracies_svm)/len(accuracies_svm)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracySVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy NB with smooting and CV: \", accuracy_nb_smooting)\n",
    "print(\"Accuracy SVM: \", accuracySVM)\n",
    "print(\"-------------------------------------------\")\n",
    "print(accuracies_nb_smooting)\n",
    "print(accuracies_svm)\n",
    "\n",
    "print(\"Significance test NB smoothing and cv VS SVM\")\n",
    "p_value = sign_test(all_folds_nb_smooting, all_fold_predictions_svm)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifXVWcK0V9qY"
   },
   "source": [
    "### POS disambiguation (2pts)\n",
    "\n",
    "Now add in part-of-speech features. You will find the\n",
    "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
    "replicate the results obtained by Pang et al. (2002).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA3I82o4oWGu"
   },
   "source": [
    "####(Q4.2) Replace your features with word+POS features, and report performance with the SVM. Does this help? Perform cross-validation and concatenate the predictions from all folds to compute the significance. Are the results significant? *Why?*  (1pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOvjYe-t2Br6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                         | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N FOLDDDDD 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                         | 0/9 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|███████████████████▋                                                                                                                                                             | 1/9 [00:05<00:43,  5.44s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|███████████████████████████████████████▎                                                                                                                                         | 2/9 [00:11<00:38,  5.54s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███████████████████████████████████████████████████████████                                                                                                                      | 3/9 [00:16<00:33,  5.56s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████████████████████████████████████████████████████████████████▋                                                                                                  | 4/9 [00:22<00:27,  5.50s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                              | 5/9 [00:27<00:21,  5.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                           | 6/9 [00:32<00:16,  5.42s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 7/9 [00:38<00:10,  5.43s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############### NB without stemming and with CV\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "\n",
    "# Set k for smoothing\n",
    "\n",
    "k = 1\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds(positiveReviews, negativeReviews, False, pos = True)\n",
    "    \n",
    "accuracies_svm_pos = []\n",
    "\n",
    "all_fold_predictions_svm_pos = []\n",
    "\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(1)):\n",
    "    print(\"N FOLDDDDD\", n_fold)\n",
    "    \n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            \n",
    "            for word in review:\n",
    "            \n",
    "                fold_vocab[word[0].lower()+word[1]] = 0\n",
    "          \n",
    "        \n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word[0].lower()+word[1]] = 0\n",
    "\n",
    "    features_train_data = []\n",
    "    features_train_labels = []\n",
    "\n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in tqdm(train_pos):\n",
    "        \n",
    "#       maak per review de bag of words weer leeg\n",
    "        for review in fold:\n",
    "            \n",
    "            bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "            \n",
    "            for word in review:\n",
    "                bag_of_words_dict[word[0].lower()+word[1]] = 1\n",
    "         \n",
    "                \n",
    "                \n",
    "                #ADD POS TAGS\n",
    "                \n",
    "            features_train_data.append(list(bag_of_words_dict.values()))\n",
    "            features_train_labels.append(1)\n",
    "                  \n",
    "    for fold in tqdm(train_neg):\n",
    "        for review in fold:\n",
    "            \n",
    "            \n",
    "            bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "            \n",
    "            for word in review:\n",
    "                bag_of_words_dict[word[0].lower()+word[1]] = 1\n",
    "                \n",
    "            features_train_data.append(list(bag_of_words_dict.values()))\n",
    "            features_train_labels.append(0)\n",
    "            \n",
    "        \n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(features_train_data, features_train_labels)\n",
    "\n",
    "    results = []\n",
    "    labels = []\n",
    "\n",
    "    \n",
    "    \n",
    "    for review in tqdm(test_pos):     \n",
    "            \n",
    "        bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "\n",
    "        for word in review:\n",
    "            \n",
    "            if word[0].lower()+word[1] in bag_of_words_dict:\n",
    "                bag_of_words_dict[word[0].lower()+word[1]] = 1\n",
    "\n",
    "        \n",
    "        prediction = clf.predict(np.array(list(bag_of_words_dict.values())) .reshape(1, -1)   )\n",
    "        results.append(prediction[0])\n",
    "\n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for review in tqdm(test_neg):     \n",
    "            \n",
    "        bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "\n",
    "        for word in review:\n",
    "            \n",
    "            if word[0].lower()+word[1] in bag_of_words_dict:\n",
    "                bag_of_words_dict[word[0].lower()+word[1]] = 1\n",
    "        \n",
    "        \n",
    "        prediction = clf.predict(np.array( list(bag_of_words_dict.values())) .reshape(1, -1))\n",
    "        results.append(prediction[0])\n",
    "        labels.append(0)\n",
    "\n",
    "    \n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_fold_predictions_svm_pos.append(1)\n",
    "        else:\n",
    "            all_fold_predictions_svm_pos.append(0)\n",
    "\n",
    "    accuracies_svm_pos.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracySVM_pos = sum(accuracies_svm_pos)/len(accuracies_svm_pos)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'everyDT': 0,\n",
       " 'nowRB': 0,\n",
       " 'andCC': 0,\n",
       " 'thenRB': 0,\n",
       " 'aDT': 0,\n",
       " 'movieNN': 0,\n",
       " 'comesVBZ': 0,\n",
       " 'alongIN': 0,\n",
       " 'fromIN': 0,\n",
       " 'suspectJJ': 0,\n",
       " 'studioNN': 0,\n",
       " ',,': 0,\n",
       " 'withIN': 0,\n",
       " 'indicationNN': 0,\n",
       " 'thatIN': 0,\n",
       " 'itPRP': 0,\n",
       " 'willMD': 0,\n",
       " 'beVB': 0,\n",
       " 'stinkerNN': 0,\n",
       " 'toTO': 0,\n",
       " 'everybodyNN': 0,\n",
       " \"'sPOS\": 0,\n",
       " 'surpriseNN': 0,\n",
       " '-lrb--LRB-': 0,\n",
       " 'perhapsRB': 0,\n",
       " 'evenRB': 0,\n",
       " 'theDT': 0,\n",
       " '-rrb--RRB-': 0,\n",
       " 'filmNN': 0,\n",
       " 'becomesVBZ': 0,\n",
       " 'criticalJJ': 0,\n",
       " 'darlingNN': 0,\n",
       " '..': 0,\n",
       " 'mtvNNP': 0,\n",
       " 'filmsNNS': 0,\n",
       " \"'POS\": 0,\n",
       " '_CD': 0,\n",
       " 'electionNN': 0,\n",
       " 'highJJ': 0,\n",
       " 'schoolNN': 0,\n",
       " 'comedyNN': 0,\n",
       " 'starringVBG': 0,\n",
       " 'matthewNNP': 0,\n",
       " 'broderickNNP': 0,\n",
       " 'reeseNNP': 0,\n",
       " 'witherspoonNNP': 0,\n",
       " 'isVBZ': 0,\n",
       " 'currentJJ': 0,\n",
       " 'exampleNN': 0,\n",
       " 'didVBD': 0,\n",
       " 'anybodyNN': 0,\n",
       " 'knowVB': 0,\n",
       " 'thisDT': 0,\n",
       " 'existedVBD': 0,\n",
       " 'weekNN': 0,\n",
       " 'beforeIN': 0,\n",
       " 'openedVBD': 0,\n",
       " '?.': 0,\n",
       " 'plotNN': 0,\n",
       " 'deceptivelyRB': 0,\n",
       " 'simpleJJ': 0,\n",
       " 'georgeNNP': 0,\n",
       " 'washingtonNNP': 0,\n",
       " 'carverNNP': 0,\n",
       " 'highNNP': 0,\n",
       " 'schoolNNP': 0,\n",
       " 'havingVBG': 0,\n",
       " 'studentNN': 0,\n",
       " 'electionsNNS': 0,\n",
       " 'tracyNNP': 0,\n",
       " 'flickNN': 0,\n",
       " 'anDT': 0,\n",
       " 'over-achieverNN': 0,\n",
       " 'herPRP$': 0,\n",
       " 'handNN': 0,\n",
       " 'raisedVBD': 0,\n",
       " 'atIN': 0,\n",
       " 'nearlyRB': 0,\n",
       " 'questionNN': 0,\n",
       " 'wayNN': 0,\n",
       " 'mr.NNP': 0,\n",
       " '````': 0,\n",
       " 'mNN': 0,\n",
       " \"''''\": 0,\n",
       " 'sickJJ': 0,\n",
       " 'ofIN': 0,\n",
       " 'megalomaniacNN': 0,\n",
       " 'encouragesVBZ': 0,\n",
       " 'paulNNP': 0,\n",
       " 'popular-but-slowJJ': 0,\n",
       " 'jockNN': 0,\n",
       " 'runVB': 0,\n",
       " 'nihilisticJJ': 0,\n",
       " 'sisterNN': 0,\n",
       " 'jumpsVBZ': 0,\n",
       " 'inIN': 0,\n",
       " 'raceNN': 0,\n",
       " 'asRB': 0,\n",
       " 'wellRB': 0,\n",
       " 'forIN': 0,\n",
       " 'personalJJ': 0,\n",
       " 'reasonsNNS': 0,\n",
       " 'darkJJ': 0,\n",
       " 'sideNN': 0,\n",
       " 'suchJJ': 0,\n",
       " 'sleeperNN': 0,\n",
       " 'successNN': 0,\n",
       " 'becauseIN': 0,\n",
       " 'expectationsNNS': 0,\n",
       " 'wereVBD': 0,\n",
       " 'soRB': 0,\n",
       " 'lowJJ': 0,\n",
       " 'goingVBG': 0,\n",
       " 'factNN': 0,\n",
       " 'wasVBD': 0,\n",
       " 'qualityNN': 0,\n",
       " 'stuffNN': 0,\n",
       " 'madeVBD': 0,\n",
       " 'reviewsNNS': 0,\n",
       " 'moreRBR': 0,\n",
       " 'enthusiasticJJ': 0,\n",
       " 'thanIN': 0,\n",
       " 'theyPRP': 0,\n",
       " 'haveVBP': 0,\n",
       " 'anyDT': 0,\n",
       " 'rightNN': 0,\n",
       " 'youPRP': 0,\n",
       " 'caMD': 0,\n",
       " \"n'tRB\": 0,\n",
       " 'helpVB': 0,\n",
       " 'baggageNN': 0,\n",
       " 'glowingJJ': 0,\n",
       " 'whichWDT': 0,\n",
       " 'contrastNN': 0,\n",
       " 'negativeJJ': 0,\n",
       " 'reviewersNNS': 0,\n",
       " 'likelyJJ': 0,\n",
       " 'haveVB': 0,\n",
       " 'goodJJ': 0,\n",
       " 'doesVBZ': 0,\n",
       " 'notRB': 0,\n",
       " 'liveVB': 0,\n",
       " 'upRP': 0,\n",
       " 'itsPRP$': 0,\n",
       " 'hypeNN': 0,\n",
       " 'whatWP': 0,\n",
       " 'makesVBZ': 0,\n",
       " '_NN': 0,\n",
       " 'disappointingJJ': 0,\n",
       " 'containsVBZ': 0,\n",
       " 'significantJJ': 0,\n",
       " 'detailsNNS': 0,\n",
       " 'liftedVBN': 0,\n",
       " 'directlyRB': 0,\n",
       " 'rushmoreNN': 0,\n",
       " 'releasedVBD': 0,\n",
       " 'fewJJ': 0,\n",
       " 'monthsNNS': 0,\n",
       " 'earlierRBR': 0,\n",
       " 'similaritiesNNS': 0,\n",
       " 'areVBP': 0,\n",
       " 'staggeringVBG': 0,\n",
       " '::': 0,\n",
       " 'presidentNN': 0,\n",
       " 'extraordinaryJJ': 0,\n",
       " 'numberNN': 0,\n",
       " 'clubsNNS': 0,\n",
       " 'involvedVBN': 0,\n",
       " 'playNN': 0,\n",
       " 'maxNNP': 0,\n",
       " 'fischerNNP': 0,\n",
       " 'mostRBS': 0,\n",
       " 'tensionNN': 0,\n",
       " 'potentialJJ': 0,\n",
       " 'relationshipNN': 0,\n",
       " 'betweenIN': 0,\n",
       " 'teacherNN': 0,\n",
       " 'hisPRP$': 0,\n",
       " 'singleJJ': 0,\n",
       " 'parentNN': 0,\n",
       " 'homeNN': 0,\n",
       " 'hasVBZ': 0,\n",
       " 'contributedVBN': 0,\n",
       " 'driveNN': 0,\n",
       " 'maleJJ': 0,\n",
       " 'bumblingVBG': 0,\n",
       " 'adultJJ': 0,\n",
       " 'pursuesVBZ': 0,\n",
       " 'extramaritalJJ': 0,\n",
       " 'affairNN': 0,\n",
       " 'getsVBZ': 0,\n",
       " 'caughtVBN': 0,\n",
       " 'wholeJJ': 0,\n",
       " 'lifeNN': 0,\n",
       " 'ruinedVBN': 0,\n",
       " 'hePRP': 0,\n",
       " 'beeNN': 0,\n",
       " 'stingNN': 0,\n",
       " 'billNNP': 0,\n",
       " 'murrayNNP': 0,\n",
       " 'severalJJ': 0,\n",
       " 'stingsNNS': 0,\n",
       " 'onIN': 0,\n",
       " 'happenedVBD': 0,\n",
       " 'howWRB': 0,\n",
       " 'individualJJ': 0,\n",
       " 'screenplayNN': 0,\n",
       " 'novelJJ': 0,\n",
       " 'containVBP': 0,\n",
       " 'manyJJ': 0,\n",
       " 'pointsNNS': 0,\n",
       " 'yetRB': 0,\n",
       " 'bothDT': 0,\n",
       " 'probablyRB': 0,\n",
       " 'awareJJ': 0,\n",
       " 'eachDT': 0,\n",
       " 'otherJJ': 0,\n",
       " 'madeVBN': 0,\n",
       " 'twoCD': 0,\n",
       " 'differentJJ': 0,\n",
       " 'studiosNNS': 0,\n",
       " 'genreNN': 0,\n",
       " 'geeksNNS': 0,\n",
       " 'revengeVBP': 0,\n",
       " 'thatWDT': 0,\n",
       " 'hadVBD': 0,\n",
       " 'beenVBN': 0,\n",
       " 'fullyRB': 0,\n",
       " 'formedVBN': 0,\n",
       " 'strengthsNNS': 0,\n",
       " 'relyVBP': 0,\n",
       " 'uponIN': 0,\n",
       " 'fantasticJJ': 0,\n",
       " 'performancesNNS': 0,\n",
       " 'newcomerNN': 0,\n",
       " 'jessicaNNP': 0,\n",
       " 'campbellNNP': 0,\n",
       " 'asIN': 0,\n",
       " 'anti-socialJJ': 0,\n",
       " 'tammyNNP': 0,\n",
       " 'hereRB': 0,\n",
       " 'playingVBG': 0,\n",
       " 'rooneyNNP': 0,\n",
       " 'roleNN': 0,\n",
       " 'ferrisNNP': 0,\n",
       " 'buellerNNP': 0,\n",
       " '_NNP': 0,\n",
       " 'seemsVBZ': 0,\n",
       " 'mostJJS': 0,\n",
       " 'funNN': 0,\n",
       " \"'sVBZ\": 0,\n",
       " 'hadVBN': 0,\n",
       " 'sinceIN': 0,\n",
       " 'revelationNN': 0,\n",
       " 'earlyJJ': 0,\n",
       " 'yearNN': 0,\n",
       " 'teenagersNNS': 0,\n",
       " 'littleJJ': 0,\n",
       " 'cloutNN': 0,\n",
       " 'butCC': 0,\n",
       " 'myPRP$': 0,\n",
       " 'moneyNN': 0,\n",
       " 'deservesVBZ': 0,\n",
       " 'oscarNNP': 0,\n",
       " 'nominationNN': 0,\n",
       " 'onceRB': 0,\n",
       " 'characterNN': 0,\n",
       " 'likeIN': 0,\n",
       " 'speechNN': 0,\n",
       " 'gymnasiumNN': 0,\n",
       " \"'reVBP\": 0,\n",
       " 'wonVBN': 0,\n",
       " 'overRP': 0,\n",
       " 'oneCD': 0,\n",
       " 'thingNN': 0,\n",
       " 'botheringVBG': 0,\n",
       " 'mePRP': 0,\n",
       " 'iPRP': 0,\n",
       " \"'veVBP\": 0,\n",
       " 'seenVBN': 0,\n",
       " 'thereEX': 0,\n",
       " 'amountNN': 0,\n",
       " 'sexualityNN': 0,\n",
       " 'supposeVBP': 0,\n",
       " 'comingVBG': 0,\n",
       " 'shouldMD': 0,\n",
       " 'expectVB': 0,\n",
       " 'noDT': 0,\n",
       " 'lessJJR': 0,\n",
       " '...:': 0,\n",
       " 'startsVBZ': 0,\n",
       " 'offRP': 0,\n",
       " 'lightJJ': 0,\n",
       " 'airyJJ': 0,\n",
       " 'sitcomNN': 0,\n",
       " 'screwsNNS': 0,\n",
       " 'tightenVBP': 0,\n",
       " 'tensionsNNS': 0,\n",
       " 'mountVBP': 0,\n",
       " 'alexanderNNP': 0,\n",
       " 'payneNNP': 0,\n",
       " 'decidesVBZ': 0,\n",
       " 'addVB': 0,\n",
       " 'elementsNNS': 0,\n",
       " 'franklyRB': 0,\n",
       " 'distractVB': 0,\n",
       " 'storyNN': 0,\n",
       " 'badJJ': 0,\n",
       " 'enoughRB': 0,\n",
       " 'mNNP': 0,\n",
       " 'likeVB': 0,\n",
       " 'determinationNN': 0,\n",
       " 'winVB': 0,\n",
       " 'allDT': 0,\n",
       " 'costsNNS': 0,\n",
       " 'throwVB': 0,\n",
       " 'student/teacherNN': 0,\n",
       " 'logicalJJ': 0,\n",
       " 'reasonNN': 0,\n",
       " 'whyWRB': 0,\n",
       " 'whenWRB': 0,\n",
       " 'lotNN': 0,\n",
       " '_VB': 0,\n",
       " 'rushmoreNNP': 0,\n",
       " 'tonalJJ': 0,\n",
       " 'nosediveJJ': 0,\n",
       " 'takesVBZ': 0,\n",
       " 'explicitlyRB': 0,\n",
       " 'sex-drivenJJ': 0,\n",
       " 'markNN': 0,\n",
       " 'disappointmentNN': 0,\n",
       " 'noticedVBN': 0,\n",
       " 'somethingNN': 0,\n",
       " 'latelyRB': 0,\n",
       " 'neverRB': 0,\n",
       " 'thoughtVBN': 0,\n",
       " 'beforeRB': 0,\n",
       " 'pseudo-substance-hollywoodJJ': 0,\n",
       " 'fakingVBG': 0,\n",
       " 'deepJJ': 0,\n",
       " 'meaningsNNS': 0,\n",
       " 'theirPRP$': 0,\n",
       " 'everRB': 0,\n",
       " 'reallyRB': 0,\n",
       " 'enjoyedVBD': 0,\n",
       " 'lookVBP': 0,\n",
       " 'backRB': 0,\n",
       " 'realizeVBP': 0,\n",
       " 'missingVBG': 0,\n",
       " 'moreJJR': 0,\n",
       " 'filmmakersNNS': 0,\n",
       " 'seemVBP': 0,\n",
       " 'puttingVBG': 0,\n",
       " 'outRP': 0,\n",
       " 'rehearsedVBN': 0,\n",
       " 'melodramaticJJ': 0,\n",
       " 'evokeVBP': 0,\n",
       " 'strongJJ': 0,\n",
       " 'connotationsNNS': 0,\n",
       " 'beingVBG': 0,\n",
       " 'greatJJ': 0,\n",
       " 'ifIN': 0,\n",
       " 'stepVB': 0,\n",
       " 'asideRB': 0,\n",
       " 'reflectVB': 0,\n",
       " 'yourPRP$': 0,\n",
       " 'experienceNN': 0,\n",
       " 'mayMD': 0,\n",
       " 'justRB': 0,\n",
       " 'discoverVB': 0,\n",
       " 'nothingNN': 0,\n",
       " 'elegantlyRB': 0,\n",
       " 'presentedVBN': 0,\n",
       " 'fluffNN': 0,\n",
       " \"'mVBP\": 0,\n",
       " 'tryingVBG': 0,\n",
       " 'sayVB': 0,\n",
       " 'cityNNP': 0,\n",
       " 'angelsNNP': 0,\n",
       " 'somewhereRB': 0,\n",
       " 'falteredVBD': 0,\n",
       " 'somehowRB': 0,\n",
       " 'underneathIN': 0,\n",
       " 'seeminglyRB': 0,\n",
       " 'poeticJJ': 0,\n",
       " 'beautyNN': 0,\n",
       " 'giganticJJ': 0,\n",
       " 'holeNN': 0,\n",
       " 'somebodyNN': 0,\n",
       " 'coveredVBN': 0,\n",
       " 'iridescentJJ': 0,\n",
       " 'glossyJJ': 0,\n",
       " 'cinematographyNN': 0,\n",
       " 'predictableJJ': 0,\n",
       " 'endingVBG': 0,\n",
       " 'shatteredVBN': 0,\n",
       " 'ourPRP$': 0,\n",
       " 'hopesNNS': 0,\n",
       " 'thoughIN': 0,\n",
       " 'wePRP': 0,\n",
       " 'sawVBD': 0,\n",
       " 'onlyRB': 0,\n",
       " 'addedVBD': 0,\n",
       " 'overIN': 0,\n",
       " 'hourNN': 0,\n",
       " 'worthIN': 0,\n",
       " 'timeNN': 0,\n",
       " 'nicolasNNP': 0,\n",
       " 'cageNNP': 0,\n",
       " 'sethNNP': 0,\n",
       " 'guardianNN': 0,\n",
       " 'angelNN': 0,\n",
       " 'whoWP': 0,\n",
       " 'hundredsNNS': 0,\n",
       " 'thousandsNNS': 0,\n",
       " 'orCC': 0,\n",
       " 'millionsNNS': 0,\n",
       " 'angelsNNS': 0,\n",
       " 'spendsVBZ': 0,\n",
       " 'eternityNN': 0,\n",
       " 'watchingVBG': 0,\n",
       " 'citizensNNS': 0,\n",
       " 'mortalityNN': 0,\n",
       " ';:': 0,\n",
       " 'humansNNS': 0,\n",
       " 'muchJJ': 0,\n",
       " 'celestialJJ': 0,\n",
       " 'interventionNN': 0,\n",
       " 'occursVBZ': 0,\n",
       " 'megNNP': 0,\n",
       " 'ryanNNP': 0,\n",
       " 'subduedVBN': 0,\n",
       " 'performanceNN': 0,\n",
       " 'playsVBZ': 0,\n",
       " 'maggieNNP': 0,\n",
       " 'doctorNN': 0,\n",
       " 'beginsVBZ': 0,\n",
       " 'ponderVB': 0,\n",
       " 'exactlyRB': 0,\n",
       " 'fightingVBG': 0,\n",
       " 'againstIN': 0,\n",
       " 'fightVBP': 0,\n",
       " 'keepVB': 0,\n",
       " 'someoneNN': 0,\n",
       " 'aliveJJ': 0,\n",
       " 'afterIN': 0,\n",
       " 'losingVBG': 0,\n",
       " 'patientNN': 0,\n",
       " 'surgeryNN': 0,\n",
       " 'tableNN': 0,\n",
       " 'theseDT': 0,\n",
       " 'questionsNNS': 0,\n",
       " 'envelopeNN': 0,\n",
       " 'thereRB': 0,\n",
       " 'overseeVB': 0,\n",
       " 'transitionNN': 0,\n",
       " 'intoIN': 0,\n",
       " 'afterlifeNN': 0,\n",
       " 'immediatelyRB': 0,\n",
       " 'captivatedVBN': 0,\n",
       " 'byIN': 0,\n",
       " 'followingVBG': 0,\n",
       " 'observingVBG': 0,\n",
       " 'fallingVBG': 0,\n",
       " 'loveNN': 0,\n",
       " 'everydayNN': 0,\n",
       " 'angelsNNPS': 0,\n",
       " 'quicklyRB': 0,\n",
       " 'learnVBP': 0,\n",
       " 'canMD': 0,\n",
       " 'experienceVB': 0,\n",
       " 'humanJJ': 0,\n",
       " 'sensationsNNS': 0,\n",
       " 'tasteNN': 0,\n",
       " 'touchNN': 0,\n",
       " 'doVBP': 0,\n",
       " 'abilityNN': 0,\n",
       " 'makeVB': 0,\n",
       " 'themselvesPRP': 0,\n",
       " 'anyoneNN': 0,\n",
       " 'desireVBP': 0,\n",
       " 'adorationNN': 0,\n",
       " 'tooRB': 0,\n",
       " 'resistVB': 0,\n",
       " 'eventuallyRB': 0,\n",
       " 'appearVB': 0,\n",
       " 'quiteRB': 0,\n",
       " 'regularlyRB': 0,\n",
       " 'althoughIN': 0,\n",
       " 'suchPDT': 0,\n",
       " 'tabooJJ': 0,\n",
       " 'amongIN': 0,\n",
       " 'angelicJJ': 0,\n",
       " 'communityNN': 0,\n",
       " 'interestinglyRB': 0,\n",
       " 'themPRP': 0,\n",
       " 'dressedVBN': 0,\n",
       " 'blackJJ': 0,\n",
       " 'reminiscentJJ': 0,\n",
       " 'hitmenNN': 0,\n",
       " 'traditionalJJ': 0,\n",
       " 'whiteJJ': 0,\n",
       " 'entitiesNNS': 0,\n",
       " 'niceJJ': 0,\n",
       " 'mereJJ': 0,\n",
       " 'attemptNN': 0,\n",
       " 'uniquenessNN': 0,\n",
       " 'wonderfullyRB': 0,\n",
       " 'versatileJJ': 0,\n",
       " 'actorNN': 0,\n",
       " 'thinkVB': 0,\n",
       " 'face/offNN': 0,\n",
       " 'raisingVBG': 0,\n",
       " 'arizonaNNP': 0,\n",
       " 'couldNNP': 0,\n",
       " 'happenNNP': 0,\n",
       " 'you-whatNNP': 0,\n",
       " 'comboNN': 0,\n",
       " '!.': 0,\n",
       " 'slipsVBZ': 0,\n",
       " 'heavenlyJJ': 0,\n",
       " 'agentNN': 0,\n",
       " 'nicelyRB': 0,\n",
       " 'threatensVBZ': 0,\n",
       " 'sappinessNN': 0,\n",
       " 'seeVB': 0,\n",
       " 'pickVB': 0,\n",
       " 'rolesNNS': 0,\n",
       " 'courageNNP': 0,\n",
       " 'underIN': 0,\n",
       " 'fireNN': 0,\n",
       " 'veryRB': 0,\n",
       " 'comparableJJ': 0,\n",
       " 'deviateVBP': 0,\n",
       " 'usualJJ': 0,\n",
       " 'intelligentlyRB': 0,\n",
       " 'ditzyJJ': 0,\n",
       " 'romanticJJ': 0,\n",
       " 'impressiveJJ': 0,\n",
       " 'goersNNS': 0,\n",
       " 'rarelyRB': 0,\n",
       " 'chanceNN': 0,\n",
       " 'enjoyVB': 0,\n",
       " 'leadsNNS': 0,\n",
       " 'jobNN': 0,\n",
       " 'dennisNNP': 0,\n",
       " 'franzNNP': 0,\n",
       " 'grabsVBZ': 0,\n",
       " 'usPRP': 0,\n",
       " 'interpretationNN': 0,\n",
       " 'hospitalNN': 0,\n",
       " 'knowsVBZ': 0,\n",
       " 'meetsVBZ': 0,\n",
       " 'eyeNN': 0,\n",
       " 'shameNN': 0,\n",
       " 'faltersVBZ': 0,\n",
       " 'finalJJ': 0,\n",
       " 'stagesNNS': 0,\n",
       " 'leavingVBG': 0,\n",
       " 'realizationNN': 0,\n",
       " 'emotionallyRB': 0,\n",
       " 'incredibleJJ': 0,\n",
       " 'outIN': 0,\n",
       " 'getVB': 0,\n",
       " 'struggleVBP': 0,\n",
       " 'impactingVBG': 0,\n",
       " 'conclusionNN': 0,\n",
       " 'windNN': 0,\n",
       " 'painfulJJ': 0,\n",
       " 'thudNN': 0,\n",
       " 'ratherRB': 0,\n",
       " 'exhilaratingJJ': 0,\n",
       " 'impressionNN': 0,\n",
       " 'lingerVBP': 0,\n",
       " 'rememberVB': 0,\n",
       " 'conveyVB': 0,\n",
       " 'othersNNS': 0,\n",
       " 'thruIN': 0,\n",
       " 'wordNN': 0,\n",
       " 'mouthNN': 0,\n",
       " 'tellingVBG': 0,\n",
       " '60CD': 0,\n",
       " '+CC': 0,\n",
       " 'minutesNNS': 0,\n",
       " 'gloriousJJ': 0,\n",
       " 'masterpieceNN': 0,\n",
       " 'sureRB': 0,\n",
       " 'leaveVBP': 0,\n",
       " 'dishearteningJJ': 0,\n",
       " 'mediocrityNN': 0,\n",
       " 'mouthsNNS': 0,\n",
       " 'basedVBN': 0,\n",
       " 'germanJJ': 0,\n",
       " 'wingsNNS': 0,\n",
       " 'desireNN': 0,\n",
       " 'englishNNP': 0,\n",
       " 'titleNN': 0,\n",
       " 'courseNN': 0,\n",
       " 'cityNN': 0,\n",
       " 'ninetyCD': 0,\n",
       " 'percentNN': 0,\n",
       " 'peopleNNS': 0,\n",
       " 'forgiveVBP': 0,\n",
       " 'shortcomingsNNS': 0,\n",
       " 'devastatinglyRB': 0,\n",
       " 'non-cynicsJJ': 0,\n",
       " 'anywayRB': 0,\n",
       " 'wrappedVBN': 0,\n",
       " 'surrealJJ': 0,\n",
       " 'atmosphereNN': 0,\n",
       " 'giveVB': 0,\n",
       " 'criticismNN': 0,\n",
       " 'thatDT': 0,\n",
       " 'needsVBZ': 0,\n",
       " 'criticizedVBN': 0,\n",
       " 'nonethelessRB': 0,\n",
       " 'beautifullyRB': 0,\n",
       " 'captivatingJJ': 0,\n",
       " 'satisfyVB': 0,\n",
       " 'thoseDT': 0,\n",
       " 'viewersNNS': 0,\n",
       " 'appreciateVB': 0,\n",
       " 'delveNN': 0,\n",
       " 'richJJ': 0,\n",
       " 'emotionalJJ': 0,\n",
       " 'territoriesNNS': 0,\n",
       " 'onePRP': 0,\n",
       " 'observeVB': 0,\n",
       " 'starNNP': 0,\n",
       " 'trekNNP': 0,\n",
       " 'expectVBP': 0,\n",
       " 'seriousJJ': 0,\n",
       " 'scienceNN': 0,\n",
       " 'fictionNN': 0,\n",
       " 'purposeNN': 0,\n",
       " 'provideVB': 0,\n",
       " 'flashyJJ': 0,\n",
       " 'innocentJJ': 0,\n",
       " 'sometimesRB': 0,\n",
       " 'storiesNNS': 0,\n",
       " 'compellingJJ': 0,\n",
       " 'exceptionNN': 0,\n",
       " 'firstJJ': 0,\n",
       " 'seriesNN': 0,\n",
       " 'providesVBZ': 0,\n",
       " 'littleRB': 0,\n",
       " 'endlessJJ': 0,\n",
       " 'shotsNNS': 0,\n",
       " 'amazedVBN': 0,\n",
       " 'facesNNS': 0,\n",
       " 'boredVBN': 0,\n",
       " 'enterpriseNNP': 0,\n",
       " 'numerousJJ': 0,\n",
       " 'missionsNNS': 0,\n",
       " 'insurrectionNNP': 0,\n",
       " 'gottenVBN': 0,\n",
       " 'someDT': 0,\n",
       " 'friendNN': 0,\n",
       " 'mineJJ': 0,\n",
       " 'actuallyRB': 0,\n",
       " 'thinksVBZ': 0,\n",
       " 'worstJJS': 0,\n",
       " 'sureJJ': 0,\n",
       " 'excitingJJ': 0,\n",
       " 'oftenRB': 0,\n",
       " 'hilariousJJ': 0,\n",
       " 'engagedVBD': 0,\n",
       " 'leftVBD': 0,\n",
       " 'readyJJ': 0,\n",
       " 'nextJJ': 0,\n",
       " 'sayVBP': 0,\n",
       " 'bitNN': 0,\n",
       " 'longJJ': 0,\n",
       " 'episodeNN': 0,\n",
       " 'specialJJ': 0,\n",
       " 'effectsNNS': 0,\n",
       " 'cheesyJJ': 0,\n",
       " 'boringJJ': 0,\n",
       " 'simplyRB': 0,\n",
       " 'secondJJ': 0,\n",
       " 'featureVB': 0,\n",
       " 'strictlyRB': 0,\n",
       " 'generationNNP': 0,\n",
       " 'castNN': 0,\n",
       " 'introducesVBZ': 0,\n",
       " 'calledVBD': 0,\n",
       " 'baNNP': 0,\n",
       " '```': 0,\n",
       " 'kuNN': 0,\n",
       " 'kuFW': 0,\n",
       " 'oldJJ': 0,\n",
       " 'aboutIN': 0,\n",
       " 'threeCD': 0,\n",
       " 'hundredCD': 0,\n",
       " 'yearsNNS': 0,\n",
       " 'appearVBP': 0,\n",
       " 'youngerJJR': 0,\n",
       " 'ageNN': 0,\n",
       " 'dueJJ': 0,\n",
       " 'strangeJJ': 0,\n",
       " 'radiationNN': 0,\n",
       " 'ringsNNS': 0,\n",
       " 'planetNN': 0,\n",
       " 'peacefulJJ': 0,\n",
       " 'hordeNN': 0,\n",
       " 'fountainNN': 0,\n",
       " 'youthNN': 0,\n",
       " 'archenemiesNNS': 0,\n",
       " 'sonNNP': 0,\n",
       " 'ledVBN': 0,\n",
       " \"ru'afoNN\": 0,\n",
       " 'f.NNP': 0,\n",
       " 'abrahamNNP': 0,\n",
       " 'messNN': 0,\n",
       " 'everythingNN': 0,\n",
       " 'upRB': 0,\n",
       " 'horriblyRB': 0,\n",
       " 'disfiguredJJ': 0,\n",
       " 'dailyJJ': 0,\n",
       " 'reconstructiveNN': 0,\n",
       " 'aestheticallyRB': 0,\n",
       " 'acceptableJJ': 0,\n",
       " 'strikeVB': 0,\n",
       " 'dealNN': 0,\n",
       " 'federationNNP': 0,\n",
       " 'moveVB': 0,\n",
       " 'elsewhereRB': 0,\n",
       " 'exploitVB': 0,\n",
       " 'secretNN': 0,\n",
       " 'dyingVBG': 0,\n",
       " 'captainNNP': 0,\n",
       " 'picardNNP': 0,\n",
       " 'patrickNNP': 0,\n",
       " 'stewartNNP': 0,\n",
       " 'stepsNNS': 0,\n",
       " 'realizesVBZ': 0,\n",
       " 'movingVBG': 0,\n",
       " 'wouldMD': 0,\n",
       " 'killVB': 0,\n",
       " 'alsoRB': 0,\n",
       " 'helpsVBZ': 0,\n",
       " 'fallsVBZ': 0,\n",
       " 'womanNN': 0,\n",
       " 'donnaNNP': 0,\n",
       " 'murphyNNP': 0,\n",
       " 'trustyJJ': 0,\n",
       " 'crewNN': 0,\n",
       " 'defiesVBZ': 0,\n",
       " 'federationNN': 0,\n",
       " 'naturalJJ': 0,\n",
       " 'habitatNN': 0,\n",
       " 'dealingVBG': 0,\n",
       " 'historyNN': 0,\n",
       " 'entirelyRB': 0,\n",
       " 'necessaryJJ': 0,\n",
       " 're-introduceVB': 0,\n",
       " 'charactersNNS': 0,\n",
       " 'believeVBP': 0,\n",
       " 'non-fansNNS': 0,\n",
       " 'hardJJ': 0,\n",
       " 'gettingVBG': 0,\n",
       " 'orderNN': 0,\n",
       " 'understandVB': 0,\n",
       " 'approachVB': 0,\n",
       " 'howeverRB': 0,\n",
       " 'surprisinglyRB': 0,\n",
       " 'newJJ': 0,\n",
       " 'aspectsNNS': 0,\n",
       " 'boldJJ': 0,\n",
       " 'alwaysRB': 0,\n",
       " 'magneticJJ': 0,\n",
       " 'screenNN': 0,\n",
       " 'presenceNN': 0,\n",
       " 'perfectlyRB': 0,\n",
       " 'capableJJ': 0,\n",
       " 'holdingVBG': 0,\n",
       " 'entireJJ': 0,\n",
       " 'togetherRB': 0,\n",
       " 'jonathanNNP': 0,\n",
       " 'frakesNNP': 0,\n",
       " 'directedVBD': 0,\n",
       " 'funnyJJ': 0,\n",
       " 'commanderNNP': 0,\n",
       " 'rikerNNP': 0,\n",
       " 'subplotNN': 0,\n",
       " 'dataNNS': 0,\n",
       " 'brentNNP': 0,\n",
       " 'spinerNNP': 0,\n",
       " 'discoveringVBG': 0,\n",
       " 'lostVBN': 0,\n",
       " 'childhoodNN': 0,\n",
       " 'fairlyRB': 0,\n",
       " 'interestingJJ': 0,\n",
       " 'perfectJJ': 0,\n",
       " 'villainNN': 0,\n",
       " 'overactingVBG': 0,\n",
       " 'crazyJJ': 0,\n",
       " 'frakesNN': 0,\n",
       " 'showedVBD': 0,\n",
       " 'similarJJ': 0,\n",
       " 'aptitudeNN': 0,\n",
       " 'directionNN': 0,\n",
       " 'firstNNP': 0,\n",
       " 'contactNN': 0,\n",
       " 'attractiveJJ': 0,\n",
       " 'actionNN': 0,\n",
       " 'apparentlyRB': 0,\n",
       " 'utilizeVB': 0,\n",
       " 'computerNN': 0,\n",
       " 'animationNN': 0,\n",
       " 'resultNN': 0,\n",
       " 'pleasingVBG': 0,\n",
       " 'particularlyRB': 0,\n",
       " 'climacticJJ': 0,\n",
       " 'scenesNNS': 0,\n",
       " 'employVBP': 0,\n",
       " 'giantJJ': 0,\n",
       " 'spaceNN': 0,\n",
       " 'shipNN': 0,\n",
       " 'suckVB': 0,\n",
       " 'cleanJJ': 0,\n",
       " 'impressivelyRB': 0,\n",
       " 'sharpJJ': 0,\n",
       " 'lookNN': 0,\n",
       " 'complaintNN': 0,\n",
       " 'triesVBZ': 0,\n",
       " 'takeVB': 0,\n",
       " 'moralJJ': 0,\n",
       " 'stanceNN': 0,\n",
       " 'appropriateJJ': 0,\n",
       " 'doVB': 0,\n",
       " 'bigJJ': 0,\n",
       " 'wantedVBD': 0,\n",
       " '600CD': 0,\n",
       " 'saveVB': 0,\n",
       " 'livesNNS': 0,\n",
       " 'betterRBR': 0,\n",
       " 'couldMD': 0,\n",
       " 'co-existedVBN': 0,\n",
       " 'feelsVBZ': 0,\n",
       " 'springVB': 0,\n",
       " 'kindsNNS': 0,\n",
       " 'audienceNN': 0,\n",
       " 'inherentJJ': 0,\n",
       " 'campNN': 0,\n",
       " 'factorNN': 0,\n",
       " 'seemVB': 0,\n",
       " 'well-equippedJJ': 0,\n",
       " 'dealVB': 0,\n",
       " 'issuesNNS': 0,\n",
       " 'preferVBP': 0,\n",
       " 'spectacleNN': 0,\n",
       " 'heardVBN': 0,\n",
       " 'allPDT': 0,\n",
       " 'faces-natalieNNP': 0,\n",
       " 'portmanNNP': 0,\n",
       " 'professionalNNP': 0,\n",
       " 'queenNNP': 0,\n",
       " 'amidalaNNP': 0,\n",
       " 'liamNNP': 0,\n",
       " 'neesonNNP': 0,\n",
       " 'schindlerNNP': 0,\n",
       " 'listNN': 0,\n",
       " 'qui-gonNNP': 0,\n",
       " 'jinnNNP': 0,\n",
       " 'ewanNNP': 0,\n",
       " 'mcgregorNNP': 0,\n",
       " 'trainspottingVBG': 0,\n",
       " 'obi-wanNNP': 0,\n",
       " 'kenobiNNP': 0,\n",
       " 'jakeNNP': 0,\n",
       " 'lloydNNP': 0,\n",
       " 'jingleNNP': 0,\n",
       " 'youngJJ': 0,\n",
       " 'anakinNNP': 0,\n",
       " 'skywalkerNNP': 0,\n",
       " 'readVBN': 0,\n",
       " 'heardVBD': 0,\n",
       " 'failsVBZ': 0,\n",
       " 'magicNN': 0,\n",
       " 'humanityNN': 0,\n",
       " 'trilogyNN': 0,\n",
       " 'oneNN': 0,\n",
       " 'kiddie-friendlyJJ': 0,\n",
       " 'enoughJJ': 0,\n",
       " 'contentNN': 0,\n",
       " 'adultsNNS': 0,\n",
       " 'stunningJJ': 0,\n",
       " 'digitalizedJJ': 0,\n",
       " 'creaturesNNS': 0,\n",
       " 'amazinglyRB': 0,\n",
       " 'realisticJJ': 0,\n",
       " 'lightsaberNN': 0,\n",
       " 'duelsNNS': 0,\n",
       " 'amazingJJ': 0,\n",
       " 'sumptuousJJ': 0,\n",
       " 'robesNNS': 0,\n",
       " 'fitVBN': 0,\n",
       " 'wornVBN': 0,\n",
       " 'elizabethNNP': 0,\n",
       " 'budgetNN': 0,\n",
       " 'effects-itNN': 0,\n",
       " '*SYM': 0,\n",
       " 'buyVB': 0,\n",
       " 'actorsNNS': 0,\n",
       " 'struggleNN': 0,\n",
       " 'bestJJS': 0,\n",
       " 'fleshNN': 0,\n",
       " 'broad-strokedJJ': 0,\n",
       " 'flatJJ': 0,\n",
       " 'successfulJJ': 0,\n",
       " 'jediNNP': 0,\n",
       " 'masterNN': 0,\n",
       " 'quietJJ': 0,\n",
       " 'dignityNN': 0,\n",
       " 'wiseJJ': 0,\n",
       " 'commandingJJ': 0,\n",
       " 'anchorNN': 0,\n",
       " 'lucasNNP': 0,\n",
       " 'spentVBD': 0,\n",
       " 'fleshingVBG': 0,\n",
       " 'fareVB': 0,\n",
       " 'futureJJ': 0,\n",
       " 'motherNN': 0,\n",
       " 'lukeNNP': 0,\n",
       " 'leiaNNP': 0,\n",
       " 'queenNN': 0,\n",
       " 'invadedVBN': 0,\n",
       " 'tradeNNP': 0,\n",
       " '?!?CD': 0,\n",
       " 'stoicJJ': 0,\n",
       " 'stiltedJJ': 0,\n",
       " 'caricaturedJJ': 0,\n",
       " 'appearsVBZ': 0,\n",
       " 'shePRP': 0,\n",
       " 'vulcanNNP': 0,\n",
       " 'geishaNNP': 0,\n",
       " 'formerJJ': 0,\n",
       " 'himPRP': 0,\n",
       " 'endearingJJ': 0,\n",
       " 'robinNNP': 0,\n",
       " 'batmanNNP': 0,\n",
       " 'manfullyRB': 0,\n",
       " 'infuseVB': 0,\n",
       " 'smallJJ': 0,\n",
       " 'supportingVBG': 0,\n",
       " 'sparkVB': 0,\n",
       " 'genuineJJ': 0,\n",
       " 'insightNN': 0,\n",
       " 'absolutelyRB': 0,\n",
       " 'nailsNNP': 0,\n",
       " 'alecNNP': 0,\n",
       " 'guinessNNP': 0,\n",
       " 'episodeNNP': 0,\n",
       " '4-6CD': 0,\n",
       " 'scottishJJ': 0,\n",
       " 'accentNN': 0,\n",
       " 'profoundJJ': 0,\n",
       " 'mightMD': 0,\n",
       " 'otherwiseRB': 0,\n",
       " 'bankNN': 0,\n",
       " 'preexistingVBG': 0,\n",
       " 'knowledgeNN': 0,\n",
       " 'problemsNNS': 0,\n",
       " 'callVB': 0,\n",
       " 'myselfPRP': 0,\n",
       " 'warsNNP': 0,\n",
       " 'fan-especiallyRB': 0,\n",
       " 'consideringVBG': 0,\n",
       " 'meansVBZ': 0,\n",
       " 'fanaticJJ': 0,\n",
       " 'days-butJJ': 0,\n",
       " 'enjoyedVBN': 0,\n",
       " 'everyoneNN': 0,\n",
       " 'elseRB': 0,\n",
       " 'americaNNP': 0,\n",
       " 'personNN': 0,\n",
       " 'recentlyRB': 0,\n",
       " 'botherVB': 0,\n",
       " 'brush-upNN': 0,\n",
       " 'namesNNS': 0,\n",
       " 'obscureJJ': 0,\n",
       " 'hopelesslyRB': 0,\n",
       " 'mainJJ': 0,\n",
       " 'nefariousJJ': 0,\n",
       " 'senatorNNP': 0,\n",
       " 'palpatineNNP': 0,\n",
       " 'steepedVBN': 0,\n",
       " 'triviaNNS': 0,\n",
       " 'emperorNNP': 0,\n",
       " 'scaryJJ': 0,\n",
       " 'hoodedJJ': 0,\n",
       " 'apparitionNN': 0,\n",
       " 'darthNNP': 0,\n",
       " 'vaderNNP': 0,\n",
       " 'returnNN': 0,\n",
       " 'empireNNP': 0,\n",
       " 'strikesVBZ': 0,\n",
       " 'obviouslyRB': 0,\n",
       " 'appearanceNN': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy SVM: \",accuracySVM)\n",
    "print(\"Accuracy SVM with POS pres: \",accuracySVM_pos)\n",
    "print(\"-------------------------------------------\")\n",
    "print(accuracies_svm)\n",
    "print(accuracies_svm_pos)\n",
    "\n",
    "print(\"Significance test SVM VS SVM with POS pres\")\n",
    "p_value = sign_test(all_fold_predictions_svm_pos, all_fold_predictions_svm)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0dt_oQupUNe"
   },
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su-3w87eMW0w"
   },
   "source": [
    "#### (Q4.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Perform cross-validation and concatenate the predictions from all folds to compute the significance. Are the results significantly better than when we don't discard the closed-class words? *Why?* (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wij denken alleen dat we woorden moetne gebruiken en niet pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "CCUPlPozCYUX"
   },
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "\n",
    "\n",
    "### Create the folds\n",
    "def make_folds_closed(positiveReviews,negativeReviews):\n",
    "    folds_dict_pos = {}\n",
    "\n",
    "    for x in range(10):\n",
    "        folds_dict_pos[x] = []\n",
    "\n",
    "    for i, r in enumerate(positiveReviews):\n",
    "\n",
    "        text = r[\"content\"] \n",
    "        words_in_review = []\n",
    "        for x in text:\n",
    "            for a in x:\n",
    "                word = a[0].lower()\n",
    "                if a[1].startswith(\"NN\") or a[1].startswith(\"VB\") or a[1].startswith(\"JJ\") or a[1].startswith(\"RB\"):\n",
    "                    \n",
    "                    words_in_review.append(word)\n",
    "\n",
    "        current_list = folds_dict_pos[i % 10]\n",
    "        new_list = current_list + [words_in_review]\n",
    "\n",
    "        folds_dict_pos[i % 10] = new_list\n",
    "\n",
    "    # # fill vocab with all unique words \n",
    "    folds_dict_neg = {}\n",
    "    for x in range(10):\n",
    "        folds_dict_neg[x] = []\n",
    "\n",
    "    for i, r in enumerate(negativeReviews):\n",
    "\n",
    "        text = r[\"content\"] \n",
    "        words_in_review = []\n",
    "        for x in text:\n",
    "            for a in x:\n",
    "                word = a[0].lower()\n",
    "                if a[1].startswith(\"NN\") or a[1].startswith(\"VB\") or a[1].startswith(\"JJ\") or a[1].startswith(\"RB\"):\n",
    "                    \n",
    "                    words_in_review.append(word)\n",
    "               \n",
    "\n",
    "        current_list = folds_dict_neg[i % 10]\n",
    "        new_list = current_list + [words_in_review]\n",
    "\n",
    "        folds_dict_neg[i % 10] = new_list\n",
    "    return folds_dict_pos, folds_dict_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                         | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N FOLDDDDD 0\n",
      "44611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                         | 0/9 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|███████████████████▋                                                                                                                                                             | 1/9 [00:04<00:36,  4.52s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|███████████████████████████████████████▎                                                                                                                                         | 2/9 [00:08<00:31,  4.48s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███████████████████████████████████████████████████████████                                                                                                                      | 3/9 [00:13<00:26,  4.45s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████████████████████████████████████████████████████████████████▋                                                                                                  | 4/9 [00:18<00:22,  4.59s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                              | 5/9 [00:22<00:18,  4.56s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                           | 6/9 [00:27<00:13,  4.53s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 7/9 [00:31<00:09,  4.56s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 8/9 [00:36<00:04,  4.49s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:40<00:00,  4.50s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                         | 0/9 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|███████████████████▋                                                                                                                                                             | 1/9 [00:04<00:36,  4.53s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|███████████████████████████████████████▎                                                                                                                                         | 2/9 [00:08<00:31,  4.47s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███████████████████████████████████████████████████████████                                                                                                                      | 3/9 [00:13<00:26,  4.47s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████████████████████████████████████████████████████████████████▋                                                                                                  | 4/9 [00:17<00:22,  4.44s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                              | 5/9 [00:22<00:17,  4.45s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                           | 6/9 [00:26<00:13,  4.48s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 7/9 [00:31<00:08,  4.49s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 8/9 [00:35<00:04,  4.44s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:40<00:00,  4.50s/it]\n",
      "C:\\Users\\lolab\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|█▊                                                                                                                                                                             | 1/100 [00:00<00:14,  6.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|███▌                                                                                                                                                                           | 2/100 [00:00<00:14,  6.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█████▎                                                                                                                                                                         | 3/100 [00:00<00:15,  6.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███████                                                                                                                                                                        | 4/100 [00:00<00:14,  6.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|████████▊                                                                                                                                                                      | 5/100 [00:00<00:14,  6.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██████████▌                                                                                                                                                                    | 6/100 [00:00<00:14,  6.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|████████████▎                                                                                                                                                                  | 7/100 [00:01<00:13,  6.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████████████                                                                                                                                                                 | 8/100 [00:01<00:13,  6.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████████████▊                                                                                                                                                               | 9/100 [00:01<00:13,  6.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█████████████████▍                                                                                                                                                            | 10/100 [00:01<00:13,  6.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|███████████████████▏                                                                                                                                                          | 11/100 [00:01<00:13,  6.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████████████████████▉                                                                                                                                                         | 12/100 [00:01<00:13,  6.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████████████████▌                                                                                                                                                       | 13/100 [00:01<00:12,  6.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|████████████████████████▎                                                                                                                                                     | 14/100 [00:02<00:12,  6.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████████████████████████                                                                                                                                                    | 15/100 [00:02<00:12,  6.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████████████▊                                                                                                                                                  | 16/100 [00:02<00:12,  6.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████████████████████▌                                                                                                                                                | 17/100 [00:02<00:12,  6.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████████████████████████████▎                                                                                                                                              | 18/100 [00:02<00:12,  6.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█████████████████████████████████                                                                                                                                             | 19/100 [00:02<00:11,  6.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██████████████████████████████████▊                                                                                                                                           | 20/100 [00:02<00:11,  6.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████████████████████████████████▌                                                                                                                                         | 21/100 [00:03<00:11,  6.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██████████████████████████████████████▎                                                                                                                                       | 22/100 [00:03<00:11,  6.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████████████████████████████████████                                                                                                                                      | 23/100 [00:03<00:11,  6.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████████████████████████████████████▊                                                                                                                                    | 24/100 [00:03<00:11,  6.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████████████████████████████████████▌                                                                                                                                  | 25/100 [00:03<00:10,  6.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████████████████████████████████████████▏                                                                                                                                | 26/100 [00:03<00:10,  6.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████████████████████████████████████████▉                                                                                                                               | 27/100 [00:04<00:10,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|████████████████████████████████████████████████▋                                                                                                                             | 28/100 [00:04<00:10,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████████████████████████████████████████████▍                                                                                                                           | 29/100 [00:04<00:10,  6.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████████████████████████████████████████████▏                                                                                                                         | 30/100 [00:04<00:10,  6.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████████████████████████████████████████████▉                                                                                                                        | 31/100 [00:04<00:10,  6.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███████████████████████████████████████████████████████▋                                                                                                                      | 32/100 [00:04<00:09,  6.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████████████████████████████████████████████████▍                                                                                                                    | 33/100 [00:04<00:09,  6.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███████████████████████████████████████████████████████████▏                                                                                                                  | 34/100 [00:05<00:09,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|████████████████████████████████████████████████████████████▉                                                                                                                 | 35/100 [00:05<00:09,  6.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████████████████████████████████████████████████████▋                                                                                                               | 36/100 [00:05<00:09,  6.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|████████████████████████████████████████████████████████████████▍                                                                                                             | 37/100 [00:05<00:09,  6.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████████████████████████████████████████████████████████                                                                                                            | 38/100 [00:05<00:08,  6.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████████████████████████████████████████████████████████████▊                                                                                                          | 39/100 [00:05<00:08,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|█████████████████████████████████████████████████████████████████████▌                                                                                                        | 40/100 [00:05<00:08,  6.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████████████████████████████████████████████████████████████▎                                                                                                      | 41/100 [00:06<00:08,  6.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|█████████████████████████████████████████████████████████████████████████                                                                                                     | 42/100 [00:06<00:08,  6.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████████████████████████████████████████████████████████████▊                                                                                                   | 43/100 [00:06<00:08,  6.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████████████████████████████████████████████████████████████████▌                                                                                                 | 44/100 [00:06<00:08,  6.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████████████████████████████████████████████████████████████████▎                                                                                               | 45/100 [00:06<00:08,  6.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████████████████████████████████████████████████████████████████████████████████                                                                                              | 46/100 [00:06<00:07,  6.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████████████████████████████████████████████████████████████████████▊                                                                                            | 47/100 [00:06<00:07,  6.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                          | 48/100 [00:07<00:07,  6.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                                        | 49/100 [00:07<00:07,  6.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████████████████████████████████████████████████████████████████████████                                                                                       | 50/100 [00:07<00:07,  6.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|████████████████████████████████████████████████████████████████████████████████████████▋                                                                                     | 51/100 [00:07<00:07,  6.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                                   | 52/100 [00:07<00:07,  6.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                 | 53/100 [00:07<00:06,  6.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                | 54/100 [00:07<00:06,  6.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                                              | 55/100 [00:08<00:06,  6.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                            | 56/100 [00:08<00:06,  6.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 57/100 [00:08<00:06,  6.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                         | 58/100 [00:08<00:06,  6.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                       | 59/100 [00:08<00:06,  6.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                     | 60/100 [00:08<00:05,  6.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                   | 61/100 [00:08<00:05,  6.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                  | 62/100 [00:09<00:05,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 63/100 [00:09<00:05,  6.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                              | 64/100 [00:09<00:05,  6.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 65/100 [00:09<00:05,  6.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                           | 66/100 [00:09<00:04,  6.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 67/100 [00:09<00:04,  6.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 68/100 [00:09<00:04,  6.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 69/100 [00:10<00:04,  6.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                    | 70/100 [00:10<00:04,  6.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 71/100 [00:10<00:04,  6.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                | 72/100 [00:10<00:04,  6.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                               | 73/100 [00:10<00:03,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                             | 74/100 [00:10<00:03,  6.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                           | 75/100 [00:10<00:03,  6.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 76/100 [00:11<00:03,  6.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 77/100 [00:11<00:03,  6.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 78/100 [00:11<00:03,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                    | 79/100 [00:11<00:03,  6.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                  | 80/100 [00:11<00:02,  6.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                 | 81/100 [00:11<00:02,  6.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                               | 82/100 [00:12<00:02,  6.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                             | 83/100 [00:12<00:02,  6.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 84/100 [00:12<00:02,  6.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 85/100 [00:12<00:02,  6.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 86/100 [00:12<00:02,  6.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 87/100 [00:12<00:01,  6.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 88/100 [00:12<00:01,  6.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 89/100 [00:13<00:01,  6.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 90/100 [00:13<00:01,  6.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 91/100 [00:13<00:01,  6.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 92/100 [00:13<00:01,  6.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 93/100 [00:13<00:01,  6.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 94/100 [00:13<00:00,  6.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 95/100 [00:13<00:00,  6.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 96/100 [00:14<00:00,  6.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 97/100 [00:14<00:00,  6.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 98/100 [00:14<00:00,  6.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 99/100 [00:14<00:00,  5.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:14<00:00,  6.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|█▊                                                                                                                                                                             | 1/100 [00:00<00:14,  6.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|███▌                                                                                                                                                                           | 2/100 [00:00<00:14,  6.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█████▎                                                                                                                                                                         | 3/100 [00:00<00:14,  6.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|███████                                                                                                                                                                        | 4/100 [00:00<00:13,  6.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|████████▊                                                                                                                                                                      | 5/100 [00:00<00:13,  6.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██████████▌                                                                                                                                                                    | 6/100 [00:00<00:14,  6.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|████████████▎                                                                                                                                                                  | 7/100 [00:01<00:14,  6.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████████████                                                                                                                                                                 | 8/100 [00:01<00:15,  6.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███████████████▊                                                                                                                                                               | 9/100 [00:01<00:14,  6.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█████████████████▍                                                                                                                                                            | 10/100 [00:01<00:15,  5.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|███████████████████▏                                                                                                                                                          | 11/100 [00:01<00:14,  6.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████████████████████▉                                                                                                                                                         | 12/100 [00:01<00:14,  6.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|██████████████████████▌                                                                                                                                                       | 13/100 [00:02<00:14,  5.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|████████████████████████▎                                                                                                                                                     | 14/100 [00:02<00:14,  5.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████████████████████████                                                                                                                                                    | 15/100 [00:02<00:14,  5.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|███████████████████████████▊                                                                                                                                                  | 16/100 [00:02<00:14,  5.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█████████████████████████████▌                                                                                                                                                | 17/100 [00:02<00:13,  6.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████████████████████████████▎                                                                                                                                              | 18/100 [00:02<00:13,  6.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█████████████████████████████████                                                                                                                                             | 19/100 [00:03<00:13,  5.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██████████████████████████████████▊                                                                                                                                           | 20/100 [00:03<00:13,  5.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████████████████████████████████▌                                                                                                                                         | 21/100 [00:03<00:13,  5.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██████████████████████████████████████▎                                                                                                                                       | 22/100 [00:03<00:12,  6.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████████████████████████████████████                                                                                                                                      | 23/100 [00:03<00:12,  5.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████████████████████████████████████▊                                                                                                                                    | 24/100 [00:03<00:13,  5.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████████████████████████████████████▌                                                                                                                                  | 25/100 [00:04<00:12,  5.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████████████████████████████████████████▏                                                                                                                                | 26/100 [00:04<00:12,  5.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████████████████████████████████████████▉                                                                                                                               | 27/100 [00:04<00:12,  5.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|████████████████████████████████████████████████▋                                                                                                                             | 28/100 [00:04<00:12,  5.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████████████████████████████████████████████▍                                                                                                                           | 29/100 [00:04<00:11,  6.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████████████████████████████████████████████▏                                                                                                                         | 30/100 [00:04<00:11,  5.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████████████████████████████████████████████▉                                                                                                                        | 31/100 [00:05<00:11,  5.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███████████████████████████████████████████████████████▋                                                                                                                      | 32/100 [00:05<00:11,  6.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████████████████████████████████████████████████▍                                                                                                                    | 33/100 [00:05<00:11,  5.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███████████████████████████████████████████████████████████▏                                                                                                                  | 34/100 [00:05<00:11,  5.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|████████████████████████████████████████████████████████████▉                                                                                                                 | 35/100 [00:05<00:11,  5.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████████████████████████████████████████████████████▋                                                                                                               | 36/100 [00:06<00:11,  5.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|████████████████████████████████████████████████████████████████▍                                                                                                             | 37/100 [00:06<00:11,  5.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████████████████████████████████████████████████████████                                                                                                            | 38/100 [00:06<00:10,  5.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████████████████████████████████████████████████████████████▊                                                                                                          | 39/100 [00:06<00:10,  5.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|█████████████████████████████████████████████████████████████████████▌                                                                                                        | 40/100 [00:06<00:10,  5.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████████████████████████████████████████████████████████████▎                                                                                                      | 41/100 [00:06<00:10,  5.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|█████████████████████████████████████████████████████████████████████████                                                                                                     | 42/100 [00:07<00:10,  5.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████████████████████████████████████████████████████████████▊                                                                                                   | 43/100 [00:07<00:10,  5.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████████████████████████████████████████████████████████████████▌                                                                                                 | 44/100 [00:07<00:09,  5.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████████████████████████████████████████████████████████████████▎                                                                                               | 45/100 [00:07<00:09,  5.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████████████████████████████████████████████████████████████████████████████████                                                                                              | 46/100 [00:07<00:09,  5.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████████████████████████████████████████████████████████████████████▊                                                                                            | 47/100 [00:07<00:09,  5.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                          | 48/100 [00:08<00:09,  5.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                                        | 49/100 [00:08<00:08,  5.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████████████████████████████████████████████████████████████████████████                                                                                       | 50/100 [00:08<00:08,  5.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|████████████████████████████████████████████████████████████████████████████████████████▋                                                                                     | 51/100 [00:08<00:08,  5.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                                   | 52/100 [00:08<00:08,  5.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                 | 53/100 [00:08<00:08,  5.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                | 54/100 [00:09<00:08,  5.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                                              | 55/100 [00:09<00:07,  5.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                            | 56/100 [00:09<00:07,  5.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 57/100 [00:09<00:07,  5.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                         | 58/100 [00:09<00:07,  5.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                       | 59/100 [00:10<00:07,  5.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                     | 60/100 [00:10<00:06,  5.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                   | 61/100 [00:10<00:06,  5.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                  | 62/100 [00:10<00:06,  5.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 63/100 [00:10<00:06,  5.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                              | 64/100 [00:10<00:06,  5.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 65/100 [00:11<00:06,  5.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                           | 66/100 [00:11<00:05,  5.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 67/100 [00:11<00:05,  5.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 68/100 [00:11<00:05,  5.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 69/100 [00:11<00:05,  5.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                    | 70/100 [00:11<00:04,  6.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 71/100 [00:12<00:04,  6.39it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                | 72/100 [00:12<00:04,  6.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                               | 73/100 [00:12<00:04,  6.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                             | 74/100 [00:12<00:03,  6.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                           | 75/100 [00:12<00:03,  6.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 76/100 [00:12<00:03,  6.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 77/100 [00:12<00:03,  6.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 78/100 [00:13<00:03,  6.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                    | 79/100 [00:13<00:03,  6.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                  | 80/100 [00:13<00:02,  6.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                 | 81/100 [00:13<00:02,  6.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                               | 82/100 [00:13<00:02,  6.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                             | 83/100 [00:13<00:02,  6.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 84/100 [00:13<00:02,  6.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 85/100 [00:14<00:02,  6.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 86/100 [00:14<00:02,  6.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 87/100 [00:14<00:01,  6.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 88/100 [00:14<00:01,  6.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 89/100 [00:14<00:01,  6.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 90/100 [00:14<00:01,  6.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 91/100 [00:14<00:01,  6.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 92/100 [00:15<00:01,  6.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 93/100 [00:15<00:01,  6.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 94/100 [00:15<00:00,  6.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 95/100 [00:15<00:00,  6.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 96/100 [00:15<00:00,  6.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 97/100 [00:15<00:00,  6.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 98/100 [00:15<00:00,  6.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 99/100 [00:16<00:00,  6.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:34<00:00, 274.70s/it]\n"
     ]
    }
   ],
   "source": [
    "############### NB without stemming and with CV\n",
    "\n",
    "\n",
    "# fill vocab with all unique words \n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "\n",
    "# Set k for smoothing\n",
    "\n",
    "k = 1\n",
    "\n",
    "folds_dict_pos, folds_dict_neg = make_folds_closed(positiveReviews, negativeReviews)\n",
    "    \n",
    "accuracies_svm_closed = []\n",
    "\n",
    "all_fold_predictions_svm_closed = []\n",
    "\n",
    "\n",
    "# we trainen 10 keer, dus elke loop is een andere fold de test fold\n",
    "for n_fold in tqdm(range(10)):\n",
    "    print(\"N FOLDDDDD\", n_fold)\n",
    "    \n",
    "    # selecteer test folds\n",
    "    test_pos = folds_dict_pos[n_fold]\n",
    "    test_neg = folds_dict_neg[n_fold]\n",
    "    \n",
    "    # selecteer de training data \n",
    "    train_pos = pick_training_boiz(n_fold, folds_dict_pos)\n",
    "    train_neg = pick_training_boiz(n_fold, folds_dict_neg)\n",
    "    \n",
    "    fold_vocab = {}\n",
    "    \n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in train_pos:\n",
    "        for review in fold:\n",
    "            \n",
    "        \n",
    "            \n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "        \n",
    "\n",
    "            \n",
    "    for fold in train_neg:\n",
    "        for review in fold:\n",
    "            for word in review:\n",
    "                fold_vocab[word.lower()] = 0\n",
    "                \n",
    "    print(len(fold_vocab))\n",
    " \n",
    "    features_train_data = []\n",
    "    features_train_labels = []\n",
    "\n",
    "    # loop door de 9 folds die training data bevatten\n",
    "    for fold in tqdm(train_pos):\n",
    "        \n",
    "#       maak per review de bag of words weer leeg\n",
    "        for review in fold:\n",
    "            \n",
    "            bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "            \n",
    "            for word in review:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "                \n",
    "            features_train_data.append(list(bag_of_words_dict.values()))\n",
    "            features_train_labels.append(1)\n",
    "       \n",
    "            \n",
    "            \n",
    "    for fold in tqdm(train_neg):\n",
    "        for review in fold:\n",
    "            \n",
    "            \n",
    "            bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "            \n",
    "            for word in review:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "                \n",
    "            features_train_data.append(list(bag_of_words_dict.values()))\n",
    "            features_train_labels.append(0)      \n",
    "        \n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(features_train_data, features_train_labels)\n",
    "\n",
    "    results = []\n",
    "    labels = []  \n",
    "    \n",
    "    for review in tqdm(test_pos):     \n",
    "            \n",
    "        bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "\n",
    "        for word in review:\n",
    "            \n",
    "            if word.lower() in bag_of_words_dict:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "        \n",
    "        prediction = clf.predict(np.array(list(bag_of_words_dict.values())) .reshape(1, -1)   )\n",
    "        results.append(prediction[0])\n",
    "        \n",
    "        labels.append(1)\n",
    "        \n",
    "        \n",
    "    for review in tqdm(test_neg):     \n",
    "            \n",
    "        bag_of_words_dict = copy.deepcopy(fold_vocab)\n",
    "\n",
    "        for word in review:\n",
    "            \n",
    "            if word.lower() in bag_of_words_dict:\n",
    "                bag_of_words_dict[word.lower()] += 1\n",
    "        \n",
    "        \n",
    "        prediction = clf.predict(np.array( list(bag_of_words_dict.values())) .reshape(1, -1))\n",
    "        results.append(prediction[0])\n",
    "        \n",
    "        labels.append(0)\n",
    "\n",
    "    \n",
    "    amount_of_correct = 0\n",
    "    for r, l in zip(results, labels):\n",
    "        if r == l:\n",
    "            amount_of_correct += 1\n",
    "            all_fold_predictions_svm_closed.append(1)\n",
    "        else:\n",
    "            all_fold_predictions_svm_closed.append(0)\n",
    "\n",
    "    accuracies_svm_closed.append( (amount_of_correct / len(labels)) * 100)\n",
    "\n",
    "accuracySVM_closed = sum(accuracies_svm_closed)/len(accuracies_svm_closed)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy SVM: \",accuracySVM)\n",
    "print(\"Accuracy SVM closed POS: \",accuracySVM_closed)\n",
    "print(\"-------------------------------------------\")\n",
    "print(accuracies_svm)\n",
    "print(accuracies_svm_closed)\n",
    "\n",
    "print(\"Significance test SVM VS SVM closed POS\")\n",
    "p_value = sign_test(all_fold_predictions_svm_closed, all_fold_predictions_svm)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaxCVrs8pWSp"
   },
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfwqOciAl2No"
   },
   "source": [
    "# (Q5) Discussion (max. 500 words). (5pts)\n",
    "\n",
    "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
    "Why is this important? What are the limitations of these features and techniques?\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYuse5WLmekZ"
   },
   "source": [
    "Discussion:\n",
    "\n",
    "-Lexicon based approach (naïef)\n",
    "    - lexicon met magnitude (iets beter maar niet veel)\n",
    "\n",
    "-Naive Bayes (BOW - betere representatie van de review) maar wel probleem met nog nooit eerder voorgekomen  woorden\n",
    "    - Smoothing (manier om P=0 te fixen)\n",
    "    - Stemming (minder overfitten -> meer robust en generaliseerbaar\n",
    "    - Ook tegen overfitten op de trainingdata -> cross validation -> ook voordeel als je weinig data hebt. \n",
    "    - Uni/Bi/Tri grams -> meer context in2acc \n",
    "-SVM -> Super tof ding Zeker ook met computer vision was echt super leuk\n",
    "    - POS tags meer accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwaKwfWQhRk_"
   },
   "source": [
    "# Submission \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOUeaET5ijk-"
   },
   "outputs": [],
   "source": [
    "# Write your names and student numbers here:\n",
    "# Student 1 #12345\n",
    "# Student 2 #12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A9K-H6Tii3X"
   },
   "source": [
    "**That's it!**\n",
    "\n",
    "- Check if you answered all questions fully and correctly. \n",
    "- Download your completed notebook using `File -> Download .ipynb` \n",
    "- Check if your answers are all included in the file you submit.\n",
    "- Submit your .ipynb file via *Canvas*. One submission per group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHslatYAKBrF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uhU_tk-BOaXb"
   ],
   "name": "NLP1 2020 Practical 1 (student version)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
