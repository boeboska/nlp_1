{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptMZZMQlfn7c"
   },
   "source": [
    "------\n",
    "**You cannot apply any changes to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jILqpPLlE9r0"
   },
   "source": [
    "# Practical 2: Representing Sentences with Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JXOZ5uhQ8Qq"
   },
   "source": [
    "In this second practical, we will train neural network models to obtain sentence representations. We can then use these sentence representations for a downstream task such as sentiment classification. \n",
    "\n",
    "In this notebook, we will help you to develop models for your experiments. But this time, next to completing the notebook, **you are expected to write a four-page scientific report with your findings**. Please still submit the notebook together with your scientific report so that we can reproduce your experiments. (Note: if you find it useful, you can split this notebook into multiple notebooks. If you do so, keep it mind that it should be possible for your TAs to reproduce the entire content of the notebooks without having to ask for clarifications or to copy and paste functions from one sub-notebook to another.)\n",
    "\n",
    "**Important!** The main purpose of this lab is for you to learn how to answer research questions by experimenting and then writing a scientific report.\n",
    "So you will be *judged by the quality of your report* but will lose points if your experiments are not reproducible.\n",
    "You can find the requirements for the report at the end of this notebook.\n",
    "\n",
    "\n",
    "### Data set\n",
    "We will use the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/) (SST), which provides sentences, their binary tree structure, and fine-grained sentiment scores.\n",
    "This dataset is different from the one we used in the first practical. \n",
    "In Practical 1, a review consisted of several sentences, and we had one sentiment score for the whole review. Now, a review consists of a single sentence, and we have a sentiment score for each node in the binary tree that makes up the sentence, including the root node (i.e., we still have an overall sentiment score for the entire review). We will look at an example below.\n",
    "\n",
    "In the first part of this practical we will only make use of the sentence tokens whereas in the second part we will also exploit the tree structure that is provided by the SST.\n",
    "\n",
    "We will cover the following approaches:\n",
    "\n",
    "- Bag-of-words (BOW)\n",
    "- Continuous bag-of-words (CBOW)\n",
    "- Deep continuous bag-of-words (Deep CBOW)\n",
    "- LSTM\n",
    "- Tree-LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbNKef3lymaj"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jxTkpg59FlU"
   },
   "source": [
    "Let's first download the data set and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WZp53HmMP3F2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TovFkDTgE_d6"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
    "!unzip trainDevTestTrees_PTB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0IpAphkBO5eW"
   },
   "outputs": [],
   "source": [
    "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
    "def filereader(path): \n",
    "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      yield line.strip().replace(\"\\\\\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP_jpquiprH8"
   },
   "source": [
    "Let's look at a data point. It is a **flattened binary tree**, with sentiment scores at every node, and words as the leaves (or *terminal nodes*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ylkIopm0QJML"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n"
     ]
    }
   ],
   "source": [
    "s = next(filereader(\"trees/dev.txt\"))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7_U7HTFwdrWt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              3                                                                     \n",
      "  ____________|____________________                                                  \n",
      " |                                 4                                                \n",
      " |        _________________________|______________________________________________   \n",
      " |       4                                                                        | \n",
      " |    ___|______________                                                          |  \n",
      " |   |                  4                                                         | \n",
      " |   |         _________|__________                                               |  \n",
      " |   |        |                    3                                              | \n",
      " |   |        |               _____|______________________                        |  \n",
      " |   |        |              |                            4                       | \n",
      " |   |        |              |            ________________|_______                |  \n",
      " |   |        |              |           |                        2               | \n",
      " |   |        |              |           |                 _______|___            |  \n",
      " |   |        3              |           |                |           2           | \n",
      " |   |    ____|_____         |           |                |        ___|_____      |  \n",
      " |   |   |          4        |           3                |       2         |     | \n",
      " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
      " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
      " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
      " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use NLTK to better visualise the tree structure of the sentence\n",
    "from nltk import Tree\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "tree = Tree.fromstring(s)\n",
    "print(TreePrettyPrinter(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekAWKsji9t93"
   },
   "source": [
    "The sentiment scores range from 0 (very negative) to 5 (very positive). Again, as you can see, every node in the tree is labeled with a sentiment score. For now, we will only use the score at the **root node**, i.e., the sentiment score for the complete sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DKynLm0xPKr2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# Let's first make a function that extracts the tokens (the leaves).\n",
    "\n",
    "def tokens_from_treestring(s):\n",
    "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
    "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
    " \n",
    "# let's try it on our example tree\n",
    "tokens = tokens_from_treestring(s)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8vFkeqN-NLP"
   },
   "source": [
    "> *Warning: you could also parse a treestring using NLTK and ask it to return the leaves, but there seems to be an issue with NLTK not always correctly parsing the input, so do not rely on it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Akr9K_Mv4dym"
   },
   "outputs": [],
   "source": [
    "# We will also need the following function, but you can ignore this for now.\n",
    "# It is explained later on.\n",
    "\n",
    "SHIFT = 0\n",
    "REDUCE = 1\n",
    "\n",
    "\n",
    "def transitions_from_treestring(s):\n",
    "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
    "  s = re.sub(\"\\)\", \" )\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
    "  s = re.sub(\"\\)\", \"1\", s)\n",
    "  return list(map(int, s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mNtPdlwPgRat",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees/train.txt  8544\n",
      "trees/dev.txt    1101\n",
      "trees/test.txt   2210\n"
     ]
    }
   ],
   "source": [
    "# Now let's first see how large our data sets are.\n",
    "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
    "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HexlSqTR_UrY"
   },
   "source": [
    "You can see that the number of sentences is not very large. That's probably because the data set required so much manual annotation. However, it is large enough to train a neural network on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfRjelOcsXuC"
   },
   "source": [
    "It will be useful to store each data example in an `Example` object,\n",
    "containing everything that we may need for each data point.\n",
    "It will contain the tokens, the tree, the top-level sentiment label, and \n",
    "the transitions (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4I07Hb_-q8wg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from nltk import Tree\n",
    "from nltk import pos_tag\n",
    "\n",
    "# A simple way to define a class is using namedtuple.\n",
    "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\", \"tags\"])\n",
    "\n",
    "   \n",
    "def examplereader(path, lower=False):\n",
    "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
    "  for line in filereader(path):\n",
    "    line = line.lower() if lower else line\n",
    "    tokens = tokens_from_treestring(line)\n",
    "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
    "    label = int(line[1])\n",
    "    trans = transitions_from_treestring(line)\n",
    "    ### Add tags for later POS use\n",
    "    tags = pos_tag(tokens)\n",
    "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans, tags=tags)\n",
    "  \n",
    "\n",
    "# Let's load the data into memory.\n",
    "LOWER = False  # we will keep the original casing\n",
    "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
    "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
    "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n",
      "0.720708561788275\n",
      "0.09287220582032897\n",
      "0.18641923239139604\n"
     ]
    }
   ],
   "source": [
    "print((len(train_data)+len(dev_data)+len(test_data)))\n",
    "print(len(train_data)/(len(train_data)+len(dev_data)+len(test_data)))\n",
    "print(len(dev_data)/(len(train_data)+len(dev_data)+len(test_data)))\n",
    "print(len(test_data)/(len(train_data)+len(dev_data)+len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.']\n",
      "[('The', 'DT'), ('Rock', 'NNP'), ('is', 'VBZ'), ('destined', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('the', 'DT'), ('21st', 'JJ'), ('Century', 'NNP'), (\"'s\", 'POS'), ('new', 'JJ'), ('``', '``'), ('Conan', 'NNP'), (\"''\", \"''\"), ('and', 'CC'), ('that', 'IN'), ('he', 'PRP'), (\"'s\", 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('make', 'VB'), ('a', 'DT'), ('splash', 'NN'), ('even', 'RB'), ('greater', 'JJR'), ('than', 'IN'), ('Arnold', 'NNP'), ('Schwarzenegger', 'NNP'), (',', ','), ('Jean-Claud', 'NNP'), ('Van', 'NNP'), ('Damme', 'NNP'), ('or', 'CC'), ('Steven', 'NNP'), ('Segal', 'NNP'), ('.', '.')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#from nltk import pos_tag\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#sentence_pos_tags = [pos_tag(sentence.tokens) for sentence in train_data]\n",
    "\n",
    "\n",
    "print(train_data[0].tokens)\n",
    "print(train_data[0].tags)\n",
    "print(train_data[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pos_remover(tags, prob):\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    for pos_sentence, train_sentence in zip(sentence_pos_tags, train_data):\n",
    "#         print(pos_sentence, train_sentence.tokens)\n",
    "        \n",
    "        sentence = []\n",
    "        for pos_word , train_word in zip(pos_sentence, train_sentence.tokens):\n",
    "\n",
    "            for tag in tags:\n",
    "                if str(pos_word[1]).startswith(tag) and random.uniform(0, 1) < prob:\n",
    "                    continue\n",
    "                else:\n",
    "                    sentence.append(pos_word[0])\n",
    "\n",
    "        new_data.append(sentence)\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "                \n",
    "\n",
    "#remove_noun_etc = pos_remover([\"NN\", \"RB\", \"JJ\"], 0.75)\n",
    "#50_remove_noun_etc = pos_remover([\"NN\", \"RB\", \"JJ\"], 0.50)\n",
    "\n",
    "# adverb = rb\n",
    "# adjective = JJ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.143960674157302\n",
      "9.305588412283665\n",
      "6.406791907514451 1384\n",
      "34.22309337134711 1403\n"
     ]
    }
   ],
   "source": [
    "lengtes = []\n",
    "for data in train_data:\n",
    "    lengtes.append(len(data.tokens))\n",
    "print(np.average(lengtes))\n",
    "print(np.std(lengtes))\n",
    "kort = []\n",
    "lang = []\n",
    "for lengte in lengtes:\n",
    "    if lengte <= np.average(lengtes) - np.std(lengtes):\n",
    "        kort.append(lengte)\n",
    "    if lengte >= np.average(lengtes) + np.std(lengtes):\n",
    "        lang.append(lengte)\n",
    "print(np.average(kort), len(kort))\n",
    "print(np.average(lang),len(lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_train = []\n",
    "long_train = []\n",
    "for data in train_data:\n",
    "    if len(data.tokens) <= np.average(lengtes) - np.std(lengtes):\n",
    "        short_train.append(data)\n",
    "    if len(data.tokens) >= np.average(lengtes) + np.std(lengtes):\n",
    "        long_train.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1384 1403\n"
     ]
    }
   ],
   "source": [
    "print(len(short_train),len(long_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KM0bDyeVZtP"
   },
   "source": [
    "Let's check out an `Example` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "J8mwcaZwxP1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], tree=Tree('3', [Tree('2', ['It']), Tree('4', [Tree('4', [Tree('2', [\"'s\"]), Tree('4', [Tree('3', [Tree('2', ['a']), Tree('4', [Tree('3', ['lovely']), Tree('2', ['film'])])]), Tree('3', [Tree('2', ['with']), Tree('4', [Tree('3', [Tree('3', ['lovely']), Tree('2', ['performances'])]), Tree('2', [Tree('2', ['by']), Tree('2', [Tree('2', [Tree('2', ['Buy']), Tree('2', ['and'])]), Tree('2', ['Accorsi'])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], tags=[('It', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('lovely', 'JJ'), ('film', 'NN'), ('with', 'IN'), ('lovely', 'JJ'), ('performances', 'NNS'), ('by', 'IN'), ('Buy', 'NNP'), ('and', 'CC'), ('Accorsi', 'NNP'), ('.', '.')])\n",
      "First example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "First example label: 3\n"
     ]
    }
   ],
   "source": [
    "example = dev_data[0]\n",
    "print(\"First example:\", example)\n",
    "print(\"First example tokens:\", example.tokens)\n",
    "print(\"First example label:\",  example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WDSprDBVcr-"
   },
   "source": [
    "#### Vocabulary \n",
    "A first step in most NLP tasks is collecting all the word types that appear in the data into a vocabulary, and counting the frequency of their occurrences. On the one hand, this will give us an overview of the word distribution of the data set (what are the most frequent words, how many rare words are there, ...). On the other hand, we will also use the vocabulary to map each word to a unique numeric ID, which is a more handy index than a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VvNgKx7usRSt"
   },
   "outputs": [],
   "source": [
    "# Here we first define a class that can map a word to an ID (w2i)\n",
    "# and back (i2w).\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "  def __repr__(self):\n",
    "    return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "  def __reduce__(self):\n",
    "    return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.freqs = OrderedCounter()\n",
    "    self.w2i = {}\n",
    "    self.i2w = []\n",
    "\n",
    "  def count_token(self, t):\n",
    "    self.freqs[t] += 1\n",
    "    \n",
    "  def add_token(self, t):\n",
    "    self.w2i[t] = len(self.w2i)\n",
    "    self.i2w.append(t)    \n",
    "    \n",
    "  def build(self, min_freq=0):\n",
    "    '''\n",
    "    min_freq: minimum number of occurrences for a word to be included  \n",
    "              in the vocabulary\n",
    "    '''\n",
    "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
    "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)   \n",
    "    \n",
    "    tok_freq = list(self.freqs.items())\n",
    "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    for tok, freq in tok_freq:\n",
    "      if freq >= min_freq:\n",
    "        self.add_token(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOvkH_llVsoW"
   },
   "source": [
    "The vocabulary has by default an `<unk>` token and a `<pad>` token. The `<unk>` token is reserved for all words which do not appear in the training data (and for which, therefore, we cannot learn word representations). The function of the `<pad>` token will be explained later.\n",
    "\n",
    "\n",
    "Let's build the vocabulary!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GwGQgQQBNUSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18280\n"
     ]
    }
   ],
   "source": [
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "v = Vocabulary()\n",
    "for data_set in (train_data,):\n",
    "  for ex in data_set:\n",
    "    for token in ex.tokens:\n",
    "      v.count_token(token)\n",
    "\n",
    "v.build()\n",
    "print(\"Vocabulary size:\", len(v.w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UNIedPrPdCw"
   },
   "source": [
    "Let's have a closer look at the properties of our vocabulary. Having a good idea of what it is like can facilitate data analysis and debugging later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oJyuogmh0CA7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973\n"
     ]
    }
   ],
   "source": [
    "# What is the ID for \"century?\"\n",
    "print(v.w2i['century'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "O8OkPQ8Zv-rI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', ',', 'the', 'and', 'a', 'of', 'to', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "# What are the first 10 words in the vocabulary (based on their IDs)?\n",
    "print(v.i2w[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kmXwu02lOLWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 8024\n",
      ", 7131\n",
      "the 6037\n",
      "and 4431\n",
      "a 4403\n",
      "of 4386\n",
      "to 2995\n",
      "'s 2544\n",
      "is 2536\n",
      "that 1915\n"
     ]
    }
   ],
   "source": [
    "# What are the 10 most common words?\n",
    "sortedfreqs = sorted(v.freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "i = 0\n",
    "for key,value in sortedfreqs:\n",
    "    print(key,value)\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "__NDPaCeOT_m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9543\n"
     ]
    }
   ],
   "source": [
    "# And how many words are there with frequency 1?\n",
    "# (A fancy name for these is hapax legomena.)\n",
    "i = 0\n",
    "for key,value in sortedfreqs:\n",
    "    if value == 1:\n",
    "        i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xKHocugctZGM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bai\n",
      "conjure\n",
      "bug\n",
      "pap\n",
      "Street\n",
      "cruelties\n",
      "diatribes\n",
      "Reeboir\n",
      "disastrous\n",
      "footnote\n",
      "Manas\n",
      "b\n",
      "Harris\n",
      "homosexual\n",
      "screenplays\n",
      "Hopelessly\n",
      "Wong\n",
      "searches\n",
      "tenderly\n",
      "Terri\n"
     ]
    }
   ],
   "source": [
    "# Finally 20 random words from the vocabulary.\n",
    "# This is a simple way to get a feeling for the data. \n",
    "# You could use the `choice` function from the already imported `random` package\n",
    "i = 0\n",
    "while i < 20:\n",
    "    print(random.choice(v.i2w))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGWaZahKV_dH"
   },
   "source": [
    "#### Sentiment label vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AmTC-rvQelpl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
      "very positive\n"
     ]
    }
   ],
   "source": [
    "# Now let's map the sentiment labels 0-4 to a more readable form\n",
    "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "print(i2t)\n",
    "print(i2t[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "D7UI26DP2dr2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('very negative', 0), ('negative', 1), ('neutral', 2), ('positive', 3), ('very positive', 4)])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# And let's also create the opposite mapping.\n",
    "# We won't use a Vocabulary for this (although we could), since the labels\n",
    "# are already numeric.\n",
    "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
    "print(t2i)\n",
    "print(t2i['very positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0067ax54-rd"
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "In Colab notebooks, the last available version of PyTorch is already installed.The current stable version is 1.7.\n",
    "\n",
    "*For installing PyTorch in your own computer, follow the instructions on [pytorch.org](pytorch.org) instead. This is for Google Colab only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qKQMGtkR5KWr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using torch\", torch.__version__) # should say 1.7.0+cu101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mnvPcd_E1xH8"
   },
   "outputs": [],
   "source": [
    "# Let's also import torch.nn, a PyTorch package that  \n",
    "# makes building neural networks more convenient.\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "BYt8uTyGCKc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
    "# This cell selects the GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2d1VMOOYx1Bw"
   },
   "outputs": [],
   "source": [
    "# Seed manually to make runs reproducible\n",
    "# You need to set this again if you do multiple runs of the same model\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# When running on the CuDNN backend two further options must be set for reproducibility\n",
    "if torch.cuda.is_available():\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWBTzkuE3CtZ"
   },
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBAjYYySOA5W"
   },
   "source": [
    "Our first model is a rather simple neural **bag-of-words (BOW) model**.\n",
    "Unlike the bag-of-words model that you used in the previous lab, where we would look at the presence / frequency of words in a text, here we associate each word with a multi-dimensional vector which expresses what sentiment is conveyed by the word. In particular, our BOW vectors will be of size 5, exactly our number of sentiment classes. \n",
    "\n",
    "To classify a sentence, we **sum** the vectors of the words in the sentence and a bias vector. Because we sum the vectors, we lose word order: that's why we call this a neural bag-of-words model.\n",
    "\n",
    "```\n",
    "this   [0.0, 0.1, 0.1, 0.1, 0.0]\n",
    "movie  [0.0, 0.1, 0.1, 0.2, 0.1]\n",
    "is     [0.0, 0.1, 0.0, 0.0, 0.0]\n",
    "stupid [0.9, 0.5, 0.1, 0.0, 0.0]\n",
    "\n",
    "bias   [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "--------------------------------\n",
    "sum    [0.9, 0.8, 0.3, 0.3, 0.1]\n",
    "\n",
    "argmax: 0 (very negative)\n",
    "```\n",
    "\n",
    "The **argmax** of this sum is our predicted label.\n",
    "\n",
    "We initialize all vectors *randomly* and train them using cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLtBAIQGynkB"
   },
   "source": [
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZfNklWf3tvs"
   },
   "outputs": [],
   "source": [
    "class BOW(nn.Module):\n",
    "  \"\"\"A simple bag-of-words model\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, vocab):\n",
    "    super(BOW, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    \n",
    "    # this is a trainable look-up table with word embeddings\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    # this is a trainable bias term\n",
    "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)        \n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # this is the forward pass of the neural network\n",
    "    # it applies a function to the input and returns the output\n",
    "\n",
    "    # this looks up the embeddings for each word ID in inputs\n",
    "    # the result is a sequence of word embeddings\n",
    "    embeds = self.embed(inputs)\n",
    "\n",
    "    \n",
    "    # the output is the sum across the time dimension (1)\n",
    "    # with the bias term added\n",
    "    logits = embeds.sum(1) + self.bias\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKHvBnoBAr6z"
   },
   "outputs": [],
   "source": [
    "# Let's create a model.\n",
    "vocab_size = len(v.w2i)\n",
    "n_classes = len(t2i)\n",
    "bow_model = BOW(vocab_size, n_classes, v)\n",
    "print(bow_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfCx-HvMH1qQ"
   },
   "source": [
    "> **Hey, wait, where is the bias vector?**\n",
    "> PyTorch does not print Parameters, only Modules!\n",
    "\n",
    "> We can print it ourselves though, to check that it is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Fhvk5HenAroT"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here we print each parameter name, shape, and if it is trainable.\n",
    "def print_parameters(model):\n",
    "  total = 0\n",
    "  for name, p in model.named_parameters():\n",
    "    total += np.prod(p.shape)\n",
    "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
    "  print(\"\\nTotal number of parameters: {}\\n\".format(total))\n",
    "    \n",
    "\n",
    "#print_parameters(bow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSAw292WxuP4"
   },
   "source": [
    "#### Preparing an example for input\n",
    "\n",
    "To feed sentences to our PyTorch model, we need to convert a sequence of tokens to a sequence of IDs. The `prepare_example` function below takes care of this for us. We then use these IDs as indices for the word embedding table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "YWeGTC_OGReV"
   },
   "outputs": [],
   "source": [
    "def prepare_example(example, vocab, shuffle = False):\n",
    "    \"\"\"\n",
    "    Map tokens to their IDs for a single example\n",
    "    \"\"\"\n",
    "  \n",
    "      # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
    "    x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
    "  \n",
    "    x = torch.LongTensor([x])\n",
    "    if shuffle:\n",
    "        x =  x[:,torch.randperm(x.size()[1])]\n",
    "        \n",
    "    x= x.to(device)\n",
    "    y = torch.LongTensor([example.label])\n",
    "    y = y.to(device)\n",
    "  \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "sfbdv9px3uFF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 998,   16,   28,    6,    0,   32,    2,   18,    5,  998, 7688,    9,\n",
      "          135]])\n",
      "y: tensor([3], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PosVocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b8e23d95114d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPosVocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PosVocab' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = prepare_example(dev_data[0], v, shuffle = True)\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "\n",
    "\n",
    "x1, y1 = prepare_example(train_data[0], PosVocab)\n",
    "print(x1)\n",
    "print(y1)\n",
    "\n",
    "x2, y2 = pos_prepare_example(train_data[0], PosVocab)\n",
    "\n",
    "print(x2)\n",
    "print(y2)\n",
    "\n",
    "##$$$$$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKNQjEc0yXnJ"
   },
   "source": [
    "#### Evaluation\n",
    "We now need to define an evaluation metric.\n",
    "How many predictions do we get right? The accuracy will tell us.\n",
    "Make sure that you understand this code block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "yGmQLcVYKZsh"
   },
   "outputs": [],
   "source": [
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout (explained later)\n",
    "\n",
    "  for example in data:\n",
    "    \n",
    "    # convert the example input and label to PyTorch tensors\n",
    "    x, target = prep_fn(example, model.vocab)\n",
    "    print(x)\n",
    "    # forward pass without backpropagation (no_grad)\n",
    "    # get the output from the neural network for input x\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "    \n",
    "    # get the prediction\n",
    "    prediction = logits.argmax(dim=-1)\n",
    "    \n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (prediction == target).sum().item()\n",
    "    total += 1\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KlIGFXllWWm"
   },
   "source": [
    "We are using accuracy as a handy evaluation metric. Please consider using [alternative metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) for your experiments if that makes more theoretical sense (see, e.g., Q3.3 in Practical 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIk6OtSdzGRP"
   },
   "source": [
    "#### Example feed\n",
    "For stochastic gradient descent (SGD) we will need a random training example for every update.\n",
    "We implement this by shuffling the training data and returning examples one by one using `yield`.\n",
    "\n",
    "Shuffling is optional so that we get to use this function to get validation and test examples, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "dxDFOZLfCXvJ"
   },
   "outputs": [],
   "source": [
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  for example in data:\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g09SM8yb2cjx"
   },
   "source": [
    "#### Exercise: Training function\n",
    "\n",
    "Your task is now to complete the training loop below.\n",
    "Before you do so, please read the section about optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVfUukVdM_1c"
   },
   "source": [
    "**Optimisation**\n",
    "\n",
    "As mentioned in the \"Intro to PyTorch\" notebook, one of the perks of using PyTorch is automatic differentiation. We will use it to train our BOW model. \n",
    "\n",
    "We train our model by feeding it an input, performing a **forward** pass, obtaining an output prediction, and calculating a **loss** with our loss function.\n",
    "After the gradients are computed in the **backward** pass, we can take a step on the surface of the loss function towards more optimal parameter settings (gradient descent). \n",
    "\n",
    "The package we will use to do this optimisation is [torch.optim](https://pytorch.org/docs/stable/optim.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "KhQigDrQ--YU"
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGIvcTZU_Cez"
   },
   "source": [
    "Besides implementations of stochastic gradient descent (SGD), this package also implements the optimisation algorithm Adam, which we'll be using in this practical. \n",
    "For the purposes of this assignment you do not need to know what Adam does besides that it uses gradient information to update our model parameters by calling: \n",
    "\n",
    "```\n",
    "optimizer.step()\n",
    "```\n",
    "Remember when we updated our parameters in the PyTorch tutorial in a loop?\n",
    "\n",
    "\n",
    "```python\n",
    "# update weights\n",
    "learning_rate = 0.5\n",
    "for f in net.parameters():\n",
    "    # for each parameter, take a small step in the opposite dir of the gradient\n",
    "    p.data = p.data - p.grad.data * learning_rate\n",
    "\n",
    "```\n",
    "The function call optimizer.step() does effectively the same thing.\n",
    "\n",
    "*(If you want to know more about optimisation algorithms using gradient information, [this blog](http://ruder.io/optimizing-gradient-descent/.) gives a nice intuitive overview.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "ktFnKBux25lD"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_iterations=10000, \n",
    "                print_every=1000, eval_every=1000,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None, Shuffle=False, OnlyTest=False, data = train_data):\n",
    "    \"\"\"Train a model.\"\"\"  \n",
    "    #print(\"SHUFFLE\", Shuffle)\n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    start = time.time()\n",
    "    criterion = nn.CrossEntropyLoss() # loss function\n",
    "    best_eval = 0.\n",
    "    best_iter = 0\n",
    "\n",
    "    # store train loss and validation accuracy during training\n",
    "    # so we can plot them afterwards\n",
    "    losses = []\n",
    "    accuracies = []  \n",
    "    \n",
    "    #print(\"DDDAAATAAHAHAHA\", len(data))\n",
    "\n",
    "    if eval_batch_size is None:\n",
    "        eval_batch_size = batch_size\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        for batch in batch_fn(data, batch_size=batch_size):\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            \n",
    "            x, targets = prep_fn(batch, model.vocab, Shuffle)\n",
    "            \n",
    "            logits = model(x)\n",
    "\n",
    "            B = targets.size(0)  # later we will use B examples per update\n",
    "\n",
    "            # compute cross-entropy loss (our criterion)\n",
    "            # note that the cross entropy loss function computes the softmax for us\n",
    "            loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # backward pass (tip: check the Introduction to PyTorch notebook)\n",
    "\n",
    "            # erase previous gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "            # update weights - take a small step in the opposite dir of the gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "                print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "                      (iter_i, train_loss, time.time()-start))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0        \n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "                _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
    "                                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                accuracies.append(accuracy)\n",
    "                print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
    "\n",
    "                # save best model parameters\n",
    "                if accuracy > best_eval:\n",
    "                    print(\"new highscore\")\n",
    "                    best_eval = accuracy\n",
    "                    best_iter = iter_i\n",
    "                    path = \"{}.pt\".format(model.__class__.__name__)\n",
    "                    ckpt = {\n",
    "                      \"state_dict\": model.state_dict(),\n",
    "                      \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                      \"best_eval\": best_eval,\n",
    "                      \"best_iter\": best_iter\n",
    "                    }\n",
    "                    torch.save(ckpt, path)\n",
    "\n",
    "            # done training\n",
    "            if iter_i == num_iterations:\n",
    "                print(\"Done training\")\n",
    "\n",
    "                # evaluate on train, dev, and test with best model\n",
    "                print(\"Loading best model\")\n",
    "                path = \"{}.pt\".format(model.__class__.__name__)        \n",
    "                ckpt = torch.load(path)\n",
    "                model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "#                 _, _, train_acc = eval_fn(\n",
    "#                     model, train_data, batch_size=eval_batch_size, \n",
    "#                     batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "#                 _, _, dev_acc = eval_fn(\n",
    "#                     model, dev_data, batch_size=eval_batch_size,\n",
    "#                     batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "                _, _, test_acc = eval_fn(\n",
    "                    model, test_data, batch_size=eval_batch_size, \n",
    "                    batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "\n",
    "#                 print(\"best model iter {:d}: \"\n",
    "#                       \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
    "#                           best_iter, train_acc, dev_acc, test_acc))\n",
    "                if OnlyTest:\n",
    "                    return test_acc\n",
    "\n",
    "                return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEPsLvI-3D5b"
   },
   "source": [
    "### Training the BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9luJnNuN_d3q"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BOW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-d9b0204395d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# If everything is in place we can now train our first model!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbow_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBOW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbow_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BOW' is not defined"
     ]
    }
   ],
   "source": [
    "# If everything is in place we can now train our first model!\n",
    "bow_model = BOW(len(v.w2i), len(t2i), vocab=v)\n",
    "print(bow_model)\n",
    "\n",
    "bow_model = bow_model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(bow_model.parameters(), lr=0.0005)\n",
    "bow_losses, bow_accuracies = train_model(\n",
    "    bow_model, optimizer, num_iterations=200000, \n",
    "    print_every=1000, eval_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvYLj8LIAzfS"
   },
   "outputs": [],
   "source": [
    "# This will plot the validation accuracies across time.\n",
    "plt.plot(bow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUHm9JfSI4po"
   },
   "outputs": [],
   "source": [
    "# This will plot the training loss over time.\n",
    "plt.plot(bow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psxh-Le1BMDQ"
   },
   "source": [
    "Please note that we set the number of iterations to 30K as an indicative value, after which we simply stop training without checking for convergence. You should choose an appropriate number of iterations and motivate your decision. **This holds for all pre-set numbers of iteration in the following code blocks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9mB1_XhMPNN"
   },
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWk78FvNMw4o"
   },
   "source": [
    "We now continue with a **continuous bag-of-words (CBOW)** model. (*This is not the same as the word2vec CBOW model!*)\n",
    "\n",
    "It is similar to the BOW model above, but now embeddings can have a dimension of *arbitrary size*. \n",
    "This means that we can choose a higher dimensionality and learn more aspects of each word. We will still sum word vectors to get a sentence representation, but now the size of the resulting vector will no longer correspond to the number of sentiment classes. \n",
    "\n",
    "So to turn the size of our summed vector into the number of output classes, we can *learn* a parameter matrix $W$ and multiply it by the sum vector $x$: $$Wx$$\n",
    "If the size of $x$ is `d x 1`, we can set $W$ to be `5 x d`, so that the output of the matrix multiplication will be the of the desired size, `5 x 1`. Then, just like for the BOW model, we can obtain a prediction using the argmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIjrCPfCwsXI"
   },
   "source": [
    "## Exercise: implement and train the CBOW model\n",
    "\n",
    "Write a class `CBOW` that:\n",
    "\n",
    "- has word embeddings with size 300\n",
    "- sums the word vectors for the input words (just like in `BOW`)\n",
    "- projects the resulting vector down to 5 units using a linear layer and a bias term (check out `nn.Linear`)\n",
    "\n",
    "Train your CBOW model and plot the validation accuracy and training loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEV22aR2MP0Q"
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, vocab):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        #self.w = torch.tensor((5,embedding_dim), requires_grad =True)\n",
    "        \n",
    "        # this is a trainable look-up table with word embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(embedding_dim,5)\n",
    "        # this is a trainable bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # this is the forward pass of the neural network\n",
    "        # it applies a function to the input and returns the output\n",
    "\n",
    "        # this looks up the embeddings for each word ID in inputs\n",
    "        # the result is a sequence of word embeddings\n",
    "        embeds = self.embed(inputs)\n",
    "        summ = embeds.sum(1) + self.bias\n",
    "        linear = self.linear(summ)\n",
    "        #print(linear.size())\n",
    "        #print(linear)\n",
    "        \n",
    "        # the output is the sum across the time dimension (1)\n",
    "        # with the bias term added\n",
    "       # logits = linear.sum(1) \n",
    "        #+ self.bias\n",
    "        logits = linear\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = CBOW(len(v.w2i), 300, vocab=v)\n",
    "print(cbow_model)\n",
    "\n",
    "cbow_model = cbow_model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=0.0005)\n",
    "cbow_losses, cbow_accuracies = train_model(\n",
    "    cbow_model, optimizer, num_iterations=100000, \n",
    "    print_every=1000, eval_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will plot the validation accuracies across time.\n",
    "plt.plot(cbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will plot the training loss over time.\n",
    "plt.plot(cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpFt_Fo2TdN0"
   },
   "source": [
    "# Deep CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZanOMesTfEZ"
   },
   "source": [
    "To see if we can squeeze some more performance out of the CBOW model, we can make it deeper and non-linear by adding more layers and, e.g., tanh-activations.\n",
    "By using more parameters we can learn more aspects of the data, and by using more layers and non-linearities, we can try to learn a more complex function. \n",
    "This is not something that always works. If the input-output mapping of your data is simple, then a complicated function could easily overfit on your training set, thereby leading to poor generalization. \n",
    "\n",
    "#### Exercise: write Deep CBOW class and train it\n",
    "\n",
    "Write a class `DeepCBOW`.\n",
    "\n",
    "In your code, make sure that your `output_layer` consists of the following:\n",
    "- A linear transformation from E units to D units.\n",
    "- A Tanh activation\n",
    "- A linear transformation from D units to D units\n",
    "- A Tanh activation\n",
    "- A linear transformation from D units to 5 units (our output classes).\n",
    "\n",
    "E is the size of the word embeddings (please use E=300) and D for the size of a hidden layer (please use D=100).\n",
    "\n",
    "We recommend using [nn.Sequential](https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential) to implement this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8Z1igvpTrZq"
   },
   "outputs": [],
   "source": [
    "class DCBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, vocab,hidden_units):\n",
    "        super(DCBOW, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # this is a trainable look-up table with word embeddings\n",
    "        layers = []\n",
    "        layers.append(nn.Embedding(vocab_size, embedding_dim))\n",
    "        layers.append(nn.Linear(embedding_dim,hidden_units))\n",
    "        layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_units,hidden_units))\n",
    "        layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_units,5))\n",
    "        \n",
    "        self.layers=nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # this is the forward pass of the neural network\n",
    "        # it applies a function to the input and returns the output\n",
    "\n",
    "        # this looks up the embeddings for each word ID in inputs\n",
    "        # the result is a sequence of word embeddings\n",
    "        \n",
    "        logits = self.layers(inputs)\n",
    "        logits = logits.sum(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcbow_model = DCBOW(len(v.w2i), 300, vocab=v, hidden_units=100)\n",
    "print(dcbow_model)\n",
    "\n",
    "dcbow_model = dcbow_model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(dcbow_model.parameters(), lr=0.0005)\n",
    "dcbow_losses, dcbow_accuracies = train_model(\n",
    "    dcbow_model, optimizer, num_iterations=100000, \n",
    "    print_every=1000, eval_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will plot the validation accuracies across time.\n",
    "plt.plot(dcbow_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will plot the training loss over time.\n",
    "plt.plot(cbow_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQZ5flHwiiHY"
   },
   "source": [
    "# Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NX35vecmHy6"
   },
   "source": [
    "The Stanford Sentiment Treebank is a rather small data set, since it required fine-grained manual annotatation. This makes it difficult for the Deep CBOW model to learn good word embeddings, i.e. to learn good word representations for the words in our vocabulary.\n",
    "In fact, the only error signal that the network receives is from predicting the sentiment of entire sentences!\n",
    "\n",
    "To start off with better word representations, we can download **pre-trained word embeddings**. \n",
    "You can choose which pre-trained word embeddings to use:\n",
    "\n",
    "- **GloVe**. The \"original\" Stanford Sentiment classification [paper](http://aclweb.org/anthology/P/P15/P15-1150.pdf) used Glove embeddings, which are just another method (like *word2vec*) to get word embeddings from unannotated text. Glove is described in the following paper which you should cite if you use them:\n",
    "> Jeffrey Pennington, Richard Socher, and Christopher Manning. [\"Glove: Global vectors for word representation.\"](https://nlp.stanford.edu/pubs/glove.pdf) EMNLP 2014. \n",
    "\n",
    "- **Word2Vec**. This is the method that you learned about in class, described in:\n",
    "> Mikolov, Tomas, et al. [\"Distributed representations of words and phrases and their compositionality.\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Advances in neural information processing systems. 2013.\n",
    "\n",
    "Using these pre-trained word embeddings, we can initialize our word embedding lookup table and start form a point where similar words are already close to one another in the distributional semantic space. \n",
    "\n",
    "You can choose to keep the word embeddings **fixed** or to train them further, specialising them to the task at hand.\n",
    "We will keep them fixed for now.\n",
    "\n",
    "For the purposes of this lab, it is enough if you understand how word2vec works (whichever vectors you use), but if you are interested, we encourage you to also check out the GloVe paper.\n",
    "\n",
    "You can either download the word2vec vectors, or the Glove vectors.\n",
    "If you want to compare your results to the Stanford paper later on, then you should use Glove. \n",
    "**At the end of this lab you have the option to compare which vectors give you the best performance. For now, simply choose one of them and continue with that.**\n",
    "\n",
    "[**OPTIONAL in case you don't want to mount Google Drive:** instead of running all the 5 boxes below, you can 1) download the GloVe and word2vec in your local machine, 2) upload them on your Drive folder (\"My Drive\"). Then, uncomment the first 2 lines in box 6 before writing your code!]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGYr02WWO993"
   },
   "outputs": [],
   "source": [
    "# This downloads the Glove 840B 300d embeddings.\n",
    "# The original file is at http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# Since that file is 2GB, we provide you with a *filtered version*\n",
    "# which contains all the words you need for this data set.\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this cell out after downloading.\n",
    "\n",
    "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NLsgFGiTjmI"
   },
   "outputs": [],
   "source": [
    "# This downloads the word2vec 300D Google News vectors \n",
    "# The file has been truncated to only contain words that appear in our data set.\n",
    "# You can find the original file here: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this out after downloading.\n",
    "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "GXBITzPRQUQb"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (to save the downloaded files)\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFvzPuiKSCbl"
   },
   "outputs": [],
   "source": [
    "# Copy word vectors *to* Google Drive\n",
    "\n",
    "# You only need to do this once.\n",
    "# Please comment this out after running it. \n",
    "!cp \"glove.840B.300d.sst.txt\" \"/gdrive/My Drive/\"\n",
    "!cp \"googlenews.word2vec.300d.txt\" \"/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUMH0bM6BuY9"
   },
   "outputs": [],
   "source": [
    "# If you copied the word vectors to your Drive before,\n",
    "# here is where you copy them back to the Colab notebook.\n",
    "\n",
    "# Copy Glove vectors *from* Google Drive\n",
    "!cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" .\n",
    "!cp \"/gdrive/My Drive/googlenews.word2vec.300d.txt\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcpkoh6PIjfe"
   },
   "outputs": [],
   "source": [
    "# Uncomment these 2 lines below if went for the OPTIONAL method described above\n",
    "# !cp \"glove.840B.300d.sst.txt\" \"./\"\n",
    "# !cp \"googlenews.word2vec.300d.txt\" \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX2GJVHILM8n"
   },
   "source": [
    "At this point you have the pre-trained word embedding files, but what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ChsChH14Ruxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n",
      ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.30691 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.0052483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412 -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0.40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.060807 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0.012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0.20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0.18516 -0.21408 0.40616 -0.28974 0.074174 -0.17795 0.28595 -0.039626 -0.2339 -0.36054 -0.067503 -0.091065 0.23438 -0.0041331 0.003232 0.0072134 0.008697 0.21614 0.049904 0.35582 0.13748 0.073361 0.14166 0.2412 -0.013322 0.15613 0.083381 0.088146 -0.019357 0.43795 0.083961 0.45309 -0.50489 -0.10865 -0.2527 -0.18251 0.20441 0.13319 0.1294 0.050594 -0.15612 -0.39543 0.12538 0.24881 -0.1927 -0.31847 -0.12719 0.4341 0.31177 -0.0040946 -0.2094 -0.079961 0.1161 -0.050794 0.015266 -0.2803 -0.12486 0.23587 0.2339 -0.14023 0.028462 0.56923 -0.1649 -0.036429 0.010051 -0.17107 -0.042608 0.044965 -0.4393 -0.26137 0.30088 -0.060772 -0.45312 -0.19076 -0.20288 0.27694 -0.060888 0.11944 0.62206 -0.19343 0.47849 -0.30113 0.059389 0.074901 0.061068 -0.4662 0.40054 -0.19099 -0.14331 0.018267 -0.18643 0.20709 -0.35598 0.05338 -0.050821 -0.1918 -0.37846 -0.06589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./glove.840B.300d.sst.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for x in f:\n",
    "        print(len(x.split(' ')))\n",
    "        print(x)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIVCkUkE_IjR"
   },
   "source": [
    "#### Exercise: New Vocabulary\n",
    "\n",
    "Since we now use pre-trained word embeddings, we need to create a new vocabulary. \n",
    "This is because of two reasons:\n",
    "\n",
    "1. We do not have pre-trained word embeddings for all words in our SST training set, and we do not want words in our vocabulary for which we have no word embeddings.\n",
    "2. We should be able to look up the pre-trained word embedding for words in the validation and test set, even if these words are unseen in training. \n",
    "\n",
    "Now, create a new vocabulary object `v` based on the word set of pre-trained embeddings, and load the corresponding embeddings into a list `vectors`.\n",
    "\n",
    "The vocabulary `v` should consist of:\n",
    " - a  `<unk>` token at position 0,\n",
    " - a  `<pad>` token at position 1, \n",
    " - and then all words in the pre-trained embedding set.\n",
    " \n",
    "\n",
    "After storing each vector in a list `vectors`, turn the list into a numpy matrix like this:\n",
    "```python\n",
    " vectors = np.stack(vectors, axis=0)\n",
    "```\n",
    "\n",
    "Remember to add new embeddings for the `<unk>` and `<pad>` tokens, as they're not part of the word2vec/GloVe embeddings. These embeddings can be randomly initialized or 0-valued, think about what makes sense and see what the effects are.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ITyyCvDnCL4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "v = Vocabulary()\n",
    "print(v.w2i)\n",
    "print(v.i2w)\n",
    "vectors = []\n",
    "with open(\"./glove.840B.300d.sst.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for x in f:\n",
    "     \n",
    "        split = x.split(' ')\n",
    "       \n",
    "        word = split[0]\n",
    "#         print(word)\n",
    "        \n",
    "        split = split[1:]\n",
    "        \n",
    "        v.count_token(word)\n",
    "        #vv.add_token(word)\n",
    "        \n",
    "        \n",
    "        embedding = np.zeros(300)\n",
    "\n",
    "        i = 0\n",
    "        for element in split:\n",
    "            embedding[i] = float(element)\n",
    "            i += 1\n",
    "        vectors.append(embedding)\n",
    "    \n",
    "\n",
    "vectors.insert(0,np.zeros(300))\n",
    "vectors.insert(0,np.zeros(300))\n",
    "vectors = np.stack(vectors, axis = 0)\n",
    "v.build()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./glove.840B.300d.sst.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for x in f:\n",
    "        print(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['<unk>', '<pad>', ',', '.', 'the', 'and', 'to', 'of', 'a', 'in', ':', 'is', 'for', 'I', ')', '(', 'that', '-', 'on', 'you', 'with', \"'s\", 'it', 'The', 'are', 'by', 'at', 'be', 'this', 'as', 'from', 'was', 'have', 'or', '...', 'your', 'not', '!', '?', 'will', 'an', \"n't\", 'can', 'but', 'all', 'my', 'has', 'do', 'we', 'they', 'more', 'one', 'about', 'he', ';', \"'\", 'out', '$', 'their', 'so', 'his', 'up', 'It', '&', 'like', '/', '1', 'which', 'if', 'would', 'our', 'me', 'who', 'just', 'This', 'time', 'what', 'A', '2', 'had', 'when', 'there', 'been', 'some', 'get', 'were', 'other', 'also', 'In', 'her', 'them', 'You', 'new', 'We', 'no', 'any', 'people', 'than', 'into', 'only', '3', 'how', 'its', 'first', 'said', 'If', 'over', 'make', 'good', 'know', 'very', '%', 'am', 'now', 'see', 'may', 'she', 'could', 'most', 'then', \"'m\", 'use', 'these', 'did', 'And', 'way', 'New', '4', 'here', 'well', 'work', 'two', '=', 'think', 'much', 'should', 'us', 'years', 'many', 'back', 'because', '5', '#', 'For', 'after', 'He', 'need', 'year', 'even', 'does', 'really', 'through', 'want', 'great', 'him', 'where', 'go', 'day', '--', 'such', '10', '*', 'right', 'best', 'made', \"'re\", 'views', 'But', 'find', 'take', 'being', \"'ve\", 'those', 'information', 'before', 'used', 'off', 'love', 'home', 'too', '+', 'last', 'What', 'going', 'help', 'To', 'still', 'free', 'down', 'There', 'All', 'May', 'They', '6', 'life', 'same', 'around', \"'ll\", 'world', 'My', 'own', 'little', 'while', 'each', 'look', 'By', 'say', 'got', 'So', 'business', '7', 'As', 'That', 'part', 'both', 'long', 'better', 'every', 'Home', 'few', 'ago', 'things', 'between', '8', 'something', 'game', 'With', 'No', 'available', 'never', 'using', 'place', 'under', 'come', '12', 'online', 'another', 'system', 'found', 'How', 'When', 'days', 'without', 'always', 'On', 'different', '20', 'next', 'set', 'must', 'since', 'high', 'video', 's', 'sure', 'lot', 'Your', 'again', 'More', 'number', 'service', 'three', 'including', 'give', 'She', 'during', 'show', 'end', 'family', 'looking', 'thing', 'name', 'One', 'post', 'might', 'today', 'week', '15', 'old', 'why', 'put', 'Do', 'read', '11', '9', 'company', 'money', 'top', '30', 'big', 'man', 'book', 'area', 'full', 'page', 'keep', 'away', 'against', 'small', 'feel', 'support', 'ca', 'real', 'Of', 'times', \"'d\", 'says', 'school', 'play', 'left', 'team', 'water', 'point', 'music', 'Our', 'ever', 'having', 'News', 'second', 'able', 'order', 'children', 'within', 'City', 'Now', 'though', 'start', 'American', 'months', 'services', 'experience', 'car', 'until', 'case', 'making', 'getting', 'done', 'God', 'state', 'provide', 'course', 'At', 'list', 'June', 'change', 'local', 'enough', 'night', '100', 'hard', 'person', 'April', '18', 'let', 'line', 'From', '13', 'website', 'least', 'live', 'power', 'try', '25', 'doing', 'important', 'working', 'less', 'quality', 'group', 'care', 'makes', 'thought', 'side', 'call', 'United', 'per', 'room', 'actually', 'These', 'program', 'public', 'John', 'products', 'January', 'following', 'job', 'yet', 'called', 'buy', 'hours', 'already', 'Is', 'price', 'easy', 'design', 'far', 'World', 'please', 'State', 'THE', 'include', 'problem', 'open', 'health', 'food', 'women', 'US', 'based', 'went', 'body', 'County', 'house', 'process', 'needs', 'story', 'York', 'search', 'large', 'Not', 'together', 'came', 'bit', 'After', 'someone', 'run', 'Here', 'others', 'de', 'anything', 'often', 'fact', 'means', 'offer', 'country', 'control', 'once', 'friends', 'view', 'become', 'September', 'possible', 'several', 'Black', 'along', 'bad', 'market', 'city', 'fun', 'believe', 'nice', 'pretty', 'Please', 'product', 'August', 'seen', 'Day', 'web', 'form', 'took', 'results', 'Just', 'points', 'students', 'games', 'Center', 'past', 'started', 'School', '19', 'South', 'tell', 'level', 'Business', 'news', '22', 'government', 'kind', 'members', 'comes', 'head', 'light', 'Price', 'check', 'hand', 'minutes', 'North', 'TV', 'U.S.', 'hope', 'National', 'black', 'type', '50', 'above', 'whole', 'probably', 'software', 'season', 'everything', 'below', 'Thanks', 'either', 'Read', 'example', 'four', 'community', 'trying', 'Video', 'special', 'review', 'anyone', 'million', 'idea', 'States', 'access', 'added', 'contact', 'add', 'men', 'An', 'given', 'However', 'everyone', 'Search', 'value', 'Last', 'phone', 'quite', 'House', 'nothing', 'known', 'early', 'near', 'current', 'First', 'pay', 'Best', 'later', 'research', 'mind', 'question', 'personal', 'See', 'project', 'young', 'plan', 'told', 'offers', 'almost', 'development', 'movie', 'Internet', 'Some', 'true', 'link', 'month', 'Great', 'About', 'version', 'event', 'create', 'mean', 'future', 'size', 'West', 'taking', 'words', 'face', 'single', 'features', 'Time', 'close', 'white', 'beautiful', 'looks', 'via', 'history', 'reading', 'series', 'America', 'across', 'cost', 'seems', 'else', 'Why', 'visit', 'short', 'song', '26', 'front', 'works', 'White', 'Park', 'High', 'wanted', 'class', 'law', 'coming', 'whether', 'sex', 'share', 'problems', 'perfect', 'Get', 'weeks', 'His', 'film', 'low', 'taken', 'reason', 'details', 'soon', 'party', 'complete', 'child', 'questions', 'card', 'range', 'report', 'sale', 'rather', 'property', 'comments', 'Post', 'Love', 'hot', 'hotel', 'friend', 'issue', 'Well', 'understand', 'due', 'learn', 'heart', 'computer', 'office', 'Music', 'kids', 'stop', 'especially', 'English', 'watch', 'Friday', 'provides', 'simple', 'art', 'major', 'issues', 'UK', 'move', 'content', 'training', 'building', 'etc.', 'asked', 'miles', 'credit', 'couple', 'industry', 'Date', 'original', 'age', 'air', 'AND', 'five', 'date', 'Good', 'includes', 'Other', 'Street', 'study', 'half', 'social', 'shows', 'space', 'photos', 'present', 'happy', 'matter', 'turn', 'needed', 'David', 'gets', 'image', 'performance', 'media', 'Then', 'human', 'simply', 'stay', 'running', 'deal', 'living', 'girl', 'style', 'London', 'provided', 'ask', 'main', 'enjoy', 'member', 'energy', 'required', 'bring', 'further', 'Most', 'technology', 'upon', 'Are', 'playing', 'action', 'Add', 'rate', 'San', 'comment', 'saw', 'insurance', 'sound', 'code', 'takes', 'talk', 'Hotel', 'test', 'While', 'Sunday', 'key', 'stuff', 'woman', 'however', 'related', 'win', 'Saturday', 'California', 'various', 'among', 'usually', 'meet', 'parts', 'guys', 'behind', 'word', 'result', 'amount', 'store', 'yourself', 'wo', 'areas', 'position', 'return', 'field', 'interest', 'Red', 'guy', 'books', 'events', 'myself', 'hit', 'longer', 'Yes', 'leave', 'Big', 'items', 'total', 'remember', 'Mr.', 'wrong', 'address', 'lost', 'message', 'rest', 'natural', 'picture', 'cause', 'general', 'goes', 'created', 'favorite', 'color', 'drive', 'clear', 'outside', 'click', 'Part', 'paper', 'Life', 'shall', 'held', 'inside', 'Buy', 'private', '40', 'currently', 'morning', 'model', 'similar', 'maybe', 'designed', 'professional', 'heard', 'internet', 'writing', 'lead', 'hear', 'security', 'certain', 'cover', 'rights', 'Group', 'Full', 'period', 'account', 'sales', 'Can', 'third', 'pictures', 'common', 'addition', 'Washington', 'subject', 'record', 'likely', 'St.', 'saying', 'Monday', 'sites', 'hair', 'written', 'changes', 'James', 'source', 'received', 'continue', 'ready', 'photo', 'customers', 'sense', 'interesting', 'recent', 'Art', 'forward', 'weight', 'popular', 'reviews', 'wrote', 'ways', 'latest', 'Green', 'oil', 'instead', 'players', 'Inc.', 'East', 'Road', 'cut', 'strong', 'death', 'General', 'recently', 'save', 'thanks', 'late', 'Dr.', 'network', 'red', 'education', 'financial', 'cool', 'included', 'treatment', 'Thank', 'unique', 'Paul', 'send', 'write', 'baby', 'seem', 'player', 'Show', 'section', 'Company', 'Photo', 'Like', 'choose', 'thinking', 'amazing', 'choice', 'according', 'Christmas', 'travel', 'Blue', 'wish', 'Let', 'People', 'Michael', 'China', 'fine', 'center', 'allow', 'wait', 'increase', 'chance', 'final', 'Who', 'answer', 'Up', 'played', 'fast', 'likes', 'worth', 'Department', 'programs', 'release', 'box', 'material', 'tax', 'location', 'specific', '2000', 'ones', 'meeting', 'town', 'entire', 'individual', 'eyes', 'themselves', 'Texas', 'Real', 'girls', 'board', 'Wednesday', 'Be', 'particular', 'higher', 'huge', 'Women', 'medical', 'term', 'Club', 'Two', 'receive', 'marketing', 'hold', 'text', 'Book', 'tried', 'table', 'Oh', 'Me', 'itself', 'B', 'summer', 'gave', 'build', 'follow', 'easily', 'Canada', 'loss', 'Family', 'worked', 'Report', 'cheap', 'road', 'options', 'additional', 'wide', 'India', 'standard', 'Have', 'Next', 'opportunity', 'decided', 'step', 'role', 'lives', 'pages', 'national', 'ideas', 'parents', 'terms', 'collection', 'international', 'method', 'piece', 'S', 'Many', 'practice', 'title', 'skin', 'six', 'policy', 'door', 'green', 'wife', 'DVD', 'gives', 'excellent', 'Contact', 'knowledge', 'stock', 'production', 'interested', 'war', 'quickly', 'risk', 'Beach', 'son', 'sometimes', 'images', 'Or', 'Even', 'System', 'throughout', 'track', 'videos', 'starting', 'giving', 'lower', 'groups', 'looked', 'average', 'talking', 'shot', 'effect', 'wonderful', 'completely', 'political', 'hands', 'dog', 'moment', 'activities', 'sent', 't', 'knew', 'land', 'Game', 'Back', 'involved', 'band', 'Church', 'difficult', 'finally', '60', 'success', 'Check', 'built', 'felt', 'screen', 'Lake', 'himself', 'student', 'fit', 'II', 'clean', 'wants', 'born', 'album', 'fire', 'hour', '2002', 'language', 'potential', 'device', 'attention', 'equipment', 'feet', 'Live', 'King', 'mother', 'ability', 'Car', 'fall', 'extra', 'former', 'Year', 'variety', 'bed', 'cases', 'Since', 'necessary', 'Will', 'guess', 'allows', 'turned', 'Go', 'plans', 'published', 'blood', 'leading', 'effects', 'types', 'response', 'costs', 'began', 'focus', 'became', '`', 'consider', 'levels', 'learning', 'daily', 'happen', 'resources', 'Apple', 'No.', 'Make', 'weekend', 'exactly', 'wedding', 'Only', 'growth', 'note', 'Where', 'feature', 'sell', 'brought', 'ground', 'speed', 'base', 'trip', 'decision', 'pain', 'plus', 'Set', 'notes', 'environment', 'Public', 'X', 'D', 'Man', 'released', 'skills', 'rates', 'Series', 'effective', 'court', 'conditions', 'career', 'college', 'character', 'voice', 'War', 'CD', 'solution', 'safe', 'although', 'Mark', 'Association', 'digital', 'police', 'walk', 'break', 'directly', 'round', 'item', 'difference', 'shown', 'Press', 'sign', 'M', 'option', 'tools', 'projects', 'moving', '2001', 'places', 'function', 'situation', 'numbers', 'definitely', 'providing', 'Digital', 'stories', 'won', 'stand', 'Europe', 'paid', 'George', 'previous', 'display', 'feeling', 'French', 'floor', 'gift', 'race', 'legal', 'modern', 'Star', 'pick', 'camera', 'Watch', 'guide', 'yes', 'eat', 'lyrics', 'Jesus', 'pass', 'engine', 'estate', 'goal', 'gone', 'Sports', 'E', 'lots', 'limited', 'British', 'England', 'nature', 'cards', 'fans', 'expected', 'awesome', 'List', 'spent', 'Christian', 'certainly', 'entry', 'History', 'considered', 'planning', 'spend', 'claim', 'relationship', 'benefits', 'Games', 'blue', 'alone', 'J.', 'charge', 'machine', 'approach', 'Science', 'Robert', 'lines', 'Once', 'quick', 'brand', 'highly', 'schools', 'church', 'beginning', 'Food', 'Brown', 'match', 'W', 'changed', 'Men', 'homes', 'uses', 'unit', 'economic', 'ensure', 'Take', 'happened', 'Her', 'shop', 'PC', 'fully', 'movies', 'Program', 'bar', 'Old', 'songs', 'father', 'regular', 'Call', 'Another', 'sort', 'improve', 'Its', 'clients', 'expect', 'Australia', 'sounds', 'normal', 'fan', 'Each', 'reports', 'Chicago', 'memory', 'Bill', 'Hot', 'advice', 'deep', 'loved', 'serious', 'Project', 'bought', 'A.', 'Because', 'trade', 'n', 'multiple', 'Out', 'River', 'died', 'begin', 'pressure', 'towards', 'states', 'fight', 'knows', 'William', 'rooms', 'materials', 'analysis', 'gas', 'helps', 'Island', 'successful', 'b', 'European', 'cars', 'nearly', 'map', 'tool', 'direct', 'Although', 'stage', 'Care', 'author', 'com', 'Chris', 'developed', 'channel', 'eye', 'Valley', 'join', 'thank', 'fresh', 'Any', 'ball', 'watching', 'gold', 'official', 'basic', 'bottom', 'copy', 'rules', 'perhaps', 'whose', 'Times', 'Smith', 'update', 'double', 'force', 'moved', 'funny', 'Very', 'drug', 'military', '80', 'traditional', 'touch', 'gay', 'sold', 'commercial', 'cell', 'Mike', 'Light', 'significant', 'allowed', 'physical', 'apply', 'region', 'L', 'truly', 'window', 'activity', 'offered', 'growing', 'act', 'club', 'wall', 'require', 'poor', 'cash', 'names', 'middle', 'reported', 'Special', 'positive', 'met', 'press', 'Lord', 'evidence', 'surface', 'reach', 'star', '90', 'Long', 'vehicle', 'effort', 'led', 'heat', 'Water', 'whatever', 'weather', 'seeing', 'impact', 'Does', 'recommend', 'Over', 'construction', 'shopping', 'Mary', 'hotels', 'Maybe', 'sports', 'finish', 'N', 'selection', 'dead', 'daughter', 'waiting', 'Rock', 'Thomas', 'useful', 'figure', 'dark', 'husband', 'degree', 'boy', 'cancer', 'Little', 'science', 'Los', 'named', 'benefit', 'showing', 'purpose', 'rock', 'condition', 'earlier', 'Peter', 'vote', 'president', 'cold', 'beyond', 'updated', 'seconds', 'onto', 'Chinese', 'businesses', 'disease', 'reasons', 'associated', 'glass', 'healthy', 'supply', 'straight', 'develop', 'Us', 'Super', 'basis', 'Indian', 'easier', 'NOT', 'S.', 'setting', 'finished', 'lose', 'wear', 'teams', 'Hill', 'sweet', 'Welcome', 'connection', 'particularly', 'helpful', 'profile', 'offering', 'actual', 'budget', 'except', 'models', 'campaign', 'characters', 'street', 'kept', 'culture', 'families', 'opinion', 'calls', 'Director', 'traffic', 'Under', 'pool', 'opening', 'print', 'statement', 'driving', 'speak', 'delivery', 'Hall', 'La', 'powerful', 'tour', 'button', 'overall', 'bank', 'radio', 'truth', 'unless', 'immediately', 'kitchen', 'Bank', 'individuals', 'Children', 'buying', 'economy', 'regarding', 'fish', 'older', 'respect', 'holiday', 'generally', 'understanding', 'Box', 'responsible', 'Did', 'damage', 'enter', 'T', 'served', 'beach', 'correct', 'Night', 'finding', 'appear', 'notice', 'Joe', 'extremely', 'France', 'creating', 'letter', 'Baby', 'avoid', 'contains', '70', 'Tom', 'solutions', 'passed', 'R', 'stars', 'Parts', 'appropriate', 'serve', 'comfortable', 'Room', 'discussion', 'background', 'totally', 'greater', 'Wedding', 'scene', 'Every', 'topic', 'Journal', 'forum', 'wonder', 'director', 'mine', 'sleep', 'produce', 'Silver', '1999', 'records', 'and/or', 'package', 'married', 'investment', 'ahead', 'grow', 'annual', 'debt', 'M.', 'status', 'attack', 'bag', 'reference', 'battery', 'Plus', 'runs', 'Movie', 'miss', 'Christ', 'lack', 'Japan', 'happens', 'J', 'structure', 'IS', 'beat', 'la', 'population', 'Johnson', 'minute', 'bill', 'Africa', 'Richard', 'prior', 'mentioned', 'followed', 'helped', 'drop', 'metal', 'F', 'society', 'larger', 'fat', 'compared', 'pieces', 'Society', 'evening', 'shoes', 'challenge', 'contract', 'carry', 'hate', 'Their', 'welcome', 'Western', 'football', 'dating', 'Angeles', 'learned', 'fashion', 'Right', 'length', 'host', 'vs.', 'century', 'During', 'spot', 'self', 'movement', 'protect', '45', 'Young', 'Co.', 'selling', 'C.', 'studies', 'wine', 'park', 'error', 'reduce', 'Israel', 'tickets', 'described', 'absolutely', 'cute', 'ride', 'sorry', 'sources', 'Stock', 'produced', 'block', 'dress', 'patient', 'values', 'opportunities', 'windows', 'Steve', 'friendly', 'distance', 'station', 'coffee', 'audio', 'tree', 'existing', 'established', 'colors', 'direction', '1/2', 'seven', 'forget', 'reality', 'covered', 'famous', 'female', 'complex', 'otherwise', 'nation', 'yesterday', 'listen', 'According', 'interview', 'driver', 'German', 'primary', 'thousands', 'Boston', 'warm', 'Look', 'Lee', 'heavy', 'steps', 'doctor', 'biggest', 'thoughts', 'classic', 'sharing', 'efforts', 'handle', 'advantage', '1998', 'Daily', 'therefore', 'ten', 'Three', 'appears', 'requires', 'et', 'seemed', 'category', 'mix', 'ice', 'devices', 'rich', 'placed', 'Total', 'forms', 'Scott', 'exercise', 'closed', 'Japanese', 'beauty', 'adult', 'plays', 'score', 'goals', 'H', 'killed', 'soft', 'leader', 'Girl', 'dance', 'standards', 'dinner', 'putting', 'dry', 'manager', 'liked', 'Country', 'clearly', 'thus', 'communication', 'Girls', 'artist', 'End', 'plant', 'Play', 'presented', 'helping', 'experienced', 'foot', 'fair', 'Edition', 'spring', 'die', 'Happy', 'accept', 'prevent', 'highest', 'sun', 'starts', 'owners', 'Before', 'target', 'toward', 'solid', 'w', 'format', 'completed', 'becomes', 'brother', 'testing', 'doubt', 'Work', 'classes', 'Bush', 'continued', 'Case', 'Those', 'walking', 'competition', 'net', 'Americans', 'Job', 'central', 'parties', 'deals', 'properties', 'Martin', 'advanced', 'Earth', 'calling', 'keeping', 'remain', 'meaning', 'Sex', 'Hey', 'winter', 'Germany', 'shape', 'mention', 'balance', 'mostly', 'stores', 'elements', 'Ford', 'Using', 'Color', 'alternative', 'eating', 'theme', 'leaving', 'ended', 'trust', 'sexy', 'brain', 'technical', 'menu', 'claims', 'boys', 'animals', 'expensive', 'demand', 'wood', 'teaching', 'mom', 'separate', 'glad', 'opened', '300', 'Museum', 'caused', 'League', 'Jack', 'session', 'train', 'asking', 'feed', 'path', 'Both', 'workers', 'creative', 'drugs', '**', 'Jones', 'enjoyed', 'independent', 'dream', 'missing', 'peace', 'decide', 'Mexico', 'spending', 'foreign', 'teen', 'cable', 'gain', 'sit', 'teacher', 'slow', 'fear', 'Jersey', 'Summer', 'mode', 'plenty', 'Jim', 'recipe', 'aware', 'Spring', 'document', 'Off', 'Way', 'Shipping', 'sets', 'strategy', 'earth', 'operating', 'Place', 'flow', 'Auto', 'Fire', 'super', 'nor', 'television', 'meant', 'pair', 'laws', 'male', 'kill', 'lovely', 'sea', 'showed', 'taste', 'Kids', 'concept', 'strength', 'majority', 'frame', 'attempt', 'eight', 'anyway', 'determine', 'faith', 'fill', 'ring', 'perform', 'proper', 'cup', 'Garden', 'Class', 'adding', 'Order', 'Looking', 'smaller', 'critical', 'sitting', 'whom', 'discuss', 'mouth', 'discount', 'dollars', 'units', 'ideal', 'partner', 'Movies', 'busy', 'library', 'corporate', 'Bob', 'porn', 'fantastic', 'drink', 'Bar', 'crazy', 'description', '75', 'rent', 'agency', 'trial', 'ship', 'realize', 'Building', 'edge', 'Events', 'slightly', 'excited', 'department', 'becoming', 'marriage', 'maintain', 'guests', 'combination', 'object', 'returned', 'schedule', 'willing', 'transfer', 'Second', 'G', 'continues', 'Charles', 'essential', 'winning', 'flight', 'filled', 'leaves', 'mail', 'possibly', 'failed', 'plastic', 'hospital', 'Jackson', 'caught', 'seat', 'Still', 'holding', 'changing', 'bike', 'prepared', 'OK', 'Human', 'suggest', '1997', 'dogs', 'gonna', 'dedicated', 'W.', 'anywhere', 'artists', 'hearing', 'developing', 'detail', 'Canadian', '3D', 'rental', 'worse', 'Paris', 'senior', 'count', 'documents', 'R.', 'raised', 'techniques', 'Dog', 'Santa', 'trouble', 'manage', 'electronic', 'Williams', 'birthday', 'agreement', 'explain', 'Master', 'fuel', 'audience', 'volume', 'Yeah', 'Body', 'guitar', 'luck', 'proud', 'defense', 'fix', 'corner', 'remains', 'Town', 'O', 'hell', 'Features', 'Personal', 'bus', 'managed', 'removed', 'progress', 'Police', 'o', 'brings', 'connected', 'animal', 'quarter', 'joined', 'Festival', 'shared', 'Was', 'Professional', 'kid', 'flat', 'Action', 'Property', 'capacity', 'mission', 'Resources', 'Would', 'achieve', 'supposed', 'Credit', 'despite', 'Francisco', 'wind', 'applied', '48', 'cast', 'Which', 'exciting', 'oh', 'portion', 'updates', 'P', 'core', 'golf', 'plants', 'ran', 'none', 'reached', 'Georgia', 'pics', 'Heart', 'negative', 'battle', 'agent', 'episode', 'load', 'K', 'cream', 'generation', 'silver', 'Spanish', 'tomorrow', 'Talk', 'missed', 'appreciate', 'Story', 'flowers', 'waste', 'Bible', 'sister', 'advance', 'birth', 'detailed', 'Mr', 'factors', 'catch', 'bedroom', 'vacation', 'edition', 'Money', 'numerous', 'cities', 'D.', 'master', 'actions', 'performed', 'Point', 'Start', 'Film', 'teachers', 'Low', 'folks', 'Hope', 'lived', 'imagine', 'worry', 'square', 'recommended', 'Down', 'magazine', 'Magazine', 'L.', 'presence', 'Entertainment', 'spread', 'yeah', 'minimum', 'parking', 'election', 'exchange', 'Award', 'broken', 'Italian', 'comfort', 'religious', 'turns', 'treat', 'rise', 'sexual', 'afternoon', 'Episode', 'eventually', 'houses', 'Nice', 'boat', 'maximum', 'sample', 'relevant', 'pull', 'species', 'limit', 'stopped', 'entertainment', 'Pictures', 'abuse', 'markets', 'twice', 'Made', 'raise', 'register', 'paying', 'tonight', 'tough', 'administration', 'noticed', 'housing', 'standing', 'theory', 'wearing', 'seriously', 'serving', 'greatest', 'Asian', 'apartment', 'experiences', 'hits', 'telling', 'seeking', 'switch', 'signs', 'Matt', 'Hospital', 'deliver', 'feels', 'designs', 'rating', 'B.', 'Dark', 'concerned', 'trees', 'fees', 'manner', 'Private', 'Dr', 'messages', 'sugar', 'Space', 'faster', 'leads', 'cat', 'throw', 'internal', 'stress', 'ends', 'tells', 'forces', 'draw', 'guest', 'initial', 'Should', 'centre', 'indeed', 'gallery', 'layer', 'prescription', 'edit', '42', 'gun', 'input', 'sad', 'African', 'pack', 'properly', 'facts', 'Gift', 'Southern', 'charges', 'Need', 'secret', 'lunch', 'purposes', 'intended', 'residents', 'responsibility', 'stupid', 'Ryan', 'clothes', 'behavior', 'searching', 'loves', 'freedom', 'functions', 'Large', 'answers', 'medium', 'lights', 'stick', 'Sorry', 'previously', 'cross', 'failure', 'prefer', 'electric', 'Middle', 'forced', 'covers', 'films', 'Player', 'mark', 'PA', 'Louis', 'task', 'vision', 'ad', 'Child', 'allowing', 'environmental', 'scale', 'bigger', 'launch', 'specifically', 'worst', 'Friends', 'usual', 'Brian', 'arms', 'writer', 'locations', 'causes', 'paint', 'desire', 'topics', 'components', 'appeared', 'speech', 'decisions', 'brown', 'featuring', 'Remember', 'Leave', 'fell', 'sides', 'Sea', 'Land', '1995', 'Cheap', 'orders', 'wireless', 'leather', 'maintenance', 'hurt', 'III', 'automatically', 'combined', 'Easy', 'tea', 'suit', 'snow', 'spirit', 'chicken', 'license', 'award', 'reserved', 'Australian', 'picked', 'influence', 'expert', 'discovered', 'owned', 'U', 'Andrew', 'practices', 'installation', 'arrived', 'Days', 'aid', 'Original', 'relationships', 'truck', 'zone', 'obvious', 'compare', 'smart', 'professionals', 'THIS', '65', 'island', 'kinds', 'increasing', 'determined', 'treated', 'rain', 'contain', 'command', 'edited', 'bathroom', 'sick', 'considering', 'hundreds', 'measure', 'Whether', 'wild', 'teach', 'Italy', 'Creek', 'vehicles', 'Things', 'Four', 'surprised', 'Dave', 'begins', 'Ben', 'keeps', 'Band', 'Glass', 'pop', 'milk', 'Russian', 'horse', 'measures', 'obviously', 'factor', 'formed', 'Weight', 'recorded', 'upper', 'element', 'fly', 'taxes', 'roll', 'novel', 'false', 'chocolate', 'Age', 'speaking', 'push', 'resolution', 'Coast', 'gifts', 'Tim', 'exist', 'defined', 'mass', 'bright', 'i.e.', 'emergency', 'bet', 'Message', 'Asia', 'crime', 'fishing', 'Miami', 'seek', 'anymore', 'Q', 'tend', 'somewhere', 'journey', 'Awards', 'comprehensive', 'improved', 'committed', 'Song', 'Academy', 'airport', 'Never', 'soul', 'instance', 'yellow', 'herself', 'interests', 'studio', 'introduced', 'De', 'Hollywood', 'Pink', 'charged', 'fourth', 'grade', 'adults', 'recording', '400', 'Daniel', 'tight', 'quiet', 'ads', 'Sometimes', 'camp', 'replace', 'fruit', 'Taylor', 'appearance', 'affect', 'Henry', 'Has', 'university', 'Wow', 'Side', 'knowing', 'aspects', 'fighting', 'goods', 'Queen', 'winner', 'mental', 'Double', 'Try', 'rare', 'Sarah', 'legs', 'closer', 'Golden', 'agents', 'hole', 'presentation', 'Again', 'surprise', 'Frank', 'Lady', 'pet', 'watched', 'beer', 'ourselves', 'recovery', 'connect', 'Dan', 'Dance', 'educational', 'Wood', 'Harry', 'invention', 'shooting', 'Rose', 'Jewish', 'cheese', 'focused', 'featured', 'chain', 'concerns', 'remote', 'Complete', 'hardware', 'Through', 'familiar', 'authority', 'examples', 'carried', 'youth', 'Short', 'depending', 'choices', 'consumers', 'flash', 'shots', 'Finally', 'Trade', 'village', 'suitable', 'Electric', 'perfectly', 'Ca', 'listening', 'debate', 'Joseph', 'Leather', 'Kevin', 'religion', 'Beautiful', 'reduced', 'river', 'evil', 'cultural', 'accurate', 'bringing', 'follows', 'Death', 'Jeff', 'Cross', 'trading', 'executive', 'creation', 'challenges', 'F.', 'apart', 'therapy', 'buildings', 'receiving', '20th', 'Without', 'definition', 'agreed', 'Mother', 'importance', 'Hard', '1994', 'equal', 'confidence', 'partners', 'doors', 'hoping', 'Boy', 'historical', 'Ireland', 'Catholic', 'reasonable', 'Yet', 'tracks', 'fields', 'motion', 'losing', 'delivered', 'sheet', 'efficient', 'honest', 'violence', 'argument', 'context', 'mature', '120', 'concern', 'cooking', 'clothing', 'arm', 'bags', 'smile', 'Quick', 'Adam', 'plate', 'Disney', 'officer', 'smooth', 'filter', 'judge', 'retail', 'originally', 'turning', 'Northern', 'pink', 'Perhaps', 'climate', 'Indiana', 'degrees', 'Field', 'Sam', 'till', 'Instead', 'chief', 'positions', 'conversation', 'settings', 'Performance', '99', 'dates', 'massive', 'Bad', 'shoot', 'personally', 'lucky', 'chosen', 'Being', 'ticket', 'Seattle', 'Jason', 'photography', 'ordered', 'basically', 'Station', 'Father', 'cake', 'CEO', 'pleasure', 'jump', 'Sound', 'Holy', 'medicine', 'salt', 'chapter', 'musical', 'supplies', 'attached', 'talks', 'nine', 'log', 'talent', 'route', 'extended', 'Spain', 'Linux', 'holds', 'Enjoy', 'height', 'league', 'fairly', 'Having', 'typically', 'meat', 'entirely', 'walked', 'shower', '95', 'attacks', 'lady', 'stone', 'Speed', 'Tony', 'El', 'Orange', 'Meeting', 'letters', 'matters', 'perspective', 'accident', 'prove', 'crowd', 'thinks', 'millions', 'taught', 'Winter', 'Want', 'typical', 'pulled', 'forever', 'bunch', 'politics', 'stands', 'entered', 'Patch', 'happening', 'sport', 'returns', 'Beauty', 're', 'wondering', 'lets', 'competitive', 'moves', 'planned', 'saved', 'relatively', 'Unfortunately', 'Stage', 'Dallas', 'everyday', 'cutting', 'remaining', 'machines', 'Columbia', 'dangerous', 'extensive', 'Miss', 'f', 'gear', 'crisis', 'god', 'practical', '***', 'label', 'inspired', 'Final', 'Give', 'chair', 'discover', 'Channel', 'fabric', 'Eric', 'tiny', 'minor', 'engineering', 'lighting', 'Could', 'Max', 'shares', 'Modern', 'writes', 'Everything', 'objects', 'organic', 'east', 'Van', 'Russia', 'prepare', 'civil', 'candidate', 'painting', 'valuable', 'impossible', 'Alex', 'mixed', 'farm', 'phones', 'Everyone', 'enable', 'fellow', 'employment', 'apparently', 'suggested', 'OS', 'wins', 'figures', 'LA', 'supporting', 'pleased', 'expression', 'nude', 'telephone', 'somewhat', 'Stars', 'Diego', 'Yellow', 'neck', 'realized', '51', 'meal', 'reliable', 'slowly', 'encourage', 'upcoming', 'dropped', 'drawing', 'tired', 'cycle', 'lists', 'mountain', 'breast', 'ages', '1993', 'interior', 'Express', 'Denver', 'Boys', 'represent', 'hire', 'versions', 'Machine', '7th', '19th', 'Too', 'exact', 'normally', 'Content', 'comparison', 'explore', 'capable', 'graphics', 'Nothing', 'scientific', 'Ask', 'ancient', 'ingredients', 'occur', 'luxury', 'dad', 'Mrs.', 'walls', 'technologies', 'pure', 'ya', 'guarantee', 'participate', 'random', 'hundred', 'Really', 'Though', 'commitment', '21st', 'incredible', 'Miller', '1990', 'academic', 'heads', '52', 'illegal', 'pounds', 'Stephen', '10th', 'bear', 'unable', 'monitor', 'Davis', 'editor', '72', 'string', 'procedure', 'printed', 'nuclear', 'fail', 'wet', 'Mom', 'Five', 'frequently', '101', 'arts', 'visual', 'magic', 'respond', 'appeal', 'passing', 'Fun', 'parent', 'Making', 'G.', 'duty', 'effectively', 'Al', 'colour', 'acid', '8th', 'Irish', 'script', 'Jr.', 'Austin', 'native', 'Elizabeth', 'Iran', 'domestic', 'meetings', 'earned', 'improvement', 'Run', 'noise', 'Such', 'virtual', 'Theatre', '1992', 'describe', 'reporting', 'clinical', 'constantly', 'Stone', 'Know', 'Spirit', 'strategies', 'bath', 'True', 'affected', 'afraid', 'strange', 'Eastern', 'Fast', 'Picture', 'Ray', 'younger', 'drinking', 'tank', 'surrounding', 'patterns', 'presents', 'grown', 'yours', 'outstanding', 'Cat', 'gorgeous', 'criminal', 'delicious', 'stream', 'recognized', 'relief', 'Nick', 'alive', 'Choose', 'finds', 'programming', 'facing', 'styles', 'Ball', 'kick', 'joint', 'manual', 'forth', 'sending', 'neighborhood', 'acting', 'neither', 'Notes', 'circumstances', 'stuck', 'moments', 'hidden', 'border', 'broke', 'bridge', 'grew', 'spiritual', 'Better', 'justice', 'al.', 'empty', 'reaction', 'Sweet', 'contest', 'statements', 'technique', 'tv', 'discussed', 'wherein', 'channels', 'generated', 'singing', 'exam', 'ending', 'array', 'tape', 'profit', 'ultimate', 'Wilson', 'row', 'mainly', 'everybody', '85', 'threat', 'Dead', 'assume', 'lies', 'replaced', 'carefully', 'gotten', 'Birthday', 'admit', 'muscle', 'brief', 'believed', 'atmosphere', 'damn', 'naked', 'promise', 'integrated', 'directed', 'Secretary', 'adds', 'Stop', 'impressive', 'Philadelphia', 'Anyone', 'shirt', 'cuts', 'tested', 'thick', 'scored', 'successfully', 'Teen', 'conducted', 'streets', 'concert', 'dreams', 'T.', 'honor', 'riding', 'Zealand', 'okay', 'bodies', 'possibility', 'emotional', 'opposite', 'referred', 'Return', 'exposure', 'Camera', 'mile', 'talked', 'laugh', 'bars', 'earn', 'contemporary', 'decent', 'teeth', 'faces', 'planet', 'dollar', 'campus', 'crew', 'dealing', 'lessons', 'Bridge', 'viewed', 'innovative', 'saving', 'Alabama', 'draft', 'performing', 'Greek', 'Got', 'significantly', 'Ice', 'column', 'feelings', 'differences', 'plane', 'baseball', 'express', 'participants', 'thin', 'afford', 'Tennessee', 'investigation', 'Cool', 'Wild', 'Republic', 'flying', 'bowl', 'circuit', 'aspect', 'everywhere', 'serves', 'basketball', 'pic', 'attractive', 'packages', 'pro', 'constant', 'identity', 'Don', 'Ed', 'Course', 'passion', 'extreme', 'Working', 'nights', 'Think', 'addresses', 'depends', 'valid', 'Basketball', 'decades', '1991', 'unknown', 'proof', 'revealed', 'trailer', 'communications', 'Morning', 'Howard', 'wire', 'checking', 'sky', 'viewing', 'Nature', 'Future', 'scenes', 'tube', '66', 'situations', 'van', 'Tell', 'Am', 'depth', 'Ring', 'Jordan', 'ladies', 'Utah', 'carbon', 'crack', 'immediate', 'assets', '110', 'stayed', 'recognize', 'swimming', 'controlled', 'flag', 'bands', 'joy', 'shut', 'seats', 'aside', 'split', 'Kim', 'bonus', 'killing', 'acts', 'Hills', 'labor', 'lawyer', 'Camp', 'Despite', 'authors', 'Fair', 'organized', 'Farm', 'capture', 'codes', 'pocket', 'Allen', 'clock', 'division', 'Le', 'Based', 'Woman', 'convenient', 'skill', 'extension', 'lens', 'blow', 'adventure', 'weird', 'coast', 'loving', 'efficiency', 'Pet', 'inner', 'flights', 'Jane', 'relative', '11th', 'butter', 'tables', 'primarily', '15th', 'Anyway', 'shoulder', 'Overall', 'raw', 'memories', 'Major', 'consistent', 'racing', 'Moon', 'Left', 'tall', 'wow', 'lie', 'destination', 'tone', 'Bowl', 'Lane', 'represents', 'downtown', 'flower', 'Andy', 'preferred', 'displayed', 'writers', 'boxes', '98', 'wanna', 'titles', 'Anthony', '12th', 'suffering', 'enhance', 'smoke', 'Edward', 'permanent', 'proven', 'shift', 'trained', 'generate', 'Lewis', 'convert', 'copies', 'root', 'reduction', 'informed', 'awards', 'principles', 'amounts', 'somehow', 'liquid', 'Close', 'MY', 'dynamic', 'carrying', 'resistance', 'literally', 'extent', 'headed', 'favourite', 'suddenly', 'lay', 'Latin', 'mistake', 'necessarily', 'toys', 'puts', 'severe', 'enjoying', 'Prince', 'lake', 'invited', 'Ron', 'sessions', 'Blood', 'handling', 'Works', 'Dream', 'Iron', 'breaking', 'foundation', 'permission', 'falling', 'celebrate', 'yard', 'whenever', 'Him', 'Halloween', 'zip', 'sight', 'errors', 'causing', 'answered', 'closely', 'army', 'brilliant', 'superior', 'SO', 'plain', 'Gay', 'Phoenix', 'satellite', 'hey', 'creates', 'Gary', 'Crystal', 'tradition', 'lesson', 'Animal', 'hopefully', 'conflict', 'ships', 'lifestyle', 'careful', 'orange', 'boots', 'grand', 'Ocean', 'collect', 'Anderson', 'desired', 'hang', '13th', 'boards', 'giant', 'prison', 'trend', 'Angel', 'attitude', 'historic', 'serial', 'Scotland', 'lift', 'wanting', 'counter', 'Lost', 'awareness', '1989', 'arrested', 'favor', 'harder', 'automatic', 'holidays', 'humans', 'cotton', 'greatly', 'hat', 'era', 'thousand', 'Wars', 'hopes', 'expectations', 'believes', 'refer', 'pregnant', 'Sexy', 'stunning', 'Y', 'decade', 'vast', 'mouse', 'regard', 'pump', 'covering', 'existence', 'apartments', 'warning', 'aircraft', 'contained', 'chest', 'reputation', 'Includes', 'concerning', 'garage', 'clubs', 'collected', 'Simple', 'fingers', 'Sydney', 'producing', 'ease', 'patch', 'Fish', 'murder', 'Excellent', 'inspiration', 'aim', 'matches', 'fits', 'amateur', 'rated', 'architecture', 'Below', 'Drug', 'Jennifer', 'guard', 'Third', 'generic', 'instant', 'wealth', 'reflect', 'continuing', 'assembly', 'Korea', 'kinda', 'Matthew', 'Fine', 'flexible', 'introduction', 'urban', 'establish', 'loose', 'regardless', 'mixture', 'entitled', 'sing', 'Simon', 'staying', 'weak', 'marked', 'drawn', 'terrible', 'merely', 'achieved', 'Meet', 'intelligence', 'developers', 'limits', 'wave', 'Much', 'Roman', 'factory', 'powered', 'vary', 'Atlantic', 'Enterprise', 'sees', 'Eye', 'mini', 'victims', 'festival', 'nose', 'directions', 'Stay', 'entries', 'somebody', 'setup', 'crash', 'blame', 'broad', 'accessible', 'equivalent', 'holes', 'laid', 'juice', 'falls', 'personality', 'explains', 'preparation', 'enemy', 'Cut', 'breath', 'contribute', 'certified', 'K.', 'responses', 'risks', 'Always', 'struggle', 'driven', 'meets', 'anybody', 'nobody', 'researchers', 'Something', 'suspect', 'hanging', 'Bruce', 'solo', 'expressed', 'literature', 'closing', 'Driver', 'NBA', 'Moore', 'expand', 'dude', 'correctly', 'soil', 'arrive', 'suggests', 'plot', 'widely', 'drives', 'contrast', 'trail', 'Falls', 'Smart', 'debut', 'del', 'fake', 'consideration', 'sufficient', 'dancing', 'dirty', 'Rick', 'subjects', 'belt', 'di', 'landscape', 'sensitive', 'injuries', 'suppose', 'Pool', 'piano', 'bills', 'burn', 'releases', 'chances', 'Morgan', 'Alan', '96', 'marks', 'guaranteed', 'powers', 'strike', '88', 'exists', 'ongoing', 'Grant', 'choosing', 'crap', 'forest', 'Bear', '1988', 'stretch', 'belief', 'heading', 'western', 'ratio', 'boost', 'impressed', 'photographs', 'Moving', '71', 'grass', 'visible', 'conversion', 'Ten', 'storm', '800', 'exposed', 'cameras', 'soldiers', 'Wayne', 'sentence', 'powder', 'involving', 'opens', 'partnership', 'consists', 'finger', 'granted', 'lock', 'compatible', 'indicate', 'pregnancy', 'graphic', 'pot', 'da', 'Turkey', 'vital', 'Skin', 'scientists', 'centers', 'wake', '78', 'Battle', 'Enter', 'southern', 'recognition', 'Floor', 'hardly', 'dish', 'Deep', 'Orleans', 'expansion', 'Kate', 'awarded', 'Sure', 'Jay', 'occasion', 'zero', 'ultimately', 'strongly', 'challenging', 'guidelines', 'incident', 'smell', 'escape', 'routine', 'cry', 'Lisa', '77', 'wheels', 'diverse', 'Egypt', 'commonly', 'toy', 'spots', 'singer', 'laser', 'connections', 'pace', 'bone', 'suffer', 'household', 'hide', 'vintage', 'pointed', 'worried', 'Guy', 'NYC', 'mirror', 'summary', 'mess', '84', 'Interview', 'prayer', 'BMW', 'managing', 'actor', 'strategic', 'Sir', 'Brazil', 'suck', 'ill', 'Ms.', 'dust', 'le', 'Susan', 'incredibly', 'letting', 'Kong', 'guilty', 'Tel', 'ocean', 'consumption', 'Chicken', 'drama', 'Ultimate', 'exception', 'booking', 'offense', 'naturally', 'engaged', 'structures', 'Inside', 'behalf', 'surely', 'burning', 'newly', 'Six', 'joke', 'professor', 'Jews', 'sequence', 'absolute', 'brothers', 'Assembly', 'trips', 'journal', 'boss', 'latter', 'Rates', 'clips', 'tied', 'Flowers', 'moral', 'personnel', 'promotion', '82', 'Son', 'comedy', 'Dean', 'pants', 'fault', 'Rob', 'apple', '97', 'occurs', 'largely', 'involves', 'formal', 'struck', 'Secret', 'sleeping', 'Average', 'Cook', 'Funny', 'depression', 'Amazing', 'maintained', 'opposed', 'Jon', 'healing', 'wash', 'Topics', 'victim', 'egg', 'increasingly', 'fabulous', 'favorites', 'hall', 'reducing', 'stronger', 'rising', 'Alexander', 'scores', 'Larry', 'sand', 'rural', 'remained', 'anniversary', 'confident', 'breaks', 'babies', 'confused', 'Hunter', 'requirement', 'unusual', '86', 'comic', 'Charlie', 'offensive', 'Among', 'loud', 'secondary', 'passes', 'hearts', 'talented', 'fantasy', 'guidance', 'universe', 'workshop', 'injured', 'recall', 'Lights', 'odd', 'IV', 'philosophy', 'Rules', 'Greg', 'Universal', 'observed', 'versus', 'enterprise', 'oral', 'returning', 'tie', 'Prime', 'indicated', 'Cleveland', 'delay', 'fiction', 'angle', 'ear', 'extend', 'broadcast', 'Theater', 'dealer', 'Amy', '1987', 'Mississippi', 'attempts', 'Sept.', 'bid', 'Hong', 'Anne', 'transition', 'assistant', 'bound', 'clicking', 'sharp', 'romantic', 'formula', 'nations', 'Say', 'define', 'tissue', 'represented', 'satisfied', 'prints', 'tons', '1980', 'cheaper', 'Century', 'Must', 'Awesome', 'Christians', 'mood', 'assigned', 'officially', 'Remove', 'appointed', 'Johnny', 'reception', 'Blues', 'Miles', '89', 'roads', 'Letter', 'du', 'pray', '83', 'pitch', 'equally', 'elegant', '1986', 'pride', 'heaven', 'voting', 'Ever', 'concrete', 'entrance', 'Simply', 'PR', 'entering', 'Trailer', 'Written', 'loaded', '93', 'asks', 'forgot', 'dear', 'Looks', 'rings', 'Heat', 'circle', 'Rome', 'guns', 'Olympic', 'Dragon', 'survive', 'apparatus', 'Clean', 'variable', 'Mexican', 'accuracy', 'Pretty', 'mad', 'interviews', 'Snow', 'lifetime', 'angry', 'couples', 'Anna', 'painted', 'horses', 'rarely', 'curious', 'teens', 'Gray', 'Empire', 'raising', 'Barbara', 'describes', 'satisfaction', 'deserve', '87', 'wise', 'editing', 'explanation', 'Jimmy', 'Offers', 'Jerry', 'prime', 'hitting', 'museum', 'slide', 'enforcement', 'grab', 'sin', 'bug', 'communicate', 'Grey', 'suffered', 'lately', 'Laura', 'photographer', 'figured', 'Jose', 'appreciated', 'gained', 'Ladies', '94', 'packed', 'tracking', 'barely', 'Mind', 'childhood', 'concepts', 'conclusion', 'fluid', 'Christopher', 'translation', 'conservative', 'ordinary', 'displays', 'Sean', 'Adams', 'genuine', 'objective', 'elsewhere', 'Put', 'theater', 'contents', 'compete', 'k', 'respective', 'Several', 'Songs', 'prize', 'processor', 'vegetables', 'signals', 'dose', 'narrow', 'logic', 'Going', 'portable', 'tries', 'engage', 'feat', 'reviewed', 'instrument', 'wooden', 'Berlin', 'blend', 'Billy', 'upload', 'rank', 'northern', 'craft', 'scope', 'Josh', 'Fat', 'institution', 'flavor', 'relax', 'minds', 'Vietnam', 'enables', 'moon', 'roles', 'Jonathan', 'Harris', 'intense', '1984', 'template', 'Brothers', 'coat', 'relating', 'hero', 'fifth', 'complicated', 'Usually', 'combat', 'ABC', 'Due', 'reverse', 'performances', 'Ken', 'rush', 'sucks', 'accounting', 'jacket', 'strip', 'Antonio', 'rough', 'virtually', 'maker', 'Fresh', 'essentially', 'evolution', 'Given', 'Orlando', 'respectively', 'consequences', 'participation', 'soccer', 'finest', 'mistakes', 'diamond', 'shock', 'Friend', 'credits', 'Build', 'principal', 'dresses', 'instruments', 'pushed', 'stomach', 'None', 'conventional', 'relation', 'findings', 'boring', 'contacts', 'Between', 'references', 'blind', 'overnight', 'Freedom', 'demands', '10,000', 'Michelle', 'Brooklyn', 'Stewart', 'poster', 'mechanism', 'currency', 'maintaining', 'danger', 'tears', 'Zone', 'illness', 'icon', 'worship', 'tune', 'Hours', 'spaces', 'buttons', 'wholesale', 'quit', 'innovation', 'substantial', 'argue', 'disaster', 'Traffic', 'Steven', 'proved', 'girlfriend', 'Arkansas', 'shell', 'composition', 'difficulty', 'outcome', 'Franklin', 'episodes', 'Connection', 'potentially', 'Someone', 'contributions', 'peak', 'Vice', 'stops', 'sought', 'hill', 'introduce', 'mountains', 'Charlotte', 'collaboration', 'rapidly', 'delete', 'filling', 'celebration', 'attending', 'accused', 'losses', 'origin', 'deeply', 'lips', 'lawyers', 'applying', 'acquired', 'faced', 'signature', 'Further', 'supplied', 'exit', '105', 'ears', 'Pay', 'judgment', 'courts', 'wisdom', 'Perry', 'winners', 'ha', 'Labor', 'butt', 'combine', 'Half', 'slot', 'producer', 'classroom', 'rolling', 'pleasant', 'Lawrence', 'Besides', 'destroy', 'Cold', 'Walk', 'Russell', 'divided', 'picking', 'dishes', 'Luke', 'ignore', 'disabled', 'twenty', 'pilot', 'fewer', 'lying', 'bond', 'disappointed', 'bits', 'Nation', 'contribution', 'races', 'acres', 'harm', 'Nancy', 'Bedroom', 'ink', 'bitch', 'Korean', 'Ross', 'reveal', 'impression', 'em', 'lesbian', 'Feel', 'grace', 'studied', 'themes', 'Less', 'gap', 'Rich', 'periods', 'precious', 'swing', 'defensive', 'Dad', 'glasses', 'singles', 'framework', 'Jean', 'Craig', 'UN', 'Grace', 'liability', 'Temple', 'Hell', 'Terry', 'Roger', 'cart', 'Hamilton', 'mount', 'threads', 'horrible', 'tits', 'unlike', 'Jessica', 'Israeli', 'invest', 'Solid', 'interaction', 'pushing', 'reserve', 'Lodge', 'gotta', 'excess', 'stages', 'crucial', 'damaged', 'worn', 'measured', 'convenience', 'governments', 'Duke', 'initially', 'seeds', 'instantly', 'fundamental', 'walks', 'Block', 'Soul', 'carrier', 'plug', 'sheets', 'insight', 'exceptional', 'Pearl', 'Salt', 'hip', 'vice', 'heavily', 'tutorial', 'charity', 'max', 'layers', 'Alice', 'Nike', 'preview', 'rely', 'decline', 'Had', 'Adventure', 'Until', 'dramatic', 'buyer', 'agenda', 'invite', 'seller', 'upset', 'amongst', 'loop', '104', 'rocks', 'Shows', 'honestly', 'Kennedy', 'speaks', 'applies', 'exercises', 'reaching', 'Away', 'ownership', 'publishing', 'shame', 'reads', 'approaches', 'Dick', 'declared', 'Deal', 'highlights', 'indicates', 'Ian', 'ai', 'corn', 'Against', 'sum', 'sole', 'deeper', 'colored', 'entertaining', 'addiction', 'el', 'silly', 'pizza', 'signing', 'Rice', 'trick', 'substance', 'Soft', 'workout', 'Marriage', 'colleagues', 'Teacher', 'Fred', 'mystery', 'grounds', 'phrase', 'muscles', 'expense', 'penalty', 'segment', 'Reader', 'mounted', 'refused', 'Dogs', 'Lead', 'Same', 'Format', 'attract', 'Linda', 'attractions', 'Rachel', 'intelligent', 'dot', 'captured', 'demonstrate', 'aged', 'casual', 'Hits', 'humor', 'promised', 'profits', 'arguments', '1983', 'thrown', 'banking', 'Weekend', 'ed', 'happiness', 'sorts', 'Delhi', 'advantages', 'emphasis', 'Edge', 'Carter', '1970', 'Nelson', 'produces', 'Graham', 'stylish', 'Metro', 'mid', 'Whatever', 'Truth', 'Beyond', 'representation', 'Tiger', 'Warren', 'outer', 'Commerce', 'denied', 'rescue', 'intellectual', 'lab', 'stability', 'calm', 'dying', 'Cruise', 'Karen', 'actors', 'rapid', 'ugly', 'pin', 'drops', 'Arthur', 'involve', 'junior', 'hook', '180', 'roots', 'loads', 'besides', 'patent', 'brush', 'tongue', 'dual', 'defend', 'poverty', '1982', 'Eagle', 'liberal', 'terminal', 'gender', 'waters', 'Barry', 'Into', 'highlight', 'scared', 'locate', 'nervous', 'waves', 'completion', 'Traditional', 'resume', 'spray', 'complaint', 'poetry', 'targets', 'jazz', 'duties', 'throwing', 'dozen', 'acceptable', 'Parents', 'nuts', 'watches', 'kiss', 'delivers', 'holy', 'experiment', 'universal', 'innocent', 'subscription', 'essay', 'Worth', 'pulling', 'targeted', 'subsequent', 'struggling', 'Seven', 'representing', 'timing', 'admitted', 'Per', 'expanded', 'superb', 'spectacular', 'sons', 'Historical', 'Shot', 'witness', 'static', 'expecting', 'excuse', 'compact', 'forgotten', 'Manhattan', 'scan', 'jail', 'settled', 'Guard', 'enhanced', 'Native', 'rail', 'focusing', 'Bring', 'occasionally', 'apparent', 'alert', 'ought', 'Lots', 'surrounded', 'Flat', 'finishing', 'constructed', 'frames', 'mechanical', 'beliefs', 'soup', 'Mario', 'dealers', 'void', 'no.', 'parallel', 'frequent', 'spare', 'cure', 'restore', 'unfortunately', 'arrangements', 'Parker', 'discovery', 'Heavy', 'receives', 'spoken', 'Benefits', 'survival', 'Notice', 'chronic', 'Others', 'emotions', 'Safe', 'scoring', 'Douglas', 'complaints', 'Thanksgiving', 'calories', 'enemies', 'distinct', 'Broadway', 'hosts', 'Focus', 'searches', 'Marshall', 'dressed', 'absence', 'animation', 'artwork', 'inspection', 'Upon', 'worker', 'pen', 'Knight', 'Robin', 'fails', '1979', 'convinced', 'abstract', 'examination', 'Upper', 'candy', 'Campbell', 'annoying', 'overcome', 'destruction', 'nicely', 'abroad', 'threw', 'popularity', 'franchise', 'balanced', 'Revolution', 'locked', 'refers', 'resulted', 'realistic', 'movements', 'indoor', '140', 'durable', 'concentration', 'horror', 'asset', 'collections', 'Brother', 'paintings', 'costume', 'courtesy', 'succeed', 'divorce', 'remarkable', 'tail', 'slight', 'monster', 'bother', 'championship', 'Trip', 'fascinating', 'Twin', 'violent', 'enjoyable', 'drunk', 'venture', 'ridiculous', 'reminds', 'Woods', 'involvement', 'anger', 'Indians', 'connecting', 'consistently', 'conditioning', 'Roy', 'aimed', 'unlikely', 'outlet', 'facial', 'Carl', 'champion', 'possibilities', 'identification', 'scary', 'thoroughly', 'depend', 'aggressive', 'importantly', 'suicide', 'documentary', 'diversity', 'accomplished', 'democracy', 'relaxing', 'encounter', 'Together', 'feeding', 'Eve', 'laughing', 'fallen', 'ski', 'fancy', 'deleted', 'Monster', 'Baker', 'authentic', 'Problem', 'overview', 'wing', 'sudden', 'prevention', 'begun', 'passage', 'gathering', 'Die', 'import', 'labels', 'theatre', 'profiles', 'painful', 'edges', 'killer', 'makeup', 'footage', 'plates', 'Murray', 'Minutes', 'crimes', 'Columbus', 'abilities', 'trials', 'regards', 'Driving', 'contributed', 'Indeed', 'texture', 'musicians', 'Dennis', 'spell', 'adequate', 'grateful', 'tourist', 'glory', 'dated', 'Kid', 'interpretation', 'creativity', 'Heaven', 'excitement', 'hi', 'physically', 'physician', 'sake', 'armed', 'Almost', 'genre', 'safely', 'bottles', 'Makes', 'trusted', 'Potter', 'Opera', 'territory', 'celebrity', 'ate', 'photograph', 'shaped', 'Alternative', 'Scottish', 'frozen', 'Hour', 'Sugar', 'Cooper', 'wrap', 'actress', 'emerging', 'Birmingham', 'carries', 'hired', 'beautifully', 'spin', 'Donald', 'extraordinary', '123', 'Planet', 'handy', 'Ashley', 'Boat', 'focuses', 'defeat', 'Walter', 'deaths', 'honey', 'suggestion', 'hospitals', 'blank', 'Toy', 'delivering', 'ton', 'followers', '1978', 'odds', 'toilet', 'Past', 'hence', 'roughly', 'acquire', 'silent', 'buried', 'integrity', 'skilled', 'arrest', 'settle', 'scenario', 'manufactured', 'alleged', 'grey', 'magnetic', 'Harvard', 'hungry', 'gang', 'sophisticated', 'qualify', 'dialogue', 'chemicals', 'generations', 'intent', 'producers', 'examine', 'remind', 'alongside', 'suits', 'archive', 'neutral', 'tale', 'disagree', 'awful', 'demonstrated', 'badly', 'Chamber', 'criticism', 'minimal', 'capability', 'sub', 'catalog', 'Rule', 'anime', 'bulk', 'belong', 'evaluate', 'suspension', 'Celebrity', 'surprising', 'beaches', 'mate', 'promises', 'pad', 'Mitchell', 'derived', 'blessed', 'drum', 'shapes', 'Designed', 'Angels', 'Todd', 'occasions', 'gentle', 'romance', 'achievement', 'enormous', 'converted', 'Pick', 'steam', 'environments', 'stroke', 'handed', 'mask', 'forming', 'repeated', 'reveals', 'establishment', 'Margaret', 'Warner', 'poem', 'Chuck', 'Robinson', 'Move', 'beats', 'alike', 'Delta', 'slip', 'sisters', 'drag', 'bold', 'protecting', 'liver', 'Les', 'worthy', 'Playing', 'pursue', 'Mars', 'Ah', 'Shower', 'throat', 'Drop', 'intention', 'motorcycle', '1.2', 'Cherry', 'inform', 'nasty', 'Opening', 'Neil', 'dumb', 'directors', 'Bishop', 'composed', 'imagination', 'Unlike', 'affairs', 'penis', 'trim', 'wrapped', 'breathing', 'Generation', 'Philip', 'resolve', 'counts', 'accomplish', 'boats', 'sized', 'classical', 'execution', 'Gets', 'Pete', 'mainstream', 'rolled', 'Lines', 'Unless', 'bite', 'Harbor', 'sits', 'voices', 'chamber', 'remembered', 'Emily', 'Behind', 'exhibit', 'exploring', 'stays', 'Doug', 'con', 'Chain', 'epic', 'pie', 'canvas', 'seal', 'Julie', 'bomb', 'wars', 'Samuel', 'reward', 'associate', 'fed', 'screening', 'wildlife', 'dependent', 'Danny', 'judges', 'entity', 'Bond', 'Late', 'lemon', 'anytime', 'highway', 'fitting', 'exclusively', 'ties', 'Were', 'Strong', 'lover', 'determination', 'lovers', 'affects', 'charm', 'Crazy', 'athletes', 'sucking', 'considerable', 'placing', 'Cameron', 'Ghost', 'shadow', 'developments', 'beating', 'surfaces', 'bankruptcy', 'Bobby', 'Break', 'mere', 'attempted', 'adorable', 'engaging', 'caring', 'artificial', 'artistic', 'twin', 'spectrum', 'relate', 'Cute', 'Obviously', 'steady', 'paste', 'unemployment', 'assault', 'grip', 'Piano', 'climb', 'Rain', 'prominent', 'collective', 'torrent', 'Eyes', 'pays', 'tremendous', 'regime', 'tap', 'bow', 'workshops', 'Tonight', 'generous', 'aims', 'immune', 'seeks', 'Houses', 'makers', 'differently', 'landing', 'rape', 'refuse', 'silence', 'reminded', 'Giants', 'unlimited', '1975', 'globe', 'par', 'Evans', 'Wife', 'solely', 'veteran', 'dies', 'temple', 'Beijing', 'underground', 'treating', 'Either', 'behaviour', 'sink', 'backed', 'Rings', 'Fiction', 'Apparently', 'Hudson', 'requiring', 'so-called', 'unexpected', 'cares', 'fax', 'Studios', 'gray', 'wider', 'friendship', 'pound', 'Dawn', 'progressive', 'tourism', 'effectiveness', 'titled', 'Jeremy', 'logical', 'gently', 'Morris', 'tricks', 'jury', 'nail', 'pointing', 'endless', 'cartoon', 'celebrated', 'exterior', 'Rather', 'laundry', 'lazy', 'placement', 'Ali', 'Especially', 'Stanley', 'investing', 'grain', 'Ted', 'selecting', 'investigate', 'bearing', 'Places', 'devoted', 'Imagine', 'steal', 'Beat', 'revolution', 'Fans', 'heritage', 'betting', 'processed', 'secrets', '1972', 'accommodate', 'Ways', 'admission', 'discipline', 'Christianity', 'desert', 'specialized', 'underlying', 'thereof', 'Penn', 'sometime', 'nationwide', 'wallpaper', 'deserves', 'responsibilities', 'Count', 'assuming', 'duration', 'quest', 'mild', 'speeds', 'colorful', 'passionate', 'yield', '1973', 'precise', 'closest', 'Probably', 'welfare', 'legitimate', 'experiencing', 'enjoys', 'handled', 'folk', 'stones', 'accompanied', 'syndrome', 'graduated', 'tension', 'marine', 'Belt', 'Trek', 'operational', 'Roberts', 'Fight', 'gross', 'shelf', 'bikes', 'dimensions', 'lawn', 'Alpha', 'hilarious', 'Carol', 'Oscar', 'beings', 'pour', 'shoulders', 'twelve', 'lasting', 'difficulties', 'blast', 'tender', 'trash', 'praise', 'terrorist', '117', 'encouraging', 'bytes', 'Ward', 'centuries', 'operates', 'rolls', 'mothers', 'humanity', 'tear', 'convention', 'acceptance', 'touched', 'memorable', 'Poor', 'bored', 'flesh', 'worlds', 'swim', \"'em\", 'Heather', 'twist', 'province', 'intervention', 'relaxed', 'soap', 'coal', 'elite', 'informative', 'von', 'queen', 'Pure', 'funeral', 'Jerusalem', 'wherever', 'coupled', 'rejected', 'illustrated', 'burden', 'don', 'confusion', 'Graduate', 'shake', 'Consider', 'cultures', 'charming', 'subtle', 'departments', 'potatoes', 'grocery', 'visitor', 'Funeral', 'fence', 'Been', 'hop', 'tops', 'bones', '****', 'measurements', 'premier', 'assured', 'beneath', 'wondered', 'viewers', 'daughters', 'vacuum', 'portrait', 'nowhere', 'Chelsea', 'Jake', 'ignored', 'Kill', 'Circuit', 'exceed', 'lightweight', 'commentary', 'alternatives', 'Tank', 'isolated', 'heated', 'Copy', 'classified', 'wore', 'meaningful', 'beside', 'advised', 'puppy', 'rally', 'winds', 'handles', 'Lauren', 'Chase', 'failing', 'precisely', 'striking', 'crossed', 'laboratory', 'profession', 'Batman', 'physics', 'real-time', 'tribute', 'drawings', 'timely', 'preferably', 'shorter', 'calculated', 'controlling', 'kills', '1960', 'Wait', 'retain', 'Graphic', 'magical', 'Breaking', 'satisfy', 'deny', 'Murphy', 'Seriously', 'limitations', 'notion', 'legacy', 'bonds', 'psychological', 'reaches', 'sleeve', 'shed', 'Directors', '127', 'Evil', 'antique', 'Del', 'climbing', 'beloved', 'Far', 'Criminal', 'der', 'Sets', 'drew', 'legend', 'assignment', 'corruption', 'Holland', 'Rogers', 'strings', 'kingdom', 'sacrifice', 'dig', 'proceed', 'simultaneously', 'woods', 'clinic', 'closure', 'scratch', 'lighter', 'resist', 'Attack', '1971', 'built-in', 'reactions', 'mixing', 'intensity', 'minority', 'Tommy', 'theft', 'screens', 'waited', 'grows', 'Interesting', 'aging', 'hybrid', 'assess', 'teaches', 'Wells', 'clever', 'wines', 'Comedy', 'recommendation', 'elderly', 'generating', 'fetish', 'captain', 'inspiring', 'anonymous', 'abandoned', 'squad', 'Fabric', 'margin', 'complain', 'warned', 'Rush', 'Nicole', 'stepped', 'amp', 'shine', 'lap', 'creatures', 'knees', 'Ralph', 'reduces', 'ranks', 'cleaner', 'tub', 'competing', 'reflects', '1967', 'Writer', 'executed', 'dressing', 'flood', 'Harrison', 'instances', 'sunny', 'handful', 'curve', 'animated', 'conversations', 'sends', 'moisture', 'reflection', 'survived', 'qualities', 'AIDS', 'invitation', 'radical', 'motivation', 'marketplace', 'replacing', '``', 'darkness', 'dozens', 'idiot', 'organize', 'bride', 'independence', 'commands', 'transparent', 'Cheese', 'genius', 'variations', 'Otherwise', 'hottest', 'obligation', 'cousin', 'ethnic', 'Returns', 'appreciation', 'desperate', 'secretary', 'combines', 'mounting', 'adopt', 'punch', 'sooner', 'hoped', 'defeated', 'puzzle', 'essence', 'ye', 'instructor', 'Extreme', '3000', 'slave', 'relatives', 'explaining', 'Bears', 'demanding', 'floating', 'specialty', 'courage', 'fears', 'excessive', 'breed', 'Draft', 'belly', 'chick', 'legendary', 'Mason', 'suspended', 'Swedish', 'starter', 'interact', 'economics', 'Nobody', 'threatened', 'testimony', 'clue', 'shocked', 'tattoo', 'trails', 'implications', 'portions', 'drain', 'afterwards', 'neighbor', 'vessel', 'Dirty', 'Whole', 'vulnerable', 'Feature', 'strict', 'collar', 'Lion', 'meter', 'overwhelming', 'counting', 'saves', 'Photographer', 'occasional', 'Bryan', 'Ship', 'goodness', 'consciousness', 'flip', '9/11', 'Catherine', 'Palestinian', 'reasonably', 'brass', 'strikes', 'comics', 'controversial', 'Often', 'dough', 'sellers', 'shades', 'useless', 'contrary', 'accepting', 'Twilight', 'prospect', 'routes', 'Juan', 'midnight', 'Hit', 'Adventures', 'laughed', 'experimental', 'offset', 'patience', 'asleep', 'lounge', 'tourists', 'possess', 'Julia', 'Guardian', 'barrel', 'influenced', 'Featuring', 'punishment', 'careers', 'blues', 'pill', 'fate', 'editorial', 'magnificent', 'clouds', 'convince', 'thirty', 'ensures', 'seemingly', 'freely', 'corners', 'Rocky', 'Khan', 'raises', 'protective', 'perception', 'genes', 'tastes', 'literary', 'Brooks', 'Carlos', 'executives', 'Wonderful', 'suited', 'Beta', 'pork', 'repeatedly', 'motivated', 'Martha', 'Said', 'gradually', 'soldier', 'Blonde', 'collapse', 'descriptions', 'chemistry', 'exotic', 'Fisher', 'Dreams', 'Jesse', 'Guys', 'neat', 'transformation', 'honored', 'buddy', 'treats', 'well-known', 'substitute', 'breasts', 'bubble', 'significance', 'steering', 'Election', 'skip', 'confirmation', 'sticks', 'closet', 'Psychology', 'awhile', 'readily', 'dealt', 'embedded', 'stamp', 'partially', 'Norman', 'alternate', 'tasty', 'tends', 'critics', 'divine', 'feeds', 'delighted', 'heels', 'bare', 'affair', 'bang', 'departure', 'respected', 'icons', 'batch', 'Eddie', 'shorts', 'individually', 'exploration', 'sword', 'Oliver', 'pose', 'Daddy', 'Lucky', 'workplace', 'Mad', 'excellence', 'biography', 'bears', 'keen', 'adapted', 'traditions', 'deficit', 'boundaries', 'Highlights', 'crossing', 'tooth', 'Doors', 'archives', 'Drew', 'intimate', 'Bright', 'Outside', 'variation', 'preference', 'Rebecca', 'performs', 'slots', 'narrative', 'hood', 'offerings', 'catching', 'transform', 'fool', 'pollution', 'ESPN', 'eager', 'comparing', 'mall', 'adventures', 'desires', 'compelling', 'athletic', 'racist', 'belongs', '1950', 'sheep', 'Fame', 'portal', 'beads', 'Viewing', 'vibrant', 'verse', 'practically', 'surprisingly', 'proceedings', 'surgical', 'conscious', 'pile', 'partly', 'ranges', 'reminder', 'Foster', 'Shore', 'Thing', 'lean', 'Joel', 'deciding', 'draws', 'volumes', 'suggesting', 'U.', 'couch', 'bucks', 'countless', 'efficiently', 'paths', 'Betty', 'coating', 'Swimming', 'ours', 'achieving', 'Wave', 'drill', 'turkey', 'palm', 'trains', 'psychology', 'harsh', 'lane', 'touching', 'automated', 'enabling', 'Glenn', 'jokes', 'Skip', 'lobby', 'creator', 'Needs', 'snap', 'reflected', 'observation', 'hiding', 'torture', 'mysterious', 'restored', 'tactics', 'alien', 'sporting', 'tuned', 'advances', 'cooler', 'deer', 'strictly', 'wound', 'exhaust', 'Signs', 'extends', 'prescribed', 'venues', 'elementary', 'Sara', 'attraction', 'packs', 'Tomorrow', 'Spencer', 'promising', 'anticipated', 'spouse', 'incorporate', 'bless', 'Berry', 'terrorism', 'presenting', 'garbage', 'Iraqi', 'Egyptian', 'decides', 'Discovery', 'linear', 'terror', 'novels', 'delight', 'Vincent', 'cellular', 'unnecessary', 'resolved', 'Birds', 'tanks', 'observe', 'Teens', 'appearing', 'fights', 'Victor', 'circles', 'gaining', 'deemed', 'Patient', 'fame', 'veterans', 'shore', 'Hood', 'recovered', 'Berkeley', 'sheer', 'hint', 'chef', 'Numbers', 'fleet', 'junk', 'Blake', 'affecting', 'describing', 'uncomfortable', 'Holly', 'continually', 'versatile', 'marry', 'Gone', 'Wonder', 'synthetic', 'spreading', 'embrace', 'experiments', 'confusing', 'separation', 'Everybody', 'mechanisms', 'reporters', 'delayed', 'permits', 'theories', 'phenomenon', 'kissing', 'establishing', 'Stanford', 'Ideal', 'Lovely', 'Effective', 'shy', 'Veterans', 'recruiting', 'prizes', 'ups', 'urge', 'knock', 'dramatically', 'inspire', 'renowned', 'regret', 'sounded', 'Nathan', 'insane', 'Blair', 'Comes', 'orientation', 'Ellen', 'lightly', 'Fantastic', 'symbols', 'pressed', 'Boot', 'Comic', 'addressing', 'arena', 'graduation', 'mold', 'Jet', 'flows', 'occupied', 'Watson', 'throws', 'mesh', 'lecture', 'treasure', 'columns', 'sealed', 'insights', 'terrific', 'Marvel', 'pit', 'arrives', 'toxic', 'precision', 'heal', 'updating', 'homework', 'attachment', 'companion', 'accurately', 'bitter', 'cuisine', 'strain', 'accent', 'stadium', 'Singles', 'beaten', 'Hughes', 'existed', 'dominant', 'observations', 'indication', 'journalists', 'march', 'deadly', 'Joan', 'nerve', 'cinema', 'swap', 'backs', 'loses', 'emotion', 'niche', 'thrilled', 'cheat', 'Nurse', 'briefly', 'talents', 'casting', 'journalist', 'democratic', 'fighter', 'heroes', 'concentrate', 'stereo', 'proceeds', 'males', 'Heroes', 'rap', 'McDonald', 'borders', 'Shakespeare', 'audiences', 'Secrets', 'Portugal', 'Solo', 'appealing', 'Complex', 'Films', 'relates', 'robust', 'dive', 'challenged', 'screaming', 'Basically', 'roster', 'Taken', 'analyze', 'seasonal', 'Colin', 'Thinking', 'Lucy', 'evident', 'Newton', 'bout', 'predict', 'Viewed', 'teenage', 'gods', 'surf', 'travels', 'Simpson', '170', 'ethical', 'Smoking', 'potato', 'Cinema', 'complexity', 'Greatest', 'delicate', 'examined', 'distinctive', 'camps', 'Goes', 'alter', 'immigrants', 'Amber', 'longest', 'pre', 'blown', 'surveys', 'Shanghai', 'Fear', 'Triple', 'prospective', 'approached', 'clarity', 'Cuba', 'trademark', 'babes', 'Gibson', 'emerged', 'castle', 'Singh', 'Harvey', 'thorough', 'lacking', 'eternal', 'justify', 'underneath', 'Derek', 'builds', 'proportion', 'Provides', 'strips', 'Lil', 'Belgium', 'Personally', 'blessing', 'ethics', 'Poetry', 'Stuart', 'trace', 'fortune', 'contributing', 'heel', 'organ', 'RED', 'sensitivity', 'kicks', 'rewards', 'Uncle', 'Monroe', 'stir', 'dare', 'faithful', 'CIA', 'stake', 'compromise', 'arise', 'disclosure', 'Hands', 'distant', 'reportedly', 'comparable', 'loyal', 'riders', 'Wallace', 'boobs', 'desirable', 'bloody', 'remarks', 'Character', 'Superior', 'Selection', 'Ends', 'Nine', 'labour', 'headlines', 'Formula', 'traveled', 'cried', 'approaching', 'nails', 'acute', 'Mystery', 'suspected', 'sacred', 'foster', 'brave', 'recordings', 'Sunset', 'Holmes', 'technological', 'Cedar', 'nurses', 'smiling', 'Annie', 'Elvis', 'costumes', 'unlock', 'distinction', 'obligations', 'span', 'reviewing', 'laying', 'Important', 'rope', 'compiled', 'Casey', 'Except', 'expects', 'Lucas', 'slim', 'L.A.', 'Lock', 'recycling', 'Kiss', 'enthusiasm', 'dedication', 'springs', 'soundtrack', 'O.', 'costly', 'terrorists', 'Kenneth', 'Marcus', 'Cox', 'quietly', 'Edited', 'pub', 'creature', 'ghost', '1.8', 'Testament', 'Shoe', 'demonstration', 'Christine', 'weddings', 'evolved', 'poorly', 'convicted', 'Pan', 'transit', 'encountered', 'frustrated', 'Monica', 'grandmother', 'manages', 'controversy', 'Bang', 'subsequently', 'spirits', 'shelves', 'joins', 'fires', 'starring', 'plasma', 'matched', 'trap', '1920', 'firmly', 'solved', 'dimension', 'parade', 'employ', 'Leonard', 'allegedly', 'releasing', 'architectural', 'showcase', 'Nevertheless', 'achievements', 'shield', 'welcomed', 'meditation', 'refreshing', 'tunnel', 'fried', 'PlayStation', 'disabilities', 'tones', 'Reid', 'Broken', '20,000', 'Nicholas', 'bias', 'sweat', 'burst', 'Serving', 'needing', 'wrestling', 'baked', 'Trinity', 'hats', 'wit', 'Jeffrey', 'fulfill', 'enters', 'invested', 'washed', 'wool', 'Seems', 'Neither', 'finishes', 'educate', '1980s', 'Congrats', 'linking', 'gel', 'viewer', 'utilizing', 'wasted', 'Critical', 'er', 'sewing', 'forcing', 'opera', 'premises', 'follow-up', 'Fortunately', 'scripts', 'Charleston', 'grave', 'rhythm', 'Mediterranean', 'Hole', 'maintains', 'Arnold', 'Drama', 'coastal', 'combo', '!?', 'notable', 'hated', 'backyard', 'retreat', 'considerably', 'wonders', 'occupation', 'influences', 'rival', 'frustrating', 'inquiry', 'tragedy', 'Blade', 'Claire', 'heck', 'lit', 'china', 'celebrities', 'sustained', 'pumps', 'impacts', 'threshold', 'scientist', 'swear', 'happily', 'momentum', 'Orders', 'dentist', 'necessity', 'recruitment', 'Joshua', 'displaying', 'chase', 'ignorant', 'proves', 'answering', 'understands', 'namely', 'react', 'Chevy', 'Waste', 'Denmark', 'segments', 'blowing', 'lifted', 'schemes', 'crafts', 'spine', 'Might', 'crafted', 'varying', 'arriving', 'blanket', 'racial', 'pulse', 'Hence', 'jam', 'stuffed', 'souls', 'introducing', 'lingerie', 'quarters', 'Hunting', 'Nights', 'invasion', 'worries', 'dragon', 'Gregory', 'bullet', 'tunes', 'translate', 'cigarette', 'pulls', 'battles', 'purely', 'hooked', 'complaining', 'pickup', '1962', 'Famous', 'nominated', 'surfing', 'Bella', '1970s', 'revolutionary', 'avoiding', 'fare', 'inevitable', 'flies', 'slice', 'float', 'attach', 'Changing', 'cocktail', 'surveillance', 'soda', '24/7', 'documented', 'characteristic', 'jar', 'Absolutely', 'yarn', 'Independence', 'assembled', 'spite', 'weakness', 'sandwich', 'lonely', 'cattle', 'Thought', 'organizing', 'mill', 'firing', 'integral', 'Christina', 'Cal', 'Generally', 'invented', 'roller', 'hurts', 'conspiracy', 'fifteen', 'Provide', 'co', 'hose', 'MTV', 'whatsoever', 'delightful', 'boasts', 'highlighted', 'touches', 'navigate', 'lending', 'outline', 'Angela', 'imposed', 'pond', 'sticking', 'forests', 'inappropriate', 'corrupt', 'activists', 'Rare', 'lifting', 'Diane', 'Newcastle', 'avoided', 'punk', 'breathe', 'Saving', 'mentally', 'transformed', 'modest', 'utilized', 'prevents', 'grasp', 'angels', 'Kind', 'Marina', 'launching', 'merchant', 'dominated', 'bent', 'acted', 'fifty', 'Alfred', 'tendency', 'rip', 'und', 'revision', 'machinery', 'boom', 'associations', 'genuinely', 'Queens', 'combining', 'erotic', 'flags', 'Kathy', 'deposits', 'warmth', 'midst', 'polished', 'Sally', 'Boulevard', 'backgrounds', '1960s', 'Venice', 'Stevens', 'Gaza', 'supportive', 'threatening', 'altogether', 'Iranian', 'Ignore', 'Sullivan', 'Takes', 'Tales', 'frustration', 'mobility', 'attacking', 'Bread', 'investigating', 'beast', 'cow', 'shiny', 'Tracy', 'equation', 'loyalty', 'Griffin', 'Diesel', 'Dylan', 'Slim', 'arrow', 'Stress', 'demonstrates', 'teenagers', 'anchor', 'guessing', 'substances', 'doctrine', 'Bennett', 'Lifestyle', 'Slow', 'pretend', 'mud', 'thumbs', 'Sand', 'characterized', 'masters', 'awkward', 'Q.', 'solving', 'Few', 'hobby', 'wishing', 'structured', 'Loved', 'Spy', 'hairy', 'chaos', 'surgeon', 'handsome', 'prisoners', 'consequence', 'preliminary', 'rider', 'explosion', 'radar', 'Cam', 'invisible', 'supposedly', 'broader', 'lesser', 'probe', 'recreational', 'believing', 'unfortunate', 'resorts', 'hack', 'Mama', 'jealous', 'technically', 'crisp', 'definitions', 'liable', 'showers', 'fraction', 'forgive', 'classification', 'freeze', 'conflicts', 'Zoom', 'Sharp', 'breakdown', 'construct', 'ballot', 'humble', 'Strip', 'Leon', 'Divine', 'sentences', 'mercy', 'discretion', 'Watching', 'consistency', 'bargain', 'Ellis', 'Them', 'poet', 'grounded', 'Swift', 'Jackie', 'varies', 'Eight', 'enjoyment', 'locals', 'playoff', 'Reagan', 'Fields', 'surroundings', 'Dana', 'disappear', 'Portrait', 'dynamics', 'Calling', 'extensions', 'Floyd', 'glance', 'hints', 'princess', 'struggles', 'wounded', 'Hitler', 'Presents', 'Leading', 'profound', 'flame', 'improves', 'memorial', 'foul', 'encourages', 'sadly', 'Whenever', 'rocket', 'Merry', 'therapeutic', 'defines', 'debts', 'conviction', 'Sandra', 'Bean', 'Norton', 'develops', 'Yep', 'weigh', 'bra', 'candle', 'bull', 'slides', 'witnesses', 'masses', 'Travis', 'satisfying', 'salvation', 'ingredient', 'smallest', 'switches', 'incidents', 'pale', 'BA', 'Li', 'recycled', 'Hook', 'Pie', 'Ace', 'Midnight', 'Quite', 'ignorance', 'favour', 'teenager', 'thesis', 'rendered', 'clicks', 'gates', 'sunset', 'Throughout', 'mighty', 'Warrior', 'glue', 'altered', 'pitcher', 'pine', 'fighters', 'consuming', 'cricket', 'billing', 'mum', 'Parade', 'salaries', 'advertisement', 'empire', 'founding', 'eh', 'defining', 'Walt', 'Drawing', 'cage', 'Brett', 'duo', 'divide', 'senses', 'Mile', 'recreation', 'scandal', 'duck', 'Plays', 'journalism', 'strengths', 'conclusions', 'bizarre', 'distinguished', 'ruin', 'Beck', 'complications', 'Warm', 'grandfather', 'spinning', 'dip', 'Lily', 'Depending', 'rude', 'witnessed', 'sketch', 'glorious', 'streak', 'Raymond', 'realm', 'Myers', 'Bernard', 'consume', 'gravity', 'sucked', 'hardest', 'simulation', 'cope', 'Escape', 'nowadays', 'wears', 'struggled', 'brutal', 'sequences', 'Trying', 'kicking', 'rewarding', 'sights', 'stranger', 'spy', 'fatal', 'Warriors', 'cops', 'Hart', 'architect', 'sequel', 'influential', 'Yourself', 'wives', 'barriers', 'shallow', 'honors', 'assumption', 'deserved', 'torn', 'attitudes', 'introduces', 'bend', 'praying', 'Britney', 'households', 'arguing', 'laughter', 'undergraduate', 'performers', 'Owen', 'si', 'collaborative', 'nomination', 'blah', 'perfection', 'inviting', 'virgin', 'lo', 'promotes', 'sensation', 'Hispanic', 'exceptions', 'unity', 'AAA', 'Satin', '1959', 'miracle', 'toes', 'scam', 'Matrix', 'trapped', 'scenery', 'pencil', 'considers', 'fork', 'sampling', 'Lopez', 'pig', 'scent', 'owe', 'commerce', 'crowded', 'freshman', 'appeals', 'bronze', 'athlete', 'martial', 'competent', 'echo', 'Burns', 'smoothly', 'doc', 'practitioners', 'studios', 'Apart', 'Sunshine', 'cop', 'consumed', 'Luther', 'lively', 'tales', 'Rising', 'balloon', 'trauma', 'tailored', 'clutch', 'reject', 'Brady', 'escort', 'headache', 'rat', 'guilt', 'bucket', 'Wang', 'Adrian', 'Isle', 'Strange', 'guarantees', 'in-depth', 'Ash', 'Standing', 'laptops', 'sexually', 'envelope', 'plumbing', 'cracked', 'elevated', 'wallet', 'badge', 'tolerance', 'expressions', 'dosage', 'staring', 'dismissed', 'bolt', 'Portuguese', 'commenting', 'Geneva', 'pursuing', 'nationally', 'render', 'intentions', 'Shaw', 'fathers', 'mechanics', 'imagined', 'revealing', 'disturbing', 'terrain', 'Eventually', 'Enough', 'award-winning', 'Midwest', 'composer', 'sustain', 'pipeline', 'gem', 'por', 'abundance', 'Hugh', 'Pokemon', 'boil', 'stimulus', 'Memories', 'Lite', 'Translation', 'utterly', 'Luis', 'rounded', 'nevertheless', 'lame', 'bundle', 'readings', 'dull', 'bust', 'Highly', 'scenarios', 'depressed', 'dislike', 'fairy', 'nonsense', 'angles', 'Mostly', 'racism', 'opposing', 'skinny', 'Disneyland', 'sticky', 'dash', 'Forget', 'stiff', 'checkout', 'lord', 'refined', 'rage', 'dialog', 'societies', 'sterling', 'Killer', 'disappointing', 'renewal', 'brains', 'Eugene', 'monkey', 'nightmare', 'circular', 'buzz', 'strongest', 'clause', 'credibility', 'compassion', 'shadows', 'ART', '1957', 'tragic', 'Roses', 'Wade', 'candles', 'passive', 'Fortune', 'marathon', 'Pair', '1958', 'spicy', 'bump', 'pros', 'Pierce', 'straightforward', 'publicity', 'runner', 'Smoke', 'borrow', 'rushed', 'expose', 'Waiting', 'picnic', 'politically', 'Certainly', 'participant', 'Yale', 'sour', 'crush', 'swinging', 'decorating', 'myth', 'Sadly', 'elaborate', 'Erin', 'Com', 'rendering', 'Shawn', 'severely', 'openly', 'thoughtful', 'smiles', 'upside', 'Definitely', 'framed', 'bothered', 'Hawaiian', 'rises', 'steak', 'traditionally', 'Reynolds', 'sincere', 'grief', 'Daughter', 'Hopkins', 'emerge', 'criminals', 'daddy', 'appetite', 'pissed', 'stance', 'Alzheimer', 'Tattoo', 'VI', 'Bow', 'Gorgeous', 'connects', 'twisted', '270', 'spice', 'puppies', 'disappointment', 'implies', 'trades', 'glow', 'explicit', 'Kurt', 'Vampire', 'lottery', 'anxious', 'Pepper', 'pause', 'someday', 'Done', 'dock', 'Garcia', 'doses', 'sings', 'governance', 'cheating', 'shifts', 'succeeded', 'Coral', 'emotionally', 'illustrates', 'Citizen', 'adaptation', 'admire', 'mortality', 'trailers', 'snake', 'Willie', 'Meat', 'invites', 'shifting', 'surround', 'sturdy', '1955', 'infinite', 'breach', 'undoubtedly', 'Blind', 'spark', 'Limit', 'worthwhile', 'medal', 'glimpse', 'pronounced', 'simpler', 'screwed', 'Satan', 'Renaissance', 'origins', 'clarify', 'finale', 'simplicity', 'Ya', 'alliance', 'awake', 'scream', 'urgent', 'shoots', 'caution', 'ample', 'eighth', 'Afghan', 'bounce', 'interference', 'remainder', 'accepts', 'noble', 'doubled', 'overly', 'hurry', 'assurance', 'scare', 'Molly', 'cheer', 'Hate', 'wasting', 'Nearly', 'ambitious', 'Ribbon', 'Lynch', 'sounding', 'cheats', 'chill', 'squeeze', 'Seeking', 'wicked', 'Burning', 'crushed', 'markers', 'Ram', 'desperately', 'enthusiastic', 'partnerships', 'Meyer', 'Silent', 'eliminating', 'syrup', 'urine', 'lasts', 'tricky', 'Calvin', 'speculation', 'generates', 'shaking', 'explores', 'argues', 'liking', 'Identity', 'cozy', 'negotiate', 'toss', 'boxing', 'methodology', 'advertised', 'denial', 'Python', 'misses', 'vinegar', 'Diary', 'Paula', 'scattered', 'Klein', 'centered', 'Clothes', 'writings', 'swingers', 'T-shirt', 'dangers', 'Glory', 'Danish', 'frankly', 'cracks', 'spotlight', 'lengthy', 'hammer', 'modem', 'Bride', 'Natalie', 'compliment', 'transparency', 'Seth', 'Displaying', 'convey', 'stepping', 'specials', 'intriguing', 'virtue', 'lectures', 'ray', 'inability', 'intro', 'shocking', 'spends', 'crowds', 'Yard', 'naughty', 'hands-on', 'religions', 'wipe', 'filming', 'Nor', 'hatred', 'dawn', 'underwear', 'Damage', 'stretched', 'obstacles', 'toast', 'Morrison', 'activate', 'tuning', 'propaganda', 'rookie', 'shout', 'melt', 'ladder', 'abused', 'stretching', 'commercials', 'tissues', 'Welsh', 'progression', 'flush', 'peanut', 'Weddings', 'amid', 'nonetheless', 'cared', 'poses', 'comfortably', 'Cliff', 'Finder', 'knitting', 'Catch', 'whites', 'Madonna', 'trio', 'Throw', 'incomplete', 'XXX', 'Paid', 'survivors', 'intact', 'orgasm', 'beverage', 'laughs', '1954', 'vampire', 'minus', 'dancers', 'worrying', 'advocacy', 'filmed', 'Lives', 'enthusiasts', 'sneak', 'iconic', 'Waters', 'Nazi', 'Adults', 'closes', 'dumped', 'revenge', 'lengths', 'Hardy', 'coloring', 'bore', 'inherent', 'catches', 'Murder', 'Passion', 'Hmm', 'hype', 'indie', 'carved', 'hug', 'troubled', 'Guns', 'Seasons', 'playlist', 'lodging', 'arising', 'Sisters', 'insisted', 'honesty', 'absent', 'Pleasant', 'Claims', 'similarly', 'Seeing', 'unprecedented', 'duplicate', 'dense', 'Carey', 'Greene', '1950s', 'leap', 'mileage', 'lacks', 'Swim', 'definite', 'cult', 'Generic', 'rug', 'optimistic', 'Led', 'Rodriguez', 'cruel', 'surge', 'projection', 'responsive', 'Clare', 'civilization', 'drying', 'lightning', 'aesthetic', 'Resident', 'ego', 'Rosa', 'captures', 'Palestinians', 'pathetic', 'tens', 'glucose', 'perspectives', 'devastating', 'forty', 'slavery', 'performer', 'mankind', 'energetic', 'crude', 'Authentic', 'Apollo', 'positively', 'Pipe', 'Fellowship', 'proving', 'doubles', 'tightly', 'binary', 'dignity', 'practiced', 'diary', 'Maggie', 'appropriately', 'monsters', 'prone', 'comprise', 'aliens', 'Emperor', 'accompanying', 'blacks', 'surprises', 'Norwegian', 'darker', 'Mai', 'comparisons', '1953', 'exhausted', 'isolation', 'expressing', 'debris', 'starters', 'creamy', 'popped', 'halfway', 'False', 'Prophet', 'warnings', 'blows', 'retro', 'sleek', 'fusion', 'foil', 'merit', 'Proof', 'Allison', 'Burke', 'contests', 'constraints', 'adore', 'nursery', 'questioning', 'convincing', 'liner', 'juicy', 'obsessed', 'toddler', 'Totally', 'creepy', 'divisions', 'cloudy', 'traits', 'stud', 'foremost', 'singers', 'communicating', 'enhancing', 'Mean', 'celebrates', 'dysfunction', 'Bicycle', 'calculations', 'curves', 'shook', 'spill', 'persistent', 'entertain', 'chords', 'inhabitants', 'exquisite', 'Spears', 'gut', 'Dot', 'fond', 'gum', '1952', 'warrior', 'curiosity', 'continent', 'rats', 'irrelevant', 'distinguish', 'Aside', 'freight', 'Romantic', 'burns', 'buck', 'strangers', 'ruined', 'heights', 'Marilyn', 'unhappy', 'ignoring', 'stirring', '401', 'resolutions', 'portraits', 'lungs', 'jackets', 'Considering', 'jumps', 'timer', 'Elder', 'bombs', 'beers', 'Rouge', 'Fancy', 'arc', 'doubts', 'Powers', 'sweep', 'shortage', 'freak', 'abundant', 'scenic', \"'n\", 'sticker', 'extensively', 'marking', 'Gang', 'Petersburg', 'sentiment', 'Latino', 'implied', 'playground', 'absorb', 'Rug', 'lend', 'rode', 'tract', 'lowered', 'gadgets', 'verbal', 'absorbed', 'wounds', 'Freeman', 'bio', 'Mask', 'inquiries', 'acclaimed', 'overtime', 'kings', 'reflecting', 'jungle', 'escaped', 'uncertain', 'Sellers', 'explored', 'favors', 'Alien', 'Drives', 'addicted', 'Brussels', 'notably', 'premise', 'Venus', 'creations', 'undertaken', 'Wins', 'chess', 'discovering', 'gaps', 'imagery', 'encyclopedia', 'warn', 'Vertical', 'Meaning', 'headaches', 'Worlds', 'Quinn', 'Clara', 'Producer', 'elegance', 'Husband', 'suspects', 'Chronicles', 'interpret', 'indigenous', 'Alone', 'velocity', 'albeit', 'Lifetime', 'scrap', 'Katherine', 'yields', 'Mel', 'realizing', 'underway', 'fade', 'Beast', 'classics', 'Monte', 'Cuban', 'cardiac', 'remotely', 'terribly', 'seated', 'Chen', 'recovering', 'packaged', 'visually', 'bumper', 'explosive', 'feminine', 'reign', 'idiots', 'melted', 'inspirational', 'shining', 'compositions', 'impress', 'overwhelmed', 'Worst', 'hurting', 'Yo', 'Treasure', 'commodity', '1949', 'willingness', 'lifts', 'Seen', 'recognizes', 'infants', 'chorus', 'bowling', 'ritual', 'sympathy', 'themed', 'Ana', 'dolls', 'Daisy', 'Armed', 'gourmet', 'Visits', 'turf', 'evenings', 'cartoons', 'fog', 'Guaranteed', 'Weeks', 'productions', 'Harbour', 'deliberately', 'chasing', 'underwater', 'humidity', 'Courtney', 'behave', 'Uses', 'Cry', 'mixer', 'literacy', 'unclear', 'retrieve', 'expectation', 'excel', 'loops', 'messy', 'refuses', 'siblings', 'Zen', 'stimulate', 'capturing', 'assign', 'Pull', 'hates', 'Normally', 'sufficiently', 'historically', 'Brent', 'wonderfully', 'Shirley', 'Mat', 'Hawk', 'freaking', 'woven', 'rigid', 'spoon', 'grandparents', 'clerk', 'grains', 'Exactly', 'adjusting', 'ministers', 'breathtaking', 'vivid', 'eats', 'reunion', 'Feeling', 'economical', 'misleading', 'curtain', 'distances', 'dominate', 'unbelievable', 'rivals', 'Kane', 'Wise', 'ounce', 'relevance', 'personalities', 'Devils', 'fireworks', 'cows', 'overlooked', 'constitutes', 'lyric', 'Trouble', 'spider', 'Bollywood', 'crystals', 'Bowling', 'subscriber', 'charitable', 'feast', 'circa', 'Contains', 'credited', 'pat', 'theaters', 'slope', 'pleasing', 'elephant', 'examines', 'synthesis', 'accents', 'gifted', 'Characters', 'lush', 'detailing', 'embarrassing', 'flaws', 'Certain', 'tattoos', 'refresh', 'photographic', 'revelation', 'wholly', 'emailed', 'shifted', 'Rover', 'directing', 'continental', 'mistaken', 'uh', 'zombie', 'surviving', 'Teddy', 'sophomore', 'Elliott', 'civic', '70s', 'margins', 'Saved', 'kindness', 'outs', 'hormones', 'android', 'jaw', 'merge', 'miserable', 'batting', 'Martinez', 'Pieces', 'prince', 'examining', 'Vanessa', 'Ivy', 'dreaming', 'swept', 'Sense', 'rainbow', 'conscience', 'fills', 'collision', 'Spike', 'Bacon', 'notch', 'accompany', 'immense', 'puzzles', 'Strap', 'subjected', 'weighs', 'amazingly', 'dots', 'allergy', 'ashamed', 'ninth', 'fuels', 'hopeful', 'compressed', 'polite', 'Willis', 'farther', 'lick', '146', 'Tara', 'bombing', 'manipulation', 'unsure', 'splash', 'flashing', 'hollow', 'DJs', 'borrowed', 'Samantha', 'stitch', 'warfare', 'skies', 'swallow', 'quantum', 'high-end', 'Actress', 'all-time', 'WWII', 'sexuality', 'Mormon', 'thereafter', 'evolve', 'Matters', 'pains', 'pledge', 'waking', 'amusing', 'robots', 'Combine', 'crappy', 'illusion', 'spells', 'ensemble', 'imply', 'Watts', 'widget', 'overweight', 'Walsh', 'Streets', 'Malcolm', 'Typical', 'stockings', 'gratitude', 'discomfort', 'ruins', 'Initial', 'smarter', 'seasoned', 'curse', 'posing', 'evenly', 'Brilliant', 'distributors', 'comeback', 'Prepare', 'nerves', 'groove', 'confirms', 'Turns', 'ham', 'Disaster', 'secular', 'tempted', 'critic', 'chooses', 'exceptionally', 'Merchant', 'endorsement', 'installment', 'Tucker', 'distress', 'messenger', 'Ivory', 'exchanges', 'vague', 'Went', 'landmark', 'Sad', 'Wal-Mart', 'selfish', 'Soldier', 'definitive', 'R&D', 'Nixon', 'yourselves', 'Wesley', 'undergo', 'renting', 'Savage', 'rejection', 'minded', 'invaluable', 'philosophical', 'evolving', 'Gerald', 'exploit', 'coloured', 'peek', 'lowering', 'servants', 'capitalism', 'Nash', 'pupils', 'Pedro', 'Burger', 'clone', 'Canadians', 'Resistance', 'surrender', 'repeating', 'explanations', 'Lo', 'Serious', 'destiny', 'Kathleen', 'triangle', 'demographic', 'Fork', 'extras', 'balancing', 'anticipation', 'sausage', 'sweeping', 'thrust', 'Polished', 'assumes', 'bypass', 'Tale', 'welcomes', 'socially', 'innovations', 'boiling', 'pinch', 'Farmer', 'funky', '1938', '1937', 'surplus', 'Runner', 'noticeable', '7.00', 'Reno', 'Phantom', 'supermarket', 'Needed', 'multitude', 'ideally', 'encounters', 'ne', 'bisexual', 'prom', 'learnt', 'Ago', 'learns', 'forgiveness', 'predictable', 'critically', 'aiming', 'tossed', 'Snake', 'norm', 'freshly', 'Goldman', 'Witch', 'Leaves', 'Holocaust', 'insult', 'Phillip', 'refugees', 'semi', 'dunno', 'Coal', 'futures', 'landscapes', 'maturity', 'vent', 'stimulation', 'risky', 'theology', 'reel', 'buddies', 'hostile', 'hooks', 'Weber', 'pitched', 'Bo', 'steadily', 'deficiency', 'obscure', 'violated', 'blended', 'incorporates', 'saddle', 'Weird', 'impressions', 'exceeds', 'detached', 'goodies', 'endure', 'disgusting', 'retaining', 'bubbles', 'abandon', 'flames', 'witch', 'Bros.', 'elect', 'coral', 'Bentley', 'cardboard', 'absurd', 'Bon', 'northwest', 'piss', 'Catholics', 'prey', 'destructive', 'accomplishments', 'gears', 'Debate', 'uniquely', 'equals', 'canon', 'obsession', 'arbitrary', 'Saddam', 'critique', 'remembering', 'analytical', 'Dolls', 'professors', 'unavailable', 'unaware', 'fragile', 'biblical', 'inevitably', 'masterpiece', 'suffers', 'Gives', 'genres', 'Marks', 'variant', 'Vietnamese', 'Bubble', 'inadequate', 'aerial', 'Carmen', 'magnet', 'Roland', 'transported', 'rhetoric', 'whip', 'cheek', 'Proposal', 'classy', 'descent', 'spreads', 'burnt', 'Lovers', 'Cage', 'verdict', 'timeless', 'plea', 'questionable', 'madness', 'numbered', 'passages', 'utter', 'ho', 'respects', 'needles', 'subjective', '1933', 'relies', 'shaping', 'prisoner', 'Oriental', 'rails', 'denying', 'leaning', 'embarrassed', 'Fabulous', 'fierce', 'Putting', 'Quiet', 'reluctant', 'Austrian', 'Fraser', 'recognise', 'weaknesses', 'Hmmm', 'rainy', 'undo', 'neglected', 'sincerely', 'bees', 'Easily', 'relocation', 'sums', 'Ivan', 'hereby', \"o'clock\", 'biased', 'bind', 'longtime', 'Oprah', 'arguably', 'cushion', 'foolish', 'PBS', 'intentionally', 'Copenhagen', 'thriller', 'governmental', 'pity', 'collectively', 'morality', 'appraisal', 'reinforced', 'burger', 'sung', 'mixes', 'Carpenter', 'cleansing', 'potent', 'rash', 'rewarded', 'Bravo', 'transcript', 'contributes', 'gems', 'Laurel', 'Towers', 'skating', 'routinely', 'strokes', 'wizard', 'credible', 'demonstrating', 'stripped', 'recalls', 'backdrop', 'indoors', 'originated', 'Sitting', 'Stupid', 'ideology', 'Animated', 'affection', 'Heidi', 'Um', 'Thin', 'Helps', 'routines', 'popping', 'confined', 'hypothesis', 'reside', 'geared', 'prolonged', 'Happiness', 'detention', 'humanitarian', 'vagina', 'threaten', 'wrapping', 'curtains', 'dances', 'Expect', 'scarf', 'Vs.', 'phenomena', 'Digest', 'moist', 'provincial', 'spiral', 'bells', 'ballet', 'rant', 'Sanders', 'projector', 'nap', 'Michele', 'ripe', 'canned', 'reconstruction', 'fictional', 'sickness', 'Circus', 'tier', 'sparkling', 'offended', 'Triangle', '1934', 'adequately', 'Angelina', 'Plot', 'Davies', 'Freddie', 'vein', 'Scores', 'vibe', 'Avon', 'vulnerability', 'relying', 'bites', 'clan', 'seamless', 'mindset', 'pops', 'Dash', 'noisy', 'Elegant', 'economically', 'bullets', 'geographical', 'converts', 'wraps', 'alcoholic', 'reflective', 'simplest', 'Faces', 'Cabin', 'Belgian', 'coordinated', 'gossip', 'finely', '60s', 'employs', \"'n'\", 'juvenile', 'relieved', 'remake', 'unrelated', 'remembers', 'suburban', 'domination', 'accessibility', 'cons', 'R&B', '2,500', 'annoyed', 'Pa.', 'fulfilling', 'Spider-Man', 'atmospheric', 'champagne', 'traces', 'humour', 'textbook', 'day-to-day', 'Spider', 'Yang', 'unified', 'breakthrough', 'forbidden', 'Boyd', 'Winds', 'guiding', 'justification', 'darn', 'depicted', 'weighed', 'secretly', 'Pumpkin', 'arch', 'handheld', 'entertained', 'tackles', 'grind', 'pushes', 'symbolic', 'Pamela', 'Vera', 'Cannes', 'Castro', 'Allied', 'Hoffman', 'mentality', 'traveler', 'irony', 'Reunion', 'realization', 'Leigh', 'Croatia', 'reductions', 'loser', 'praised', 'refusing', 'suspicion', 'disposable', 'unforgettable', 'thrill', 'testimonials', 'mug', 'Mann', 'melting', 'Variety', 'detective', 'storyline', 'Anybody', 'cracking', 'Blessed', 'Soldiers', 'reviewers', 'counterparts', 'set-up', 'believer', 'grinding', 'outright', 'dependence', 'winding', 'reproduce', 'soothing', 'robbery', 'mob', 'dragged', 'comfy', 'stare', 'immigrant', 'Dover', 'downward', 'flawed', 'dreamed', 'rigorous', 'preserving', 'Bonds', 'mommy', 'outdated', 'Yankee', 'legends', 'configurations', 'pointless', 'discourse', 'parental', 'Directed', 'vacant', 'Hunger', 'fashioned', 'Lin', 'clash', 'apology', 'palette', 'Cube', 'Praise', 'conception', 'fluids', 'ecological', 'treasures', 'territories', 'depths', 'rocky', 'enhances', 'Michel', 'Romeo', 'accumulated', 'sadness', 'stakes', 'successor', 'bark', 'leaks', 'hip-hop', 'undergoing', 'psychic', 'slap', 'inferior', 'clipped', 'flick', 'artifacts', 'ironic', 'dilemma', 'Patriot', 'mornings', '163', 'Reef', 'stretches', 'Chan', 'Byron', 'cheeks', 'seldom', 'representations', 'Lonely', 'crosses', 'Bronx', 'widow', 'Intelligent', 'plots', 'Mitch', 'playful', 'photographed', 'Acting', 'Damon', 'Starts', 'greed', 'conceived', 'portrayed', 'intends', 'Cannon', 'deposited', 'primitive', 'gesture', 'figuring', 'stellar', 'paints', 'manipulate', 'satisfactory', 'meanwhile', 'merits', 'concludes', 'Tap', 'problematic', 'wreck', 'maid', 'IQ', 'ace', 'triumph', 'bonding', 'African-American', 'haul', 'Blow', 'nod', 'flavour', 'myths', 'Bucks', 'stacked', 'successes', 'Marvin', 'exemplary', 'catalyst', 'creators', 'endurance', 'benchmark', 'undertaking', 'attracting', 'grease', 'conventions', 'Stern', 'Truly', 'despair', 'compensate', 'symptom', 'trek', 'Thankfully', 'utmost', 'padded', 'rooted', 'chunk', 'demons', 'Ultimately', 'cube', 'advancing', 'clearer', 'fertility', 'grabs', 'Woody', 'mods', 'brushes', 'addictive', 'relied', 'scrutiny', 'Dangerous', 'distanced', 'Barrett', 'spit', 'splendid', 'Monsters', 'tense', 'dismiss', 'crawl', '2/3', 'Spend', 'stimulating', 'lifelong', 'atop', 'avid', 'tougher', 'Pandora', 'unpleasant', 'gaze', 'majors', 'remarkably', 'Na', 'world-class', 'Feels', 'understandable', 'downs', 'jelly', 'beg', 'registering', 'worms', 'outing', 'veins', 'ceremonies', '1930s', 'milestone', 'Revenge', 'exposing', 'Humor', 'exceeding', 'tolerate', 'Broadcast', 'continuity', 'awe', 'embraced', 'Offensive', 'Gambling', 'Schmidt', 'backward', 'Einstein', 'expands', 'guessed', 'Sixth', 'staple', 'nephew', 'Judith', 'Hal', 'compelled', 'attracts', 'theatrical', 'accomplishment', 'myriad', 'darling', 'outrageous', 'Lightweight', 'Crane', 'comparative', 'zombies', 'Chick', 'phenomenal', 'endangered', 'Melanie', 'transplant', 'victories', 'orbit', 'skirts', 'preaching', 'distracted', 'barbecue', 'blends', 'Somehow', 'addict', 'worm', 'Gothic', 'framing', 'Spare', 'unacceptable', 'devotion', 'preceded', 'Payne', 'Needless', 'infectious', 'Literary', 'savvy', '3-D', 'slowed', 'Trials', 'rubbish', 'pony', 'worthless', 'slick', 'Irvine', 'geek', 'Jacques', 'porno', 'owed', 'daytime', 'electoral', 'typed', 'whale', 'unstable', 'Rams', 'Silence', 'weaker', 'Dawson', 'blond', 'gays', 'envy', 'Mick', 'Ending', 'equations', 'chord', 'begging', 'programmer', 'reps', 'hierarchy', 'metropolitan', 'faint', 'yelling', 'Hussein', 'similarities', 'inconsistent', 'motorcycles', 'Opens', 'favored', 'humorous', 'rebel', 'insightful', 'withstand', 'Gauge', 'disco', 'privileged', 'dug', 'supernatural', 'prejudice', 'insists', 'drawers', 'adhere', 'pee', '1915', 'Myrtle', 'Battlefield', 'popcorn', 'belonged', 'crashing', 'balances', 'downside', 'destined', 'hash', 'Killing', 'endeavor', 'interrupted', 'peas', 'prevalent', 'festive', 'cutter', 'mars', 'coincidence', 'Twenty', 'fascinated', 'fantasies', 'warriors', 'sack', 'integrating', 'mock', 'comprehend', 'truths', 'Shadows', 'redemption', 'enacted', 'decay', 'holistic', 'Equal', 'bumps', 'grin', 'possessed', 'Dragons', 'intricate', 'masturbation', 'sharply', 'feathers', 'homosexual', 'proposes', 'dubbed', 'Thou', 'tick', 'regain', 'damned', 'Liu', 'realizes', 'pearls', 'plugged', 'deepest', 'Wong', 'utilizes', 'realities', 'apt', 'behold', 'schematic', 'Millions', 'resides', 'yacht', 'eternity', 'professionalism', 'lust', 'proportions', 'sketches', 'vain', 'simplify', 'mounts', 'chefs', 'Avengers', 'dodge', 'SAT', 'forgetting', 'Lang', 'notorious', 'digits', 'accidental', 'Papa', 'strains', 'amusement', 'ou', 'Aussie', 'Doyle', 'bathing', 'Stacy', 'purity', 'U.N.', 'Rafael', 'Wonderland', 'confronted', 'continuation', 'Augustine', 'tuna', 'HIV/AIDS', 'Knoxville', 'clocks', 'battling', 'aftermath', 'large-scale', 'circus', 'vanity', 'pursued', 'Pasadena', 'Nolan', 'cone', 'vicious', 'rightly', 'diapers', 'Ernest', 'populated', 'Somewhere', 'capsule', 'unconscious', 'refusal', 'swings', 'Twist', 'drained', 'Ensemble', 'insider', 'Oops', 'pedestrian', 'uncover', 'traps', 'thrilling', 'confront', 'restoring', 'entirety', 'panoramic', 'prop', 'gloss', 'Ops', 'X.', 'Randall', 'ambition', 'sensual', 'Coke', 'deliberate', 'Ethan', 'temptation', 'knot', 'PB', 'charms', 'Taxi', 'seas', 'unfamiliar', 'shouting', 'mainland', 'stationary', 'resurrection', 'disturbed', 'X-Men', 'Compared', 'standardized', 'pastry', 'exploitation', 'Audrey', 'fundamentals', 'adventurous', 'wrinkles', 'tactic', 'Morton', 'harmless', 'arrogant', 'motivate', 'fairness', 'misery', 'monkeys', 'cyber', 'faded', 'Enemy', 'paradigm', 'ringing', 'succession', 'Conrad', 'Zelda', 'anatomy', 'Moody', 'baggage', 'scratches', 'singular', 'Moves', 'Zoe', 'bounds', 'expiration', 'respectful', 'bodily', 'giants', 'herd', 'brushed', 'sorrow', 'wildly', 'probation', 'Lantern', 'explode', 'metaphor', 'collectible', 'odor', 'Afternoon', 'Chancellor', 'mediocre', 'Orchard', 'Olympus', 'spelled', 'layered', 'craving', 'Abbott', 'susceptible', 'ideals', 'boiler', 'cries', 'marvelous', 'highs', 'Everett', 'spontaneous', 'preschool', 'emphasized', 'Diaz', 'Traveler', 'Weaver', 'derivative', 'rests', 'naval', 'forefront', 'steals', 'Shame', 'lays', 'fuzzy', 'novice', 'haunted', 'nausea', 'bouquet', 'literal', 'speedy', 'Functions', 'porcelain', 'Lone', 'Piper', 'same-sex', 'nominal', 'visions', 'obstacle', 'translates', 'Roberto', 'ver', 'Brendan', 'expressly', 'righteousness', 'hem', 'calculating', 'Caesar', 'youngsters', 'conceptual', 'enduring', 'Nicolas', 'wander', 'prevalence', 'Ventura', 'disposition', 'tad', 'Static', 'Cuts', 'depressing', 'Esther', 'Synthetic', 'lump', 'threatens', 'Tune', 'hugely', 'confuse', 'Walls', 'energies', 'tucked', 'Panic', 'authenticity', 'mysteries', 'possesses', 'compromised', 'dire', 'stunned', 'faults', 'greedy', '295', 'Judaism', 'high-tech', 'glossy', 'occupational', 'reminiscent', 'spirituality', 'fundamentally', 'clown', 'resemble', 'folds', 'wisely', 'Claus', 'staged', 'chop', 'Pray', 'edits', 'Charming', 'recycle', 'Val', 'Claude', 'perceptions', 'monument', 'meaningless', 'snapshot', 'Initially', 'bowel', 'percussion', 'idol', 'Norris', 'repetitive', 'daunting', 'hears', 'combustion', 'primer', 'Killed', 'rustic', 'Matches', 'shines', 'Sharks', 'scar', 'blender', 'topical', 'gigantic', 'predominantly', 'Mood', 'feminist', 'oversized', 'shores', 'Das', 'Barney', 'aisle', 'kitten', 'testosterone', 'Lieutenant', 'Fathers', 'volatile', 'obsolete', 'crank', 'Napoleon', 'Conan', 'noises', 'resembles', 'tempting', 'permitting', 'smash', 'naive', 'reminding', 'Efficient', 'Naomi', 'enforced', 'benefited', 'guru', 'founders', 'Goose', 'horns', 'accelerated', 'limbs', 'Playboy', 'cheerful', 'urges', 'redundant', 'ghosts', 'sympathetic', 'Picnic', 'Jolie', 'preach', 'vastly', 'bald', 'shave', 'tenth', 'peculiar', 'intentional', 'indulge', 'transitions', 'aspiring', 'conditioned', 'cue', 'Chips', 'implements', 'compassionate', 'frightening', 'predecessor', 'unchanged', 'discoveries', 'Audience', 'retains', 'faulty', 'sucker', 'Begins', 'eventual', 'Zhang', 'tailor', 'villain', 'Conduct', 'murders', 'fringe', 'superstar', 'Runs', 'storytelling', 'Rookie', 'Dustin', 'brew', 'Han', 'trimmed', 'homosexuality', 'loudly', 'generosity', 'thinner', 'Friedman', 'Tyson', 'petite', 'shouted', 'Clint', 'ins', 'upscale', 'animations', 'imaginary', 'miracles', 'Wes', 'windshield', 'Bound', 'Critics', 'pertinent', 'reef', 'Stein', 'competence', 'digit', 'ropes', 'broadly', 'tease', 'motives', 'Assassin', 'Sit', 'astonishing', 'emergence', 'pristine', 'inmates', 'Bloody', 'viewpoint', 'lacked', 'flawless', 'sweetness', 'inflammatory', 'Funk', 'columnist', 'priceless', 'louder', 'highways', 'Randolph', 'pleasures', 'shakes', 'Beware', 'retiring', 'chew', 'unpredictable', 'LITTLE', 'banquet', 'quirky', 'downhill', 'Hogan', 'Sparks', 'Possibly', 'trilogy', 'digest', 'screams', 'Juliet', 'enthusiast', 'flashes', 'Russians', 'overlook', 'stocked', 'innocence', 'marginal', 'specifics', 'admired', 'forged', 'Choices', 'prose', 'Whereas', 'injected', 'Bates', 'cabins', 'weave', 'dim', 'tearing', 'fools', 'stripe', 'erase', 'guts', 'fluffy', 'topping', 'knots', 'induce', 'eagerly', 'Onion', 'sermon', 'caliber', 'interpretations', 'toughest', 'aggressively', 'assert', 'greatness', 'tapping', 'dependable', 'monopoly', 'Schneider', 'sacrifices', 'motions', 'youthful', 'unsuccessful', 'respectable', 'saga', 'staging', 'optimism', 'slips', 'gracious', 'reliance', 'historians', 'surveyed', 'Simpsons', 'recommending', 'old-fashioned', 'filmmaker', 'Frankie', 'Evelyn', 'repairing', 'reflections', 'demise', 'Reese', 'intrigued', 'glamour', 'splitting', 'catchy', 'tummy', 'crossover', 'observer', 'resonance', 'scooter', \"O'Neill\", 'doomed', 'Harlem', 'filler', 'distraction', 'palate', 'blending', 'famed', 'rewritten', 'inspector', 'Swingers', 'maternal', 'starred', 'Rudy', 'Finds', 'comedian', 'Iris', 'Stands', 'daring', 'Barker', 'settlers', 'Splash', 'depart', 'flaw', 'Def', 'novelty', 'Mariah', 'sinks', 'fading', 'richer', 'sci-fi', 'butterflies', 'Yu', 'Completely', 'Exploring', 'flop', 'Levy', 'Nightmare', 'genocide', 'Kathryn', 'soaked', 'accountant', 'poised', '50s', 'labeling', 'goose', 'mortal', 'Immediately', 'testament', 'cooker', 'spoil', 'disadvantage', 'piercing', 'grips', 'veil', 'neatly', 'grad', 'Busy', 'outward', 'formulas', 'boast', 'testify', 'twists', 'witty', 'whistle', 'plague', 'Darling', 'tended', '?!?', 'flute', 'Scooter', 'slightest', 'friendships', 'mythology', 'ripping', 'Xmas', 'akin', 'fuse', 'bouncing', 'paved', 'parody', 'surrounds', 'pornography', 'inherently', 'impulse', 'Wrote', 'biting', 'flair', 'showcases', 'dwell', 'reconciliation', 'traced', 'furious', 'checklist', 'battlefield', 'funniest', 'razor', 'puppet', 'investigator', 'Extremely', 'seams', 'completes', 'ventures', 'Israelis', 'bastard', 'crafting', 'gamble', 'bliss', 'snaps', 'aspirations', 'interiors', 'conquer', 'outrage', 'tortured', 'boiled', 'trivial', 'omitted', 'funk', 'adolescents', 'piles', 'pleasantly', 'losers', 'carpets', 'pulp', 'Diaries', 'dimensional', 'skipped', 'amidst', 'limb', 'contempt', 'mugs', 'Finn', 'inspires', 'longevity', 'Watched', 'dope', 'Hanson', 'girlfriends', 'suspend', 'pairing', 'stupidity', 'injustice', 'cassette', 'heroic', 'manners', 'Nasty', 'Bart', 'noticing', 'Fuller', 'voyage', 'gowns', 'liar', 'Thumbs', 'spun', 'self-esteem', 'padding', 'lipstick', 'instinct', 'unexpectedly', 'Jar', 'stubborn', 'Fence', 'paycheck', 'simmer', 'unparalleled', 'beginnings', 'ridiculously', 'depicts', 'confession', 'cumulative', 'sparked', 'devote', 'loosely', 'plunge', 'slash', 'Odd', 'Groove', 'Hicks', 'Cunningham', 'coping', 'scams', 'contend', 'hairs', 'rehearsal', 'immensely', 'convictions', 'postcard', 'horrific', 'filmmakers', 'depicting', 'fuss', 'mistress', 'forwards', 'disappoint', 'Beginners', 'terrified', 'wary', 'eclipse', 'rogue', 'Donovan', 'cooks', 'separates', 'imaginative', 'banging', 'poetic', 'ineffective', 'malls', 'characterization', 'Franco', 'marching', 'optic', 'stray', 'declares', 'crushing', 'hotter', 'dime', 'earns', 'schooling', 'graceful', 'Paramount', 'stuffing', 'bankrupt', 'paced', 'unusually', 'trumpet', 'imprisonment', 'sober', 'compass', 'coherent', 'terrifying', 'Triumph', 'Stitch', 'suppression', 'messing', 'thankfully', 'stunt', 'frank', 'tedious', 'Boris', 'embarrassment', 'collage', 'Lips', 'Blossom', 'cheering', 'robbed', 'pun', 'Gee', 'waits', 'Mafia', 'scratching', 'Theology', 'comforting', 'whining', 'wig', 'Giles', 'specimen', 'shocks', 'heap', 'riot', 'tighter', 'Kudos', 'casualties', 'killers', 'hail', 'assassination', 'styled', 'Griffith', 'temporal', 'bleed', 'smoother', 'Denis', 'carnival', 'Rosemary', 'Sofia', 'illumination', 'effortlessly', 'Anton', 'passions', 'volcano', 'Mandy', 'Fleming', 'unfinished', 'Pelosi', 'Attraction', 'disastrous', 'abuses', 'punches', 'Riviera', 'Elm', 'Sight', 'unpaid', 'Ones', 'telescope', 'refugee', 'drunken', 'displacement', 'decisive', 'articulate', 'Clyde', 'inactive', 'mist', 'bathtub', 'blur', 'tremendously', 'Kendall', 'orgy', 'confirming', 'real-life', 'Holden', 'Bermuda', 'bishops', 'Whale', 'comprehension', 'Brave', 'tensions', 'Sticky', 'communal', 'two-thirds', 'fencing', 'Olympia', 'ideological', 'rebellion', 'hides', 'progressed', 'Filled', 'directs', 'intimacy', 'Importance', 'fab', 'petty', 'illustrating', 'rabbits', '1899', 'resumes', 'attentive', 'outta', 'cheesy', 'spans', 'bully', 'vampires', 'urgency', 'goats', 'tidy', 'expresses', 'encompasses', 'chewing', 'dictate', 'notions', 'irritating', 'Woven', 'mosque', 'slippery', 'embracing', 'heartbeat', 'cafes', 'squeezed', 'restraint', 'Flynn', 'adolescent', 'visuals', 'illuminated', 'backbone', 'dues', 'itching', 'sweetheart', 'memoir', 'Gallagher', 'best-selling', 'Crimes', 'presses', 'Sonny', 'counterpart', 'flee', 'portray', 'insanity', 'Hilary', 'regimen', 'rationale', 'superhero', 'Becker', 'Cary', 'Doris', 'gosh', 'glamorous', 'Fiji', 'trusts', 'cannon', 'sweating', 'morally', 'pianist', 'tapped', 'Haunted', 'Accuracy', 'oily', 'realism', 'indicative', 'kinky', 'Demi', 'breadth', 'Dante', 'dragons', 'sloppy', 'aquatic', 'undermine', 'yell', 'seizures', 'majestic', 'insulting', 'moron', 'Maid', 'establishes', 'conflicting', 'downright', 'sleepy', 'pop-up', 'Allows', 'subtitles', 'lookin', 'striving', 'tar', 'zoning', 'admitting', 'rivalry', 'Panther', 'stark', 'Sheridan', 'teasing', 'stacks', 'hectic', 'asylum', 'eyebrows', 'disregard', 'Goldberg', 'Yorker', 'Horton', 'deserving', 'longing', 'groundbreaking', 'homage', 'drastic', 'chilled', 'full-length', 'climax', 'Viva', 'Sushi', 'craftsmanship', 'Diva', 'fingering', 'lows', 'pancakes', 'incapable', 'Armenian', 'moderately', 'CGI', 'ashes', 'discreet', 'richest', 'concentrating', 'hitch', 'devastated', 'preferable', 'discarded', 'hating', 'crackers', 'maiden', 'emphasizes', 'suspense', 'Kraft', 'catastrophic', 'Ryder', 'aided', 'Recall', 'philosopher', 'cape', 'massacre', 'Aliens', 'spoiler', 'deleting', 'docs', 'contender', 'skeleton', 'debating', 'promoter', 'choke', 'Speaks', 'Macy', 'brethren', 'turmoil', 'confrontation', 'Sailor', 'rhythms', 'prolific', 'Drops', 'Stevenson', 'Weiss', 'radioactive', 'sane', 'cheated', 'reap', 'rendition', 'Forrest', 'sway', 'cruelty', 'flux', 'temper', 'Lucia', 'bows', 'uneven', 'tranquil', 'Noon', 'user-friendly', 'glued', '1940s', 'salvage', 'Janice', 'strives', 'Georgian', 'attendant', 'leftover', 'blunt', 'elusive', 'staggering', 'Brit', 'trusting', 'Everywhere', 'strands', 'Hardwood', 'verge', 'pies', 'drone', 'paranoid', 'insomnia', 'spiders', 'Dolly', 'endured', 'topless', 'beware', 'alterations', 'proficient', 'accumulate', 'initiation', 'Corpus', 'reckless', 'Impossible', 'Musicians', 'precedent', 'absorbing', 'fi', 'Minority', 'Superb', 'PA.', 'derive', 'stereotypes', 'dazzling', 'Scratch', 'astounding', 'haunting', 'Ordinary', 'Oscars', 'blush', 'gestures', 'tweak', 'acknowledges', 'chaotic', 'liberation', 'Partly', 'strategically', 'acquainted', 'misuse', 'coke', 'starving', 'frontier', 'declines', 'harvesting', 'stranded', '007', 'plausible', 'spouses', 'prevail', 'cynical', 'ambitions', 'Domino', 'formidable', 'persona', 'overload', 'integrates', 'coarse', 'predators', 'pinned', 'plethora', 'cutting-edge', 'mystical', 'Gil', 'coaster', 'disappears', 'polishing', 'shortest', 'capped', 'twentieth', 'captive', 'gravy', 'operative', 'Xerox', 'Solaris', 'weary', 'Newfoundland', 'weaving', 'soaring', 'spanning', 'fertile', 'strangely', 'fooled', 'sip', 'vitality', 'irrational', 'entrepreneurial', 'radiant', 'Wedge', 'Clause', 'perpetual', 'Brings', 'liberties', 'chilly', 'Trains', 'violates', 'techno', 'spins', 'Axis', 'dusty', 'PHONE', 'cerebral', 'stalls', 'Trey', 'Deadly', 'Tango', 'Springer', 'Eleven', 'Franz', 'Rae', 'Atom', 'fragment', 'scrapbook', 'merry', 'spaghetti', 'upbeat', 'admiration', 'hardship', 'Ghosts', 'dorm', 'diluted', 'Holds', 'disguise', 'bland', 'emerges', 'effortless', 'universally', 'Rumor', 'leash', 'transporting', 'intern', 'Stuffed', 'mantra', 'Lazy', 'misunderstood', 'Grave', 'crawling', 'cylinders', 'contrasting', 'deferred', 'Frankly', 'quaint', 'intellect', 'punched', 'spiritually', 'debated', 'widescreen', 'forgiven', 'irresponsible', 'weakened', 'needy', 'indictment', 'repetition', 'tart', 'Abdul', 'chops', 'alarming', 'oops', 'hopeless', 'repeats', 'dipped', 'JFK', 'Terri', 'stumble', 'fiery', 'ants', 'Madame', 'provocative', 'handicap', 'ethnicity', 'superficial', 'Dolby', 'sentiments', 'fest', 'biz', 'humility', 'Cinderella', 'Fatal', 'Carrying', 'Sucking', 'Communion', 'protagonist', 'horribly', 'hefty', 'Debut', 'microscope', 'empathy', 'thirst', 'slack', 'nostalgia', 'redeem', 'masculine', 'gag', 'Legendary', 'brilliantly', 'advises', 'digitally', 'patriotic', 'spectacle', 'rot', 'Chances', 'swords', 'creep', 'aesthetics', 'chilling', 'Colorful', 'honorable', 'brightly', 'Edmund', 'Kung', 'Reyes', 'Sergio', 'owes', 'presume', 'Gentle', 'regeneration', 'wink', 'continual', 'lousy', 'joyful', 'Magnolia', 'fabricated', 'fascination', 'Wolfe', 'endeavors', 'sailors', 'doubling', 'maze', 'bedtime', 'affinity', 'Amos', 'Sundance', 'swiftly', '8-10', 'sanity', 'mundane', 'instincts', 'tackling', 'Monty', 'reactive', 'halftime', 'smack', 'Scarlet', 'honoring', 'olives', 'rewrite', 'Chill', 'rejects', 'Berg', 'ke', 'tipped', 'lingering', 'Essentially', 'MacDonald', 'Booty', 'timeout', 'conceive', 'high-profile', 'Ellie', 'crises', 'contemplating', 'AMC', 'sting', 'heroine', 'right-wing', 'overdose', 'tunnels', 'upfront', 'activism', 'excursion', 'crave', 'deception', 'Clouds', 'patiently', 'fearful', 'engages', 'gangs', 'estrogen', 'qualifies', 'Occasionally', 'Guided', 'grim', 'Ark', 'demographics', 'Clive', 'surreal', 'Simone', 'documenting', 'lessen', 'disconnected', 'tendencies', 'linguistic', 'earnest', 'Giovanni', 'ghetto', 'Guilty', 'restless', 'floppy', 'distinctly', 'Phi', 'pivotal', 'scripting', 'Silly', 'infusion', 'warmed', 'marvel', 'relentless', 'recognizable', 'mosaic', 'rotten', 'Lola', 'cues', 'advert', 'intensely', 'seaside', 'ponder', 'Officially', 'taps', 'virtues', 'principals', 'cultivated', 'rite', 'Crush', 'reversal', 'casts', 'flourish', 'modeled', 'sparks', 'commanding', 'Thirty', 'rampant', 'wafer', 'Townsend', 'peter', 'dread', 'sublime', 'Philippe', 'marrow', 'natives', 'furry', 'Tu', 'validated', 'alas', 'booming', 'Toward', 'staircase', 'mildly', 'delights', 'imprint', 'instability', 'sophistication', 'muddy', 'pardon', 'percentages', 'skipping', 'WW', 'immature', 'enlightened', 'alternating', 'bogus', 'reminders', 'havoc', 'substantive', 'frosting', 'expressive', 'spikes', 'cinematic', 'cultivation', 'hinge', 'Jos', 'buff', 'compensated', 'stirred', 'dubious', 'Jonah', 'raging', 'enriched', 'robotic', 'Armenia', 'persistence', 'indispensable', 'narrator', 'caf', 'abound', 'Eileen', 'wrought', 'unseen', 'Able', 'Laurence', 'intimidating', 'Skins', 'perennial', 'helm', 'unrealistic', 'Returning', 'afforded', 'regrets', 'directive', 'exaggerated', 'restrictive', 'Wives', 'fueled', 'disciplined', 'proficiency', 'oatmeal', 'clueless', 'preteen', 'brilliance', 'hardened', 'transforms', 'overdue', 'Worse', 'invariably', 'MGM', 'Bartlett', 'icy', 'duly', 'melodic', 'Told', 'sands', 'relocated', 'Mermaid', 'flavours', 'hypocrisy', 'submarine', 'Chateau', 'Fifty', 'homicide', 'Loud', 'sewage', 'amused', 'Glitter', 'BMX', 'distract', 'Seas', 'Bombay', 'chimney', 'evidenced', 'benign', 'EMI', 'slew', 'booths', 'ambience', 'dreaded', 'jealousy', 'hostage', 'screenplay', 'yielded', 'Jules', 'frenzy', 'troubling', 'slender', 'jones', 'Alas', 'interpreting', 'Dickens', 'DV', 'implication', 'hinges', 'heavyweight', 'unfold', 'sailor', 'LDS', 'Marker', 'joys', 'manifestation', 'Knows', 'nudity', 'morals', 'plentiful', 'Tender', 'first-time', 'sacrificing', 'doorstep', 'Underwood', 'tossing', 'freeway', 'Trademark', 'agile', 'Plato', 'oddly', 'invade', 'disturbance', 'hardy', 'Bridget', 'Scream', 'sibling', 'murderer', 'Farrell', 'Katz', 'fret', 'B.S.', 'Baja', 'experimentation', 'courageous', 'face-to-face', 'clinically', 'plight', 'backstage', 'aspire', 'drains', 'twisting', 'systematically', 'painfully', 'nightmares', 'flowering', 'heartfelt', 'progresses', 'Aviv', 'Lynne', 'flattering', 'gripping', 'candid', 'inventive', 'stew', 'fury', 'jams', 'cherished', 'buggy', 'courtroom', 'escapes', 'curls', 'swallowing', 'floats', 'wandered', 'inflated', 'childish', 'Turks', 'notwithstanding', 'Irwin', 'asphalt', 'splits', 'blossom', 'refreshed', 'blatant', 'arrogance', 'Minnie', 'vine', 'actresses', 'anthology', 'Malone', 'peninsula', 'Amish', 'willingly', 'informs', 'profoundly', 'avoids', 'casually', 'Schultz', 'negatives', 'placeholder', 'three-dimensional', 'applaud', 'leveling', 'tongues', 'lends', 'sunk', 'doubtful', 'embark', 'Travels', 'hoops', 'stamina', 'captivating', 'Jacqueline', 'misunderstanding', 'consciously', 'Tomatoes', 'indefinitely', 'plainly', 'humiliation', 'aura', 'procedural', 'sheds', 'quartet', 'paranormal', 'accuse', 'nightclub', 'wastes', 'nicest', 'nostalgic', 'portrayal', 'vaguely', 'streamlined', 'admittedly', 'insignificant', 'sacrificed', 'theatres', 'stitched', 'eccentric', 'Eddy', 'offend', 'non-stop', 'sarcasm', 'assures', 'Resurrection', 'Kramer', 'explosions', 'lavish', 'unanswered', 'lunar', 'reliably', 'satire', 'noteworthy', 'quests', 'Wilde', 'slapped', 'watercolor', 'Browning', 'Dirk', 'digs', 'insecure', 'punching', \"ol'\", 'speculative', 'acknowledging', 'Forced', 'Freddy', 'opaque', 'Crossroads', 'simulate', 'vanished', 'scares', 'Vega', 'blockbuster', 'quota', 'Combining', 'incompetent', 'societal', 'stylist', 'sinister', 'regimes', 'shaken', 'salute', 'clears', 'dotted', 'Juni', 'respecting', 'exposes', 'S.C.', 'foreground', 'schizophrenia', 'mar', 'roadside', 'undercover', 'Larson', 'lad', 'reinforcement', 'valentine', 'extremes', 'immortal', 'imitation', 'pathology', 'trimming', 'adulthood', 'avenues', 'rhyme', 'vectors', 'ole', 'exploits', 'Clown', 'congratulate', 'Fundamentals', 'residences', 'spawn', 'enlightenment', 'awakening', 'Spectacular', 'Toro', 'nurturing', 'desperation', 'sleeper', 'Hamlet', 'exceedingly', 'uplifting', 'irresistible', 'interpersonal', 'catastrophe', 'heightened', 'implicit', 'sensational', 'ache', 'Thriller', 'novelist', 'Letterman', 'teamwork', 'overwhelmingly', 'beasts', 'logically', 'whopping', 'meltdown', 'uncertainties', 'librarian', 'leftovers', 'enchanting', 'archival', 'Vin', 'conveyor', 'FILM', 'comparatively', 'pronounce', 'thinkers', 'openness', 'savage', 'familiarity', 'sociology', 'simplistic', 'decidedly', 'wrench', '9-11', 'Conversations', 'sterile', 'miscellaneous', 'know-how', 'tightening', 'rooting', 'overlapping', 'mourning', 'Tang', 'renders', 'bothers', 'resemblance', 'Leone', 'lurking', 'dreadful', 'Driven', 'empowerment', 'Beard', 'achieves', 'serene', 'brink', 'plum', 'clones', 'insults', 'contradiction', 'dearly', 'praises', 'Strikes', 'concession', 'subconscious', 'walker', 'seriousness', 'Woo', 'TNT', 'elbows', 'one-of-a-kind', 'Shepard', 'plagued', 'Nicholson', 'bursting', 'strained', 'stab', 'Hewitt', 'dwarf', 'Sa', 'devoid', 'misguided', 'painters', 'diplomacy', 'Springsteen', 'innate', 'etched', 'documentaries', 'seventeen', 'goodwill', 'narratives', 'distractions', 'sweaty', 'rom', 'leukemia', 'Sliding', 'Wash.', 'ailments', 'Principle', 'ambiguous', 'betrayed', 'sarcastic', 'Bernstein', 'inhabited', 'Tori', 'newcomers', 'airborne', 'succeeds', 'depict', 'sank', 'Bees', 'visualize', 'selves', 'evangelical', 'Rubin', 'cello', 'agony', 'goofy', 'discerning', 'Delight', 'inclination', 'knowingly', 'capitalize', 'blasts', 'profiling', 'downtime', 'inexperienced', 'everlasting', 'Wally', 'Sabrina', 'disgrace', 'anonymity', 'corpse', 'mating', 'villains', 'hormonal', 'suitcase', 'handicapped', 'probes', 'Tolkien', 'distracting', 'immersed', 'adherence', 'redeemed', 'richness', 'Pauline', 'visionary', 'antics', 'abruptly', 'monumental', 'revelations', 'uphill', 'Sting', 'lyrical', 'originality', 'graves', 'earthly', 'Hurley', 'defeats', 'impeccable', 'Eastwood', 'compromising', 'showdown', 'Rye', '1873', 'kidnapping', 'queens', 'fluent', 'banged', 'spotting', 'impatient', 'woodland', 'brow', 'Hip-Hop', 'platter', 'boredom', 'tack', 'cheered', 'Mention', 'midway', 'magically', 'adorned', 'asparagus', 'hiatus', 'LL', 'clad', 'extraordinarily', 'shortcomings', 'varsity', 'Dong', 'startling', 'Abrams', 'whimsical', 'greasy', 'Bourne', '10-year', 'imagining', 'tenor', 'viability', 'humane', 'deterioration', 'prompting', 'Terrible', 'Hitchcock', 'close-up', 'Grease', 'guarded', 'puff', 'believable', 'Pokmon', 'heed', 'nifty', 'Pluto', 'adjective', 'philosophers', 'poignant', 'seeming', 'screenings', 'claws', 'battered', 'Ledger', 'hailed', 'surpassed', 'pacing', 'Chin', 'Lagoon', 'mellow', 'Thief', 'Dummies', 'Malik', 'blueprint', 'lite', 'gritty', 'attributable', 'Creates', 'descend', 'stale', 'mute', 'Exciting', 'Graffiti', 'fore', 'Troll', 'mandates', 'spirited', 'gothic', 'inject', 'reacting', 'Surprisingly', 'shaky', 'sweetest', 'Caucasian', 'blurred', 'preserves', 'garnered', 'Gentlemen', 'bursts', 'Javier', 'Peaks', 'plump', 'resembling', 'instruct', 'loneliness', 'diversion', 'drowned', 'blasting', 'exhaustion', 'AWAY', 'seductive', 'anchored', 'milestones', 'civilized', 'Glover', 'vengeance', 'prominence', 'workings', 'Detox', 'prominently', 'Desmond', 'Marquis', 'cling', 'Regis', 'Scandinavian', 'thunderstorms', 'edged', 'bilingual', 'televised', 'tame', 'pedigree', 'fiddle', 'airs', 'brutally', 'shedding', 'Hu', 'psychiatrist', 'Trades', 'needless', 'bolster', 'millennium', 'psyche', 'awfully', 'saturation', 'tentative', 'contenders', 'adapting', 'lent', 'slump', 'infinitely', 'intolerance', 'Wrath', 'condensed', 'vigorously', 'bustling', 'Sensitive', 'Swinging', 'dishonest', 'swung', 'Confessions', 'admirable', 'standout', 'revived', 'diner', 'Lit', 'infused', 'warp', 'cafeteria', 'Skinner', 'suicidal', 'budding', 'boosted', 'ruthless', 'fast-paced', 'Werner', 'Beckett', 'bleak', 'Nicky', 'Theological', 'ops', 'characterize', 'acidic', 'futuristic', 'Lacey', 'crotch', 'Abel', 'Smaller', 'Argentine', 'Austen', 'Ritchie', 'plotting', 'Beautifully', 'knocks', 'Quentin', 'fashions', 'debuts', 'Judging', 'Wraps', 'stiffness', 'Freud', 'hue', 'heavy-duty', 'Ally', 'unconditional', 'obnoxious', 'Abbas', 'Performances', 'adrenaline', 'inadvertently', 'whipping', 'moan', 'fuller', 'Swamp', 'wrists', 'disbelief', 'ascertain', 'rusty', 'Rockwell', 'freaks', 'feminism', 'geeks', 'Erika', 'survives', 'Reveals', 'tempered', 'Moonlight', 'zest', 'Likely', 'relish', 'freshness', 'intermittent', 'ironically', 'lovingly', 'imaginable', 'ticks', 'leaps', 'influx', 'puzzled', 'proclaim', 'incarnation', 'backlash', 'charismatic', 'unconventional', 'footing', 'stature', 'conceal', 'Olivier', 'quieter', 'interchangeable', 'retrospective', 'disgust', 'Americana', 'imperfect', 'two-way', 'requisite', 'exhausting', 'betrayal', 'wholesome', 'intrusion', 'Titus', 'wrestler', 'Sewer', 'endlessly', 'Shapiro', 'plateau', \"C'mon\", 'facet', 'violently', 'gymnastics', 'airy', 'disturb', 'classify', 'computerized', 'fishes', 'numb', 'unleashed', 'darkest', 'boils', 'brand-new', 'stills', '451', 'Romero', 'manipulating', 'Pearce', 'facade', 'rethink', 'two-day', 'coffin', 'Kissing', 'powerhouse', 'stalking', 'hostess', 'Mummy', 'articulated', 'envisioned', 'outings', 'Ver', 'sentimental', 'would-be', 'blazing', 'urgently', 'elevate', 'Reilly', 'penetrating', 'Shining', 'lighted', 'confidently', 'smashing', 'extremists', 'echoes', 'collegiate', 'parallels', 'taboo', 'chills', 'Frankenstein', 'underrated', 'sluggish', 'top-notch', 'Spiderman', 'bends', 'Extraordinary', 'coma', 'whine', 'enormously', 'Kaufman', 'discern', 'porous', 'acclaim', 'spree', 'unnamed', 'creatively', 'interrogation', 'illicit', 'Ka', 'miraculous', 'Garth', 'Dumb', 'montage', 'eagles', 'bidder', 'Rudd', 'WWF', 'Waltz', 'drown', 'worldly', 'truthful', 'contradictory', 'Schwarzenegger', 'minimalist', 'poking', 'victorious', 'unbelievably', 'grit', 'ballroom', 'pervasive', 'intrigue', 'scum', 'settles', 'wacky', 'trajectory', 'composing', 'excessively', 'savory', 'pitfalls', 'thirsty', 'Pal', 'stimulates', 'bastards', 'locale', 'ensuing', 'chiefly', 'claw', 'richly', 'uniqueness', 'snapping', 'castles', 'Favor', 'symbolism', 'Massacre', 'edgy', 'contagious', 'culprit', 'screwing', 'fruitful', 'prank', 'eye-catching', 'stuffs', 'depiction', 'embraces', 'scented', 'tidal', 'stereotype', 'Pixar', 'Eisenhower', 'goggles', 'Aniston', 'deem', 'grinder', 'predator', 'idiotic', 'newcomer', 'emphasizing', 'pastel', 'treasured', 'magician', 'tiring', 'fleeing', 'fu', 'Ferris', 'decoder', 'cleavage', 'sprouts', 'procession', 'Dani', 'begs', 'Mueller', 'flashy', 'iced', 'strides', 'blowout', 'clashes', 'motivations', 'unnoticed', 'trolls', 'Haynes', 'SNL', 'slain', 'pleasurable', 'rake', 'underestimate', 'anchors', 'Brits', 'affirm', 'ugh', 'Jarvis', 'Zipper', 'outraged', 'Mild', 'duel', 'Novak', 'chanting', 'shovel', 'correctness', 'attackers', 'increments', 'predecessors', 'Kirsten', 'criticizing', 'stink', 'anthropology', 'ecstasy', 'revolt', 'persuasive', 'intimidated', 'veneer', 'tis', 'lofty', 'painless', 'allegiance', 'gasp', 'harmed', 'Bouquet', 'Spielberg', 'cloak', 'Ghetto', 'sparse', 'misdemeanor', 'mindless', 'creeping', 'devastation', 'redundancy', 'labelled', 'endings', 'amaze', 'Gong', 'careless', 'shootings', 'endeavour', 'enticing', 'chuckle', 'provoke', 'masked', 'dramas', 'sap', 'unnatural', 'taxing', 'disgusted', 'dominates', 'dives', 'agendas', 'resentment', 'opting', 'thinly', 'curling', 'deceptive', 'cracker', 'insanely', 'Rises', 'Showtime', 'famine', 'puppets', 'Tunes', 'cameo', 'Zeus', 'ordeal', 'depleted', 'acidity', 'Easier', 'Eh', 'Entertaining', 'doctorate', 'austerity', 'Sum', 'dared', 'rhythmic', 'dreamy', 'semen', 'creed', 'smear', 'Beau', 'contrasts', 'noses', 'commonplace', 'tasteful', 'hateful', 'shear', 'horrors', 'soar', 'Fahrenheit', 'muster', 'obligatory', 'punctuation', 'kin', 'extracting', 'foo', 'Hartley', 'symmetry', 'realistically', 'crafty', 'aching', 'acquires', 'first-class', 'reefs', 'kudos', 'itch', 'Hare', 'Raphael', 'scarcely', 'ingenious', 'parrot', 'peaked', 'outage', 'dizzy', 'scandals', 'drumming', 'perfected', 'indifferent', 'augmentation', 'auditorium', 'heaps', 'binds', 'Elmo', 'pedestal', 'sprung', 'downfall', 'blondes', 'unfolding', 'gathers', 'Automatically', 'chatter', 'mistakenly', 'Spinning', 'provoking', 'affluent', 'Snacks', 'wiser', 'positives', 'fad', 'Velocity', 'hovering', 'giggle', 'onstage', 'Kidd', 'esteemed', 'unveil', 'intellectually', 'luscious', 'moods', 'fused', 'boasting', 'limp', 'blurry', 'ticking', 'Garry', 'Wilder', 'Expanded', 'physique', 'crawled', 'laden', 'leisurely', 'bitten', 'runner-up', 'shred', 'spawned', 'prowess', 'frantic', 'chronicles', 'smoky', 'Roach', 'Violent', 'illuminate', 'glaring', 'snapshots', 'verbally', 'disguised', 'complexities', 'dictates', 'sitcom', 'arithmetic', 'Smokey', 'Daphne', 'restrained', 'dissolution', 'biographical', 'artworks', 'derives', 'individuality', 'lobbyists', 'autistic', 'interfering', 'melts', 'futile', 'fraternity', 'youngster', 'shouts', 'earthy', 'happiest', 'adept', 'fades', \"'80s\", 'unlucky', 'Barrow', 'Clooney', 'Judd', 'sprinkled', 'tackled', 'retreats', 'P.C.', 'wills', 'latent', 'improperly', 'communicates', 'atrocities', 'gutter', 'omission', 'prostitute', 'vile', 'Technically', 'devise', 'Fist', 'narration', 'vomit', 'towering', 'bangs', 'remembrance', 'Thatcher', 'shameful', 'diaries', 'webcast', 'luminous', 'dangerously', 'time-consuming', 'noticeably', 'choking', 'steaming', 'last-minute', 'flavorful', 'raft', 'freeing', 'scrape', 'dysfunctional', 'disparate', 'weakest', 'chore', 'Gilmore', 'Finch', 'tightened', 'Killers', 'Byzantine', 'riff', 'appreciates', 'Demons', 'spooky', 'soothe', 'beacon', 'Corbett', 'stand-up', 'appetizer', 'penned', 'orthodox', 'enlightening', 'distilled', 'referencing', 'muted', 'lovable', 'lighten', 'hysterical', 'paws', 'meticulous', 'Kicks', 'Norma', 'elliptical', 'misplaced', 'obsessive', 'rappers', 'Sen', 'grounding', 'couture', 'Chung', 'tangled', 'Godfather', 'Bullock', 'elegantly', 'passionately', 'stunts', 'moaning', 'fullness', 'Yorkers', 'roaring', 'psychedelic', 'riffs', 'frustrations', 'outsiders', 'iteration', 'uneasy', 'Remains', 'synchronized', 'occurrences', 'Swanson', 'choreography', 'divides', 'extremist', 'Housekeeping', 'hideous', 'stump', 'insecurity', 'Bale', 'stabbing', 'dismal', 'calculus', 'accomplishing', 'clowns', 'middle-class', 'extravagant', 'Pulp', 'bedside', 'drawbacks', 'Palma', 'reassuring', 'revered', 'augmented', 'fluff', 'nuances', 'sincerity', 'retaliation', 'Beanie', 'abrasive', 'perseverance', 'blanks', 'consolation', 'confronting', 'Sweetheart', 'Beatrice', 'joyous', 'aroused', 'stalk', 'exhilarating', 'heartbreaking', 'skateboard', 'runaway', 'appalling', 'impacting', 'McKay', 'Collision', 'blackout', 'affectionate', 'suitably', 'mayhem', 'angst', 'Fidelity', 'jumbo', 'bloated', 'Yi', 'undeniable', 'wildest', 'dormant', 'rambling', 'Achilles', 'manifestations', 'Alarms', 'cohesive', 'serenity', 'shrug', 'diver', 'wearer', 'panorama', 'noir', 'understated', 'Beneath', 'uncut', 'fearless', 'conformity', 'motifs', 'buildup', 'Steamboat', 'Unexpected', 'spies', 'middle-aged', 'evoke', 'annoyance', 'exploiting', 'sprawling', 'comedians', 'Ambrose', 'perch', 'delve', 'assassin', 'glitches', 'portrays', 'tainted', 'Poets', 'pact', 'jeopardy', 'shootout', 'encompassing', 'grieving', 'lore', 'hyper', 'satisfies', 'tug', 'orphans', 'linger', 'Westbrook', 'fart', 'bestowed', 'ROSE', 'negativity', 'galore', 'Hallmark', 'nighttime', 'Clancy', 'constructs', 'diminishing', 'Chalk', 'brutality', 'viewpoints', 'parsing', 'Melrose', 'cross-country', 'motherhood', 'warmly', 'grenade', 'turbulent', 'adaptations', 'muse', 'illusions', 'garnish', 'insiders', 'Cho', 'swirl', 'overboard', 'Hilarious', 'Prelude', 'Routine', 'pawn', 'motorized', 'surfer', 'unsuspecting', 'flowed', 'concentrates', 'Mojo', 'poo', 'Alain', 'Enchanted', 'adversity', 'freaky', 'Amir', 'diva', 'septic', 'hinted', 'solemn', 'clothed', 'apex', 'dips', 'gloomy', 'playwright', 'fumes', 'pondering', 'delinquent', 'confines', 'swirling', 'chemically', 'observes', 'glide', 'boldly', 'crystalline', 'appreciative', 'behaving', 'borderline', 'illuminating', 'adultery', 'Began', 'comedic', 'unintended', 'Signals', 'nationalism', 'behind-the-scenes', 'meticulously', 'vat', 'Ran', 'Leroy', 'prognosis', 'distinguishing', 'noodle', 'Bubba', 'on-screen', 'monstrous', 'repression', 'horrified', 'Joaquin', 'Literally', 'assaults', 'smartest', 'Harmon', 'peroxide', 'sharper', 'tenderness', 'timeframe', 'substitutes', 'ambiguity', 'adolescence', 'abrupt', 'absorbs', 'snail', 'Elmer', 'PTA', 'inhabit', 'Ja', 'gore', 'oblivious', 'Frequent', 'continuum', 'flops', 'flushed', 'soulful', 'Tuck', 'McDowell', 'voyeur', 'revoked', 'modern-day', 'shading', 'subtly', 'flicks', 'artifact', 'uniformly', 'unfolds', 'Thirteen', 'knockout', 'alluring', 'PG-13', 'sinner', 'whistles', 'deli', 'energized', 'Lux', 'fearing', 'specificity', 'cumbersome', 'contentious', 'EYE', 'prehistoric', 'unbearable', 'Bard', 'understatement', 'impart', 'unfairly', 'paranoia', 'Lizard', 'melancholy', 'wan', 'Gently', 'Generations', 'Nadia', 'Boyz', 'orphan', 'eerie', 'mergers', 'Schumacher', 'peril', 'Margarita', 'filmmaking', 'Perfectly', 'peep', 'bruised', 'kinetic', 'biscuit', 'Kahn', 'gloom', 'Lolita', 'matured', 'astronauts', 'Clever', 'enact', 'Successfully', 'throbbing', 'parting', 'conveying', 'Doo', 'expertly', 'phony', 'comedies', 'diverted', 'recovers', 'submerged', 'shabby', 'gracefully', 'Tells', 'watery', 'proverbial', 'clumsy', 'all-around', 'Bubbles', 'gangster', 'horizons', 'yarns', 'wrestle', 'commend', 'whispers', 'Afraid', 'ramifications', 'pinnacle', 'slogans', 'signify', 'Seeks', 'anguish', 'fortified', 'bon', 'self-contained', 'hallmark', 'payoff', 'triangles', 'punishing', 'skates', 'side-by-side', 'pesky', 'liberated', 'weeping', \"'70s\", 'Sensation', 'hindsight', 'distinctions', 'fascist', 'fanatic', 'sexist', 'Hubert', 'contours', 'equate', 'swipe', 'indifference', 'purge', 'Raja', 'mattered', 'Chasing', 'GUYS', 'potency', 'oppressive', 'garner', 'prophetic', 'fleeting', 'thematic', 'exposition', 'restroom', 'catalytic', 'Barlow', 'ordinances', 'synagogue', 'Astoria', 'stoked', 'Promises', 'startled', 'provoked', 'someplace', 'all-inclusive', 'aptly', 'smuggling', 'Taiwanese', 'Hardly', 'sixties', 'zeal', 'captions', 'bubbly', 'summertime', 'reverence', 'hunk', 'weep', 'Neighbor', 'Boring', 'McLaughlin', 'off-season', 'outweigh', 'musicals', 'bordering', 'populace', 'respite', 'rhetorical', 'McGrath', 'Strictly', 'vividly', 'spitting', 'insistence', 'Halle', 'agreeable', 'Horrible', 'Psycho', 'bluff', 'funnier', 'iceberg', 'blatantly', 'collapses', 'camouflage', 'balm', 'spout', 'comin', 'cleverly', 'traverse', 'Mira', 'steamy', 'vulgar', 'abiding', 'muy', 'air-conditioning', 'to-do', 'brittle', 'recalling', 'guise', 'vibes', 'nagging', 'ninety', 'swimmer', 'puberty', 'sampler', 'bravery', 'upsetting', 'parlor', 'sneaky', 'cliche', 'worldview', 'elves', 'Catcher', 'Reign', 'relegated', 'elemental', 'profanity', 'deepen', 'Consequences', 'Intended', 'Ranges', 'laughable', 'undead', 'superbly', 'rips', 'knack', 'watered', 'workable', 'abbreviated', 'much-needed', 'spectator', 'ludicrous', 'congenital', 'accompanies', 'excite', 'hypocritical', 'build-up', 'overflowing', 'fiercely', 'teaming', 'Russo', 'half-hour', 'hurried', 'Kappa', 'delusional', 'bile', 'hymn', 'intimately', 'Brody', 'braided', 'adopts', 'Barrie', 'crippled', 'scripted', 'all-star', 'supple', 'laced', 'Barely', 'Kurdish', 'intellectuals', 'approximation', 'brisk', 'trembling', 'Requiem', 'juggling', 'Expands', 'Foreman', 'Becomes', 'ridicule', 'fragmented', 'hippie', 'jockey', 'menace', 'jargon', 'Baird', 'immaculate', 'drizzle', 'conveys', 'Communism', 'debilitating', 'right-hand', 'Everytime', 'guardians', 'smelly', 'impulses', 'contradictions', 'fillers', 'tweaked', 'gimmick', 'punitive', 'devotees', 'aspiration', 'retrospect', 'rockers', 'fists', 'fanatics', 'crammed', 'gurus', 'Replacing', 'jerking', 'Leagues', 'anomaly', 'affords', 'marvellous', 'sizzling', 'cunning', 'Scorpion', 'ante', 'hangover', 'anti-virus', 'Regan', 'rigged', 'culmination', 'ingenuity', 'Sling', 'subordinate', 'sham', 'evergreen', 'cascade', 'mischief', 'presumption', 'sumptuous', 'Siegel', 'bittersweet', 'glee', 'bigotry', 'Revisited', 'strung', 'G.I.', 'stoned', 'embodies', 'chronicle', 'misfortune', 'unaffected', 'Faithful', 'Ridley', 'Saigon', 'dismay', 'summons', 'sexes', 'charisma', 'improvised', 'cringe', 'monarch', 'tacky', 'rapport', 'anew', 'wartime', 'hypnotic', 'arresting', 'Quotations', 'Notwithstanding', 'astronaut', 'demeanor', 'unloading', 'strut', 'paralyzed', 'Josef', 'enthusiastically', 'long-range', 'recital', 'ominous', 'perpetrators', 'installments', 'masterpieces', 'dissatisfied', 'resonate', 'caste', 'retina', 'minimally', 'glorified', 'sandal', 'predicament', 'Koreans', \"'90s\", 'embellished', 'yearning', 'intelligently', 'serpent', 'moody', 'tides', 'adored', 'uncanny', 'gaping', 'skewed', 'crock', 'folly', 'invaders', 'Motown', 'grossly', 'mystic', 'spoof', 'sorely', 'negligible', 'metaphysical', 'portraying', 'grader', 'babysitter', 'graveyard', 'decadent', 'alarmed', 'Hanks', 'railing', 'diplomat', 'Amid', 'Armageddon', 'libido', 'narrated', 'Delivers', 'nonfiction', 'Pageant', 'self-help', 'Mastering', 'doggie', 'rebirth', 'eloquent', 'artistry', 'friggin', 'incandescent', 'rarity', 'metaphors', 'whirlwind', 'Fears', 'McCann', 'endemic', 'unresolved', 'Pryor', 'markedly', 'Heller', 'sabotage', 'Shocking', 'IMAX', 'narcotics', 'ethos', 'Burr', 'pours', 'psychotic', 'bloodstream', 'inflict', 'unstoppable', 'Possession', 'murderous', 'eternally', 'decency', 'boomers', 'Nicely', 'wannabe', 'autopsy', 'hazy', 'footnote', 'ripper', 'Axel', 'comical', 'afloat', 'nutty', 'affirmation', 'schoolgirl', 'creme', 'disdain', 'innumerable', 'dogma', 'insulted', 'Changer', 'rebellious', 'Shrek', 'persecuted', 'overwhelm', 'grandeur', 'Excessive', 'Mohawk', 'probing', 'Kimmel', 'grasping', 'Metropolis', 'lumps', 'propelled', 'Recalls', 'drooling', 'apocalypse', 'dispense', 'Rosario', 'trump', 'junkie', 'flattened', 'craftsmen', 'black-and-white', 'prod', 'Hannibal', 'amazement', 'Blockbuster', 'Seinfeld', 'Hatfield', 'patched', 'gazing', 'Ramsay', 'streamed', 'Decent', 'squarely', 'pretended', 'banter', 'Socrates', 'old-school', 'Exxon', 'skateboarding', 'powerfully', 'Strangers', 'swan', 'unquestionably', 'musings', 'condensation', 'Throwing', 'companionship', 'admiring', 'collaborators', 'widen', 'warped', 'tonal', 'stinks', 'conspicuous', 'Alexandre', 'reigns', 'ron', 'burgeoning', 'meaty', 'Parisian', 'evokes', 'improvisation', 'goo', 'transports', 'erratic', 'moratorium', 'resists', 'usher', 'slapping', 'thrills', 'horrifying', 'bumping', 'hinged', 'Aladdin', 'Sleeper', 'remorse', 'D.J.', 'intrusive', 'shimmering', 'skillful', 'whence', 'nationalist', 'Sensual', 'impetus', 'Ritter', 'Lan', 'Slick', 'embody', 'thought-provoking', 'oblivion', 'bouts', 'nonstop', 'Savvy', 'miserably', 'evolves', 'horrid', 'avalanche', 'insensitive', 'gruesome', 'Oddly', 'Siberian', 'Damned', 'wade', 'calibrated', 'humming', 'barrage', 'unsurpassed', 'sensibility', 'Curves', 'Enron', 'conceivable', 'recount', 'rapids', 'flashback', 'delectable', 'journalistic', '1790', 'Grief', 'majesty', 'tabloid', 'fruition', 'buffs', 'ahem', 'venerable', 'Viewers', 'slug', 'senseless', 'bounces', 'Waldo', 'Effectively', 'distinguishes', 'pleas', 'payback', 'Asphalt', 'year-end', \"'til\", 'heaviest', \"'60s\", 'tapestry', 'sugary', 'cheeky', 'numbness', 'testimonial', 'loom', 'recite', 'jolly', 'strenuous', 'Vampires', 'favorably', 'appropriated', 'sheen', 'evocative', 'indulgence', 'explodes', 'Goo', 'Crocodile', 'triumphant', 'perverted', 'masterful', 'Rowling', 'kung', 'Bray', 'suggestive', 'Jacobson', 'Orwell', 'splendor', 'Carrera', 'Andr', 'ape', 'setups', 'undeniably', 'objectionable', 'Macbeth', 'marginally', 'tequila', 'collide', 'overt', 'wondrous', 'frontman', 'undone', 'marquee', 'esoteric', 'Siege', 'Kang', 'siren', 'gels', 'Pax', 'best-known', 'ignite', 'reassure', 'Mana', 'facilitator', 'Texan', 'underdog', 'Godfrey', 'appalled', 'Andrei', 'manifesto', 'prejudices', 'liberalism', 'VH1', 'terrier', 'Melville', 'Godzilla', 'Hogwarts', 'underestimated', 'Versace', 'feral', 'hand-held', 'aptitude', 'ardent', 'sequels', 'apartheid', 'pore', 'well-established', 'cursing', 'waffle', 'savor', 'degraded', 'severed', 'commended', 'Ranks', 'penchant', 'contemplation', 'stormy', 'bouncy', 'revolve', 'Rainy', 'Liza', 'Griffiths', 'rotting', 'plotted', 'tragedies', 'Ikea', 'arctic', 'relentlessly', 'Veggie', 'Sparkling', 'Isabelle', 'raving', 'clutches', 'forgets', 'family-friendly', 'Akira', 'flatter', 'Longest', 'conclusive', 'brim', 'preferring', 'humankind', 'ducts', 'gravitational', 'unexplained', 'Puts', 'evaporation', 'literate', 'Cleopatra', 'greased', 'seduce', 'anatomical', 'envious', 'pleases', 'psychologically', 'embarking', 'relays', 'detriment', 'pretentious', 'on-board', 'lucid', 'pathological', 'Mule', 'hyped', 'Tarantino', 'bona', 'tracts', 'sandbox', 'Granger', 'compulsive', 'unbalanced', 'alternately', 'prima', 'detachment', 'purported', 'Highlander', 'Maguire', 'Snowman', 'aesthetically', 'wide-ranging', 'ut', 'genesis', 'timid', 'Yanks', 'suckers', 'Cortez', 'visceral', 'laziness', 'Nickelodeon', 'uncles', 'shameless', 'baffled', 'contemporaries', 'emptiness', 'fondly', 'parable', 'Wen', 'Radcliffe', 'ex-wife', 'puzzling', 'Kingsley', 'orchestrated', 'fandom', 'Mattel', 'Jeanette', 'Beating', 'well-written', 'adjectives', 'Exhibits', 'lowly', 'underworld', 'captivated', 'sporadic', 'Berkley', 'Shorty', 'conversational', 'Alias', 'manic', 'feeble', 'expiry', 'samurai', 'Molina', 'Nervous', 'deteriorating', 'endearing', 'hysteria', 'solidly', 'quintessential', 'ancillary', 'Benoit', 'bumpy', 'niches', 'cross-section', 'eighties', 'convict', 'Nikita', 'Whitaker', 'streaks', 'clams', 'Terrific', 'densely', 'butler', 'Mika', 'Intimate', 'mined', 'ruminations', 'avant-garde', 'unprepared', 'abyss', 'sassy', 'Handsome', 'winged', 'creeps', 'uncontrolled', 'demonic', 'Adaptation', 'two-hour', 'African-Americans', 'Sleek', 'never-ending', 'unthinkable', 'decomposition', 'gamut', 'Orgasm', 'uncomplicated', 'precinct', 'Derry', 'rattling', 'Rosenthal', 'Fails', 'undermined', 'Napoli', 'Aidan', 'logistical', 'gratifying', 'intertwined', 'pegs', 'coherence', 'Analyze', 'weaponry', 'over-the-top', 'abundantly', 'eviction', 'ploy', 'screenwriter', '30-year', 'filth', 'Jagger', 'Passionate', 'BV', 'Justine', 'exaggeration', 'smug', 'adhering', 'degrading', 'troupe', 'induces', 'cranky', 'bruising', 'Throws', 'unsettling', 'ROCKS', 'resonant', 'humiliated', 'stereotypical', 'deceit', 'graced', 'murky', 'giddy', 'shudder', 'flimsy', 'Sly', 'glimpses', 'instinctively', 'holocaust', 'trims', 'hampered', 'Cedric', 'fide', 'weathered', 'two-dimensional', 'perverse', 'Buff', 'goth', 'freebie', 'Chaplin', 'ethereal', 'Howie', 'solace', 'Beers', 'upsets', 'anarchy', 'spiced', 'grinning', 'aversion', 'armchair', 'ballast', 'hastily', 'applauded', 'seduction', 'fathom', 'primal', 'lax', 'riveting', 'betray', 'late-night', 'Amazingly', 'Prozac', 'intermediary', 'eyelids', 'campfire', 'modesty', 'conscientious', 'inspirations', 'milking', 'anti-war', 'pauses', 'Boomers', 'werewolf', 'forceful', 'rein', 'Tuxedo', 'pessimistic', 'Electra', 'Lanes', 'Hush', 'improbable', 'Borg', 'Spade', 'weaves', 'wretched', 'one-sided', 'Masterpiece', 'hostages', 'luster', 'Craven', 'discouraging', 'governs', 'symmetrical', 'hooker', 'giggles', 'chewy', 'Octopus', 'Aggressive', 'Weinstein', 'Weighted', 'stricken', 'untold', 'evade', 'perpetrated', 'ransom', 'fatally', 'spiked', 'inert', 'nobility', 'treacherous', 'drags', 'disobedience', 'lightest', 'grating', 'Firth', 'dreary', 'nerdy', 'vistas', 'longs', 'genitals', 'Deeds', 'Affleck', 'defiance', 'intolerant', 'Dungeons', 'Accidental', 'encountering', 'Fabian', 'avert', 'Slight', 'co-star', 'prettiest', 'Criterion', 'fuses', 'unintentionally', 'Chai', 'categorize', 'undermining', 'Hanukkah', 'turntable', 'Sandler', 'Prejudice', 'hopelessly', 'lifeless', 'virtuous', 'Valium', 'drenched', 'POW', 'intimidate', 'Rarely', 'afterlife', 'Hepburn', 'matchmaking', 'contrived', 'arduous', 'Creepy', 'canning', 'resent', 'thoughtfully', 'Sven', 'exalted', 'geeky', 'skeptics', 'steeped', 'Salle', 'tempt', 'Clements', 'farce', 'recyclable', 'delicately', 'existential', 'repellent', 'boardwalk', 'rife', 'sultry', 'astray', 'fueling', 'hallways', 'charting', 'inconsistencies', 'Marisa', 'adamant', 'Kangaroo', 'prism', 'manhood', 'propensity', 'opium', 'delightfully', 'Lear', 'enigmatic', 'carnage', 'symbolizes', 'Confusion', 'pious', 'belated', 'breezy', 'Submarine', 'industrialized', 'locales', 'unchecked', 'deliciously', 'Alfonso', 'steadfast', 'leaping', 'Attempts', 'distressing', 'aboriginal', 'Smarter', 'seduced', 'veritable', 'espionage', 'overshadowed', 'inertia', 'expulsion', 'fishy', 'sensibilities', 'Bette', 'sexism', 'stylized', 'justifies', 'nourishing', 'maneuvers', 'drinker', 'puddle', 'paraphrase', 'underlined', 'lured', 'familial', 'skeletons', 'staggered', 'apprehension', 'Tron', 'discloses', 'Undercover', 'overtake', 'storylines', 'patchwork', 'watchful', 'fussy', 'grasped', 'obscured', 'sly', 'immersive', 'superman', 'stalker', 'spat', 'participatory', 'unmistakable', 'desolate', 'menacing', 'fictitious', 'extravaganza', 'interplay', 'straining', 'Insomnia', 'Kline', 'immerse', 'slumber', 'world-renowned', 'shoving', 'Familiar', 'herring', 'Kidman', 'Fidel', 'Ayurveda', 'clinch', 'life-changing', 'perpetually', 'adage', 'endorses', 'thrives', 'overrun', 'pitiful', 'pitted', 'fling', 'wilt', 'biologically', 'elicit', 'psycho', 'CELL', 'Characterization', 'aisles', 'unclean', 'complicate', 'distort', 'bracing', 'feces', 'transcend', 'cuddly', '18-year-old', 'Encounters', 'estranged', 'amassed', 'Feathers', 'copious', 'Bertrand', 'microwaves', 'purest', 'GUN', 'pornographic', 'Dodger', 'paces', 'tickle', 'cub', 'palpable', 'Lush', 'littered', 'inept', 'Goliath', 'Jed', 'innovators', 'stately', 'offsets', 'defunct', 'instructive', 'astute', 'Ode', 'gait', 'multi-million', 'doubtless', 'Tarzan', 'protestors', 'grown-up', 'expanse', 'inversion', 'Loaf', 'Sinks', 'Kept', 'benevolent', 'obscurity', 'Bergman', 'Cruel', 'Carmichael', 'Zhao', 'liberating', 'apes', 'interestingly', 'gall', 'substituting', 'Deeper', 'Chong', 'malnutrition', 'Lethal', 'convoluted', 'Jiang', 'Girlfriends', 'operas', 'nurtured', 'cheery', 'fosters', 'Hearst', 'heartbreak', 'arbitrarily', 'resorting', 'absurdity', 'corny', 'cocky', 'unsuccessfully', 'resourceful', 'hammering', 'triumphs', 'rut', 'Dragonfly', 'regal', 'graces', 'cram', 'Mixes', 'Witherspoon', 'oftentimes', 'no-brainer', 'overkill', 'heartache', 'forcefully', 'spasms', 'misty', 'presiding', 'Stallion', 'churning', 'peels', 'confluence', 'onscreen', 'Freaks', 'romp', 'UB', 'blockage', 'Challenging', 'objectivity', 'Somewhat', 'tolerable', 'furiously', 'blemishes', 'haunts', 'redneck', 'lagging', 'lunatic', 'Padre', 'Notorious', 'sketchy', 'hoods', 'ferocious', 'bearded', 'SLA', 'unattractive', 'lackluster', 'spaceship', 'Yoda', 'howling', 'small-scale', 'Happily', 'temptations', 'Steinberg', 'branched', 'irreversible', 'prevails', 'craziness', 'heresy', 'monologue', 'bitterly', 'assertive', 'cowardly', 'discontent', 'struts', 'curiously', 'Stiff', 'manipulative', 'Gallo', 'morbidity', 'Spalding', 'Usual', 'dumplings', '15-year-old', 'fluke', 'illogical', 'stomp', 'despicable', 'appease', 'Carlin', 'Cahill', 'racked', 'in-between', 'baroque', 'clout', '20-year-old', 'Liar', 'lament', 'crazed', 'condone', 'cinematography', 'boasted', 'Waking', 'Guillermo', 'purposeful', 'indulging', 'hogs', 'limbo', 'bummer', 'uncompromising', 'athleticism', 'Greta', 'selfless', 'treatise', 'abusers', 'blinding', 'upholstered', 'loopholes', 'underscores', 'fledgling', 'cohesion', 'protagonists', 'waged', 'storyteller', 'tireless', 'vu', 'eroded', 'chainsaw', 'unintentional', 'stylistic', 'antidote', 'outwardly', 'punishable', 'Comedian', 'Balkans', 'shenanigans', 'miraculously', 'contradicts', 'clocked', 'purposefully', 'UHF', 'nocturnal', 'Franois', 'Asbury', 'fresher', 'impair', 'horde', 'puke', 'neglecting', 'scant', 'camaraderie', 'Femme', 'secondhand', 'exacting', 'Lau', 'Johnnie', 'inhale', 'momentary', 'Worthy', 'exploratory', 'nuanced', 'flashbacks', 'ragged', 'eyeballs', 'Scooby', 'undisputed', 'first-rate', 'stroked', 'jock', '9.50', 'quirks', 'stinky', 'satirical', 'aborted', 'egregious', 'departs', 'prequel', 'graphically', 'legion', 'mischievous', 'Lai', 'amuse', 'grueling', 'good-looking', 'hammers', 'insatiable', 'potion', 'colonel', 'unimaginable', 'Fairly', 'Janine', 'hippies', 'viewfinder', 'jerky', 'relic', 'mesmerizing', 'crooks', 'namesake', 'choreographed', 'thwart', 'spin-off', 'gangsta', 'Mendes', 'fisher', 'prosthetic', 'fondness', 'Menace', 'Horns', 'fairytale', 'malice', 'tragically', 'timelines', 'Glazed', 'scrubbing', 'precarious', 'snuck', 'cynicism', 'resonates', 'videogame', 'wishful', 'breakdowns', 'intently', 'brine', 'ills', 'personable', 'hoot', 'ravaged', 'meager', 'muck', 'finesse', 'totalitarian', 'nada', 'Rambo', 'Gosling', 'clich', 'distortions', 'MIB', 'one-hour', 'bong', 'exclamation', 'lukewarm', 'infidelity', 'trumps', 'carousel', 'chases', 'wreckage', 'incomprehensible', 'Sturdy', 'ruffle', 'Ballot', 'dwellers', 'fulfills', 'Storytelling', 'Vicente', 'audacity', 'scooped', 'delicacy', 'troopers', 'warmest', 'Posey', 'trot', 'Stealing', 'Bradbury', 'excels', 'plunging', 'brat', 'marketable', 'Painful', 'justifying', 'footnotes', 'alienated', 'emptying', 'retribution', 'condescending', 'gunfire', 'invincible', 'uproar', 'long-running', 'sleepless', 'dynamite', 'SLC', 'fingernails', 'Kurds', 'unqualified', 'churn', 'Follows', 'well-rounded', 'exquisitely', 'Enigma', 'soggy', 'Julianne', 'rousing', 'inventing', 'videotape', 'Smackdown', 'shadowy', 'jaded', 'stunningly', 'grotesque', 'dares', 'leaky', 'Nicks', 'caricature', 'ilk', 'reconciled', 'Chainsaw', 'breathless', 'laid-back', '14-year-old', 'Merci', 'macho', 'interlocking', 'breathes', 'grandkids', 'post-war', 'carol', 'English-language', 'crux', 'Auschwitz', 'Juliette', 'unread', 'takeoff', 'tiresome', 'atrocious', 'willful', 'Uncertainty', 'DiCaprio', 'modulated', 'dainty', 'gliding', 'detract', '12-year-old', 'fifties', 'Colgate', 'negate', 'open-minded', 'Strangely', 'grub', 'Yiddish', 'throwback', 'breezes', 'Slap', 'woe', 'whit', 'commune', 'depictions', 'closeness', 'inflate', 'exiled', 'heady', 'Schaefer', 'figurative', 'Asks', 'Paltrow', 'Mora', 'nonexistent', 'Starship', 'transcends', 'lauded', 'violinist', 'Sophisticated', 'strikingly', 'stuffy', 'Gangster', 'skillfully', 'J.M.', 'hamburgers', 'rainbows', 'structuring', 'stomping', 'Expecting', 'admirers', 'anecdote', 'thorn', 'delusions', 'Reginald', 'underscore', 'loathe', 'Thurman', 'fraught', 'stardom', 'grainy', 'fateful', 'fright', 'alienation', 'swaying', 'open-ended', 'mediocrity', 'dashing', 'anti-Semitism', 'cashing', 'burdened', 'sickening', 'conjure', 'Orson', 'heinous', 'Ricci', 'redeeming', 'Loyal', 'Loses', 'coldest', 'up-and-coming', 'modestly', 'Awful', 'composure', 'Caine', 'hindered', 'Hawke', 'blob', 'unequivocally', 'heartily', 'voluptuous', 'tumultuous', 'Hayek', 'fable', 'strangest', 'conflicted', 'Huston', 'jackass', '22-year-old', 'Drunken', 'sadistic', 'wading', 'cutout', 'riddle', 'recreated', 'titans', 'morbid', 'Tupac', 'promenade', 'jagged', 'self-control', 'sociological', 'Ackerman', 'high-energy', 'defies', 'Hitchens', 'bourgeois', 'tagline', 'Fairies', 'tangy', 'infrequently', 'must-see', 'Anakin', 'Yong', 'groan', 'unruly', 'glacial', 'truncated', 'comforted', 'ushered', 'Altar', 'juncture', 'tributes', 'anchoring', 'scorn', 'reunions', 'heyday', 'harrowing', 'calibre', 'succumb', 'bloodshed', 'fantastically', 'custom-made', 'reserving', 'rigorously', 'sweetly', 'transplanted', 'protracted', 'choppy', 'excesses', 'crusty', 'veiled', 'anarchist', 'perplexed', 'MPAA', 'disapproval', 'hallucinations', 'instructs', 'dreamer', 'reputations', 'firmer', 'Altman', 'impressively', 'digested', 'Hanley', 'stinging', 'sorority', 'dismantle', 'ENOUGH', 'heartless', 'splashing', 'sparring', 'Halfway', 'Curling', 'playbook', 'fanboy', 'gusto', 'poodle', 'flickering', 'Vinnie', 'faked', 'inseparable', 'Horner', 'Lame', 'rudimentary', 'Paxton', 'uplift', 'Eyre', 'simmering', 'flaky', 'dissection', 'Informative', 'Ghostbusters', 'juggle', 'Scorsese', 'liberally', 'debatable', 'backseat', 'Kmart', 'doubting', 'cartons', 'opposites', 'repressed', 'wrecks', 'populist', 'stylists', 'undermines', 'spades', 'invigorating', 'Chao', 'heroism', 'oo', 'eloquently', 'marathons', 'pep', 'Verne', 'overtly', 'baggy', 'navel', 'channeling', 'authenticate', 'reactionary', 'chronically', 'perils', 'ramblings', 'supremely', 'shattering', 'mid-range', 'consummate', 'truffle', 'randy', 'Hermitage', 'Looney', 'full-blown', 'blight', 'opacity', 'botched', '13-year-old', 'unifying', 'full-fledged', 'randomness', 'convincingly', 'nonsensical', 'seizing', 'goers', 'Cows', 'Spit', 'Remarkable', 'rhino', 'yawn', 'gullible', 'barbed', 'Barrymore', 'Serbs', 'upheaval', 'idealistic', 'Genevieve', 'Christophe', 'renown', 'democracies', 'alchemy', 'countenance', 'Coppola', 'abomination', 'salient', 'Sopranos', 'smartly', 'smirk', 'assassins', 'normative', 'plastered', 'nuance', 'engulfed', 'dispel', 'palatable', 'pretends', 'tickled', 'awkwardly', 'wielding', 'calamity', 'compulsion', 'chimes', 'Pow', 'affectionately', 'affirms', 'sunburn', 'kiosks', 'revitalize', 'bravely', 'glancing', 'Theron', 'unwittingly', 'intoxication', 'hapless', 'reincarnation', 'intrepid', 'defiant', 'entree', 'incessant', 'kindred', 'keel', 'slowest', 'Bigelow', 'scandalous', 'low-key', 'projectile', 'descends', 'centering', 'harnesses', 'Shallow', 'maniac', 'orbits', 'distillation', 'action-packed', 'tinkering', 'indignation', 'busts', 'sleazy', 'explorations', 'sympathize', 'intuitively', 'undeveloped', 'marred', 'predictably', 'sedentary', 'tact', 'celebratory', 'dishonesty', 'extrusion', 'unearthed', 'pegged', 'Fierce', 'ferret', 'Flick', 'inexplicable', 'Segal', 'failings', 'sold-out', 'earnestly', 'ensues', 'revel', 'oversize', 'cad', 'boundless', 'subtlety', 'Toes', 'Nemesis', 'bogged', 'Forster', 'cheerfully', 'arcane', 'Distances', 'magnified', 'Lina', 'Woolf', 'feisty', 'nimble', 'meditative', 'Uma', 'mojo', 'superfluous', 'evoked', 'Jeopardy', 'Parton', 'sprawl', 'euphoria', 'Margot', 'humbling', 'outlandish', 'affirming', 'alleys', 'Herzog', 'extortion', 'homophobia', 'figuratively', 'Tres', 'all-out', 'mimics', 'disappointments', 'equalizer', 'Goldie', 'privy', 'trombone', 'Helms', 'torturing', 'Germanic', 'publicist', 'anxieties', 'parrots', 'stumbles', 'spectacularly', 'Tries', 'angling', 'effecting', 'mushy', 'shoddy', 'affections', 'Archibald', 'impulsive', 'romances', 'sparking', 'extant', 'brooding', 'Barbarian', 'yearn', 'clunky', 'satisfactorily', 'expectant', 'choirs', 'unbroken', 'dissolves', 'replete', 'rapes', 'serials', 'Weil', 'wanton', 'Shu', 'likened', 'ponytail', 'Troopers', 'tasteless', 'likeable', 'princesses', 'robberies', 'tangle', 'gleaned', 'auspicious', 'A.C.', 'skips', 'head-on', 'intoxicating', 'rung', 'torpedo', 'frighten', 'overuse', 'Filmmaker', 'extradition', 'grapple', 'shiver', 'burrito', 'intrinsically', 'well-defined', 'diminishes', 'excruciating', 'Gaye', 'handguns', 'gags', 'punctuated', 'Rea', 'stupidly', 'trashy', 'masala', 'impersonal', 'Ridiculous', 'Broomfield', 'tormented', 'scribe', 'adoration', 'dichotomy', 'riddled', 'Hinton', 'salvaged', 'Awkward', 'exuberant', 'Trapped', 'annex', 'Kieran', 'combatants', 'posse', '19th-century', 'assesses', 'persistently', 'Determined', 'gimmicks', 'unwavering', 'Ignoring', 'doodle', 'blunder', 'hard-earned', 'bristles', 'backstory', 'keenly', 'sieve', 'Hades', 'Redford', 'morph', 'spectacles', 'wildcard', 'cuter', 'die-hard', 'sequins', 'brimming', 'faking', 'eventful', 'Frodo', 'snappy', 'willfully', 'Legally', 'subversive', 'illuminates', 'awry', 'shreds', 'lull', 'bewildered', 'live-action', 'rattled', 'splashed', 'entangled', 'weakly', 'shortness', 'victimized', 'Clue', 'autobiographical', 'Stays', 'ode', 'motocross', 'slipper', 'suspending', 'Fluffy', 'cautions', 'Mehta', 'ascension', 'revisiting', 'likable', 'posterity', 'overgrown', 'pranks', 'intersect', 'forte', 'Gifford', 'Antitrust', 'O.K.', 'gushing', 'agnostic', 'fringes', 'intolerable', 'shockingly', 'commendable', 'Guzman', 'nave', 'smitten', 'Coolidge', 'working-class', 'crushes', 'expend', 'morphed', 'repetitions', 'Welles', 'Bambi', 'Shatner', 'fetching', 'Tonga', 'brunt', 'Bai', 'EXIT', 'Coburn', 'keg', 'concoction', 'Kev', 'mein', 'Stomp', 'Plummer', 'J.K.', 'Genet', 'tantalizing', 'Cyndi', 'inhuman', 'tenderly', 'dumbest', 'sneeze', 'Pug', 'gifting', 'wreak', 'Suffice', 'Memorable', 'hounds', 'meandering', 'whirl', 'gory', 'notches', 'deviant', 'Confirms', 'dazzle', 'needlessly', 'bickering', 'principled', 'jolt', 'Supposedly', 'aggravating', 'whiny', 'flamboyant', 'gasping', 'sandwiched', 'Jakob', 'self-interest', 'sparkles', 'valiant', 'Helene', 'daft', 'Releasing', 'sanded', 'ballerina', 'Moulin', 'coolness', 'stench', 'observant', 'tribulations', 'gratefully', 'defiantly', 'slaps', 'adapts', 'generational', 'shun', 'gratuitous', 'Smokers', 'frat', 'penetrates', 'heartwarming', 'Stooges', 'acumen', 'rowdy', 'Entertainer', 'lightness', 'provocation', 'high-powered', 'Callie', 'after-school', 'conundrum', 'confuses', 'disservice', 'hallmarks', 'glides', 'seater', 'unwillingness', 'Delia', 'novella', 'idiocy', 'sensuality', 'profane', 'holographic', 'unwise', 'trove', 'Rembrandt', 'incoherent', 'saddest', 'trumpets', 'sobering', 'Kafka', 'Franc', 'uneventful', 'animator', 'swims', 'Harsh', 'genial', 'lingers', 'gong', 'awe-inspiring', 'Laramie', 'big-time', 'Kissinger', 'Obvious', 'Carrey', 'somber', 'unorthodox', 'lint', 'Barrels', 'metaphorical', 'Kubrick', 'Adolescents', 'Fontaine', 'indulgent', '10-year-old', 'mercenary', 'Pretend', 'imitating', 'hyperbole', 'epiphany', 'glimmer', 'Sorcerer', 'civility', 'ingest', 'transcendent', 'fatality', 'dimmer', 'sustains', 'counterproductive', 'McGowan', 'sustenance', 'succinct', 'shocker', 'Stonehenge', 'fanatical', 'miniseries', 'Burkina', 'infuse', 'smoothed', 'Stallone', 'lectured', 'damning', 'suspiciously', 'Undoubtedly', 'wanders', 'inexplicably', 'wounding', 'soapy', 'stave', 'twenty-first', 'Columbine', 'Freaky', 'swagger', 'idiom', 'reworked', 'cuteness', 'warden', 'deranged', 'unhappiness', 'undercut', 'mysticism', 'subsided', 'cadence', 'disingenuous', 'arouse', 'fundamentalists', 'poise', 'Spreads', 'painkillers', 'Imitation', 'fanciful', 'detractors', 'bled', 'skyscraper', 'Heartbreak', 'self-determination', 'odyssey', 'Collateral', 'co-operative', 'sci', 'spits', 'opulent', 'pedagogy', 'Tully', 'slanted', 'Inuit', 'downsizing', 'aficionados', 'monotonous', 'artistically', 'pretense', 'Tchaikovsky', 'drab', 'ready-made', 'contemplative', 'offbeat', 'drifts', 'idealism', 'recesses', 'overpowered', 'Pentecostal', 'Lookin', 'convinces', 'jazzy', 'falcon', 'italics', 'silliness', 'crucifixion', 'weighty', 'Maud', 'twinkle', 'blushing', 'adherents', 'postmodern', 'burlesque', 'eagerness', 'intermittently', 'unsettled', 'Tailored', 'deceptively', 'erasing', 'cross-cultural', 'cautionary', 'depraved', 'scariest', 'little-known', 'Karim', 'Reaches', 'sensitivities', 'lewd', 'Daring', 'swamped', 'ditched', 'crumb', 'moustache', 'Dario', 'jarring', 'impossibly', 'ramble', 'yung', 'oozing', 'agonizing', 'inquisitive', 'tiniest', 'Marvelous', 'complacency', 'whiff', 'slugs', 'Ismail', 'spousal', 'parodies', 'ex-girlfriend', 'taut', 'Bela', 'searing', 'zealous', 'loco', 'tailor-made', 'aberration', 'exudes', 'redone', 'tome', 'Selby', 'tosses', 'bothersome', 'futility', 'Goofy', 'devious', 'Hibiscus', 'snagged', 'Elie', 'undergrad', 'sympathies', 'lingo', 'overcomes', 'pausing', 'erupt', 'Wanderers', 'accomplishes', 'heretofore', 'mind-blowing', 'merges', 'Transporter', 'stirs', 'tremble', 'spotlights', 'tacked', 'bombshell', 'nascent', 'spotty', 'skunk', 'repulsive', 'raunchy', '65th', 'wailing', 'small-town', 'bypassing', 'turd', 'self-conscious', 'substandard', 'indulged', 'zips', 'admirer', 'pompous', 'Bebe', 'Significantly', 'vices', 'woefully', 'well-balanced', 'indigestion', 'unending', 'sensuous', 'avis', 'bluntly', 'topple', 'maverick', 'perplexing', 'shrewd', 'Scrooge', 'kiddie', 'Antonia', 'grandiose', 'chastity', 'Demands', 'caterer', 'Hawley', 'darned', 'post-production', 'baffle', 'sag', 'Murdock', 'baffling', 'Armenians', 'Niro', 'Mideast', 'unrelenting', 'labors', 'drivel', 'Ilya', 'lapel', 'raucous', 'Cassel', 'Faso', 'Drowning', 'sketchbook', 'prologue', 'wobbly', 'pulsating', 'down-to-earth', 'monastic', 'injustices', 'Wiseman', 'Tropic', 'virtuoso', 'courtship', 'no-nonsense', 'abysmal', 'Fisk', 'existent', 'maxim', '26-year-old', 'Hrs', 'drips', 'multifaceted', 'stagnation', 'eponymous', 'enamored', 'wattage', 'hairline', 'afterthought', 'serviceable', 'Bundy', 'Meant', 'improvise', 'decisively', 'stalked', 'reopens', 'N.M.', 'mush', 'ecologically', 'avant', 'lags', 'dearth', 'latches', 'Wilco', 'Examines', 'X-Files', 'Shakes', 'vignettes', 'Dumas', 'pungent', 'innocuous', 'glows', 'spunk', 'satanic', 'juxtaposition', 'charred', 'Coupling', 'overtones', 'eerily', 'muffled', 'jumpsuit', 'Snatch', 'fuelled', 'conceptions', 'stoop', 'snooze', 'travesty', 'enigma', 'embellishment', 'Measured', 'inane', 'unsolved', 'wagers', 'leaner', 'directorial', 'tenuous', 'pandering', 'bigoted', 'forsaken', 'Clones', 'dyslexia', 'diss', 'chemists', 'aristocratic', 'mired', 'colonialism', 'full-on', 'Combines', 'motionless', 'flagging', 'unholy', 'Cristo', 'Denzel', 'alienate', 'Sorority', 'fluctuating', 'labored', 'whimsy', 'thyself', 'aloft', 'Earns', 'characterizes', '51st', 'diverting', 'invokes', 'Monsoon', 'excites', 'instilled', 'antagonists', 'resurrect', 'theorist', 'bugged', 'wayward', 'surrendering', 'heartland', 'aftertaste', 'Ayala', 'fatter', 'admirably', 'posturing', 'uncovers', 'Ballistic', 'dwells', 'insistent', 'Bland', 'superstitious', 'Charlize', 'touts', 'smeared', 'unscathed', 'Elmore', 'Seine', 'impresses', 'thrashing', 'Foolish', 'donna', 'narcissistic', 'skimpy', 'ever-growing', 'untrained', 'Gandalf', 'stuttering', 'quench', 'shackles', 'relayed', 'well-deserved', 'aloof', 'imagines', 'energizing', 'purportedly', 'Visually', 'Aldrich', 'benefitted', 'smarts', 'grinds', 'artsy', 'stifling', 'hookers', 'smothered', 'Matheson', 'bard', 'smacks', 'artful', 'recreating', 'Lambs', 'deftly', 'Frida', '15-year', 'teeming', 'raps', 'captives', 'Credibility', 'trimmings', 'bohemian', 'buoy', 'geriatric', 'Gump', 'Chteau', 'antiseptic', 'springing', 'kiddies', 'Westerners', 'saddled', 'uninteresting', 'criminally', 'Pap', 'preposterous', 'Professionally', 'heist', 'environs', 'darkly', 'terminally', 'clique', 'farts', 'McFarlane', 'intricately', 'introspection', 'neurotic', 'cranked', 'Ong', 'milieu', 'Macaroni', 'lapses', 'retrograde', 'voyages', 'weirdness', 'fantastical', 'cipher', 'spied', 'molestation', 'retelling', 'Invincible', 'incessantly', 'perceptive', 'enveloped', 'Shearer', 'fabulously', 'porridge', 'emphatic', 'rarest', 'modernized', 'Tian', 'criticizes', 'aristocracy', 'derailed', 'shards', 'Knees', 'rigor', 'infiltrated', 'introspective', 'handkerchief', 'brazen', 'errant', 'Everlasting', 'Tunisian', 'lustrous', 'yesteryear', 'unassuming', 'Fugitive', 'scathing', 'Chesterton', 'Shaggy', 'Emilie', 'superlative', 'uncharted', 'faintly', 'courting', 'Maintains', 'pollute', 'tundra', 'flocking', 'Wahlberg', 'violins', 'Kincaid', 'unspeakable', 'spouting', 'blip', 'moronic', 'thrusts', 'irreverent', 'thrillers', 'patriarchal', 'light-hearted', 'Ganesh', 'refreshingly', 'bask', 'wry', 'Poetic', 'Constantly', 'Addams', 'sickly', 'assuredly', 'Notting', 'robustness', 'flourishes', 'tenacious', 'modernize', 'fumbled', 'incarnations', 'lashing', 'wrenching', 'quotient', 'Stiller', 'flowery', 'bitchy', 'Mandel', 'inescapable', 'Meow', 'unsatisfied', 'shelved', 'boldness', 'autopilot', 'hibernation', 'Amidst', 'astonishingly', 'clear-cut', 'Ferrara', 'assimilated', 'sportsmanship', 'undoing', 'lopsided', 'feel-good', 'remade', 'surfacing', '11-year-old', 'Gilliam', 'hiss', 'ebb', 'four-star', 'Elizabethan', 'Tierney', 'match-up', 'Refreshing', 'meddling', 'laborious', 'institutionalized', 'Coen', 'haute', 'solemnly', 'shivers', 'Jia', 'vertigo', 'one-dimensional', 'impenetrable', 'gulp', 'painstaking', 'kilt', 'eminently', 'Allegiance', 'Earnest', 'abject', 'dazed', 'Cinematic', 'accumulates', 'elicited', 'half-dozen', 'Suspects', 'Skulls', 'outrageously', 'starry', 'embroiled', 'rediscover', 'brainer', 'appetites', 'Tunis', 'Polanski', 'recharged', 'odour', 'interfaith', 'E.T.', 'springboard', 'complicity', 'frenzied', 'lapping', 'uncomfortably', 'so-so', 'synergistic', 'bolder', 'lobbies', 'idiosyncratic', 'sketched', 'fumbles', 'connoisseur', 'overworked', 'dissidents', 'discernible', 'melodrama', 'dwarfs', 'malaise', 'shamelessly', 'overused', 'spontaneity', 'sneaks', 'puns', 'Audiences', 'bygone', 'Maids', 'Colosseum', 'noxious', 'CQ', 'inconceivable', 'infecting', 'trappings', 'Kwan', 'peppered', 'Stripped', 'opts', 'foursome', 'analgesic', 'taunt', 'demented', 'bode', 'single-handedly', 'overdone', 'Landau', 'beatings', 'Sade', 'doomsday', 'preaches', 'bottomless', 'impervious', 'co-wrote', 'Grumpy', 'tattered', 'culled', 'Schindler', 'crme', 'ephemeral', 'Pacino', 'Veggies', 'shrill', 'dimming', 'Arnie', 'extraterrestrial', 'allegory', 'cursory', 'goosebumps', 'thirty-five', 'dogged', 'adoring', 'transvestite', 'hilariously', 'softens', 'imparted', 'amiss', 'Caruso', 'lingered', 'wetsuit', 'Crit', 'quintet', 'instalment', 'moulds', 'prickly', 'multiplex', 'cliches', 'Imperfect', 'faceless', 'engrossing', 'episodic', 'compendium', 'Captures', 'Sober', 'Candid', 'quiver', 'revolutionaries', 'bearable', 'remakes', 'deafening', 'Simultaneously', 'Alcatraz', 'Passions', 'epicenter', 'anarchists', 'self-awareness', 'servitude', 'austere', 'inner-city', 'dismissive', 'Nerds', 'biennial', 'overbearing', 'rip-off', 'caustic', 'tinge', 'unbridled', 'idling', 'frothy', 'mimicking', 'tongue-in-cheek', 'relevancy', 'Jackass', 'eye-opening', 'Believer', 'magnify', 'DreamWorks', 'preamble', 'sordid', 'Arguably', 'dished', 'immorality', 'cobbled', 'Aaliyah', 'brash', 'Renner', 'outweighs', 'passable', 'Magi', 'indelible', 'Thoroughly', 'cocoon', \"'50s\", 'alienating', 'Leaks', 'racy', 'dizzying', 'humanly', 'interwoven', 'Meatballs', 'predictability', 'naught', 'outcast', 'delves', 'lighthearted', 'pinks', 'Salma', 'discord', 'supercharged', 'flopped', 'Chou', 'forgettable', 'buoyant', 'shaggy', 'Eisenberg', 'preconceived', 'harvests', 'well-meaning', 'amalgam', 'trope', 'muddled', 'potholes', 'Gilligan', 'Gyllenhaal', 'unfaithful', 'hilarity', 'scheming', 'gymnast', 'Mirren', 'twinkling', 'henna', 'subjecting', 'ghastly', 'Nair', 'piecing', 'mismatched', 'conspiracies', 'dumber', 'unedited', 'superhuman', 'attuned', 'deceitful', 'trite', 'conjures', 'distasteful', 'transgression', 'thump', 'reprieve', 'Persuasion', 'exuberance', 'awash', 'sporadically', 'soars', 'Wai', 'tabloids', 'Tolstoy', 'undertones', 'Applegate', 'lusty', 'disintegration', 'Newcomer', 'miniscule', 'Catechism', 'graze', 'overstated', 'childlike', 'pique', 'spirals', 'obsessions', 'Bibi', 'irrevocable', 'hushed', 'stubbornly', 'fireball', 'e-mailing', 'waxes', 'Primavera', 'meshes', 'weirdo', 'slurs', 'pokes', 'biopic', 'Guillen', 'Sheds', 'self-centered', 'splendour', 'squashed', 'fetishes', 'demeaning', 'sportsmen', 'Enduring', 'portraiture', 'reruns', 'squandered', 'crass', 'yank', 'Holm', 'Scarface', 'borrows', 'inconsequential', 'Garca', 'Gianni', 'whiplash', 'glitz', 'receding', 'ugliest', 'Munch', 'outnumber', 'penance', 'airtime', 'Martyr', 'Pender', 'educates', 'lifelike', 'categorization', 'icky', 're-release', 'Involving', 'LeBlanc', 'Errol', 'yikes', 'archetypal', 'audacious', 'seedy', 'uninspired', 'Ninety', 'fervently', 'Gangs', 'Duvall', 'shapely', 'abridged', 'personas', 'fancied', 'mystique', 'magnificence', 'faraway', 'lethargic', 'Aragorn', 'artfully', 'elevates', 'visualizing', 'eloquence', 'Bots', 'bundling', 'ambivalent', 'spinoff', 'exemplify', 'titular', 'concealment', 'veracity', 'dingy', 'disjointed', 'Purdy', 'pap', 'freezers', 'gleefully', 'Oliveira', 'absurdly', 'pessimism', 'unjustified', 'dissipated', 'numbing', 'operatic', 'crawls', 'amiable', 'earmarks', 'awakens', 'meow', 'Vibes', 'doings', 'boisterous', 'Pianist', 'Rodrigues', 'outbursts', 'dutifully', 'thinnest', 'Biggie', 'distinguishable', 'close-ups', 'grievous', 'delving', 'underdeveloped', 'lumpy', '2-D', 'stupidest', 'full-bodied', 'showy', 'subculture', 'co-stars', 'caved', 'Merely', 'hard-hitting', 'infuriating', 'Feral', 'A-list', 'quibble', 'helluva', 'inconclusive', 'steers', 'A.S.', 'empathize', 'frenetic', 'unfulfilled', 'anemic', 'scrappy', 'massacres', 'boomer', 'Boasting', 'sanctified', 'chipper', 'overblown', 'wheezing', 'Stains', 'Clarissa', 'suspenseful', 'inexperience', 'saucy', 'pithy', 'unexplored', 'devotes', 'dud', 'tepid', 'tinged', 'Trier', 'tantamount', 'deft', 'handily', 'Softly', 'voice-over', 'immediacy', 'bender', 'naturalistic', 'mercilessly', 'Nia', 'excepting', 'rigors', 'whoop', 'blacked', 'three-hour', 'developmentally', 'imparting', 'Cusack', 'dislocation', 'characterizing', 'banal', 'inimitable', '10-inch', 'overpower', 'adrift', 'wrenches', 'self-righteous', 'reconsideration', 'bores', 'censure', 'stoke', 'chokes', 'depress', 'chasm', 'summery', 'uptight', 'anyplace', 'Equilibrium', 'impatiently', 'casings', 'gluing', 'standup', 'Uwe', 'offends', 'Petri', 'riveted', 'hellish', 'treachery', 'fumbling', 'entertains', 'Disguise', 'Snowball', 'blaring', 'dollop', 'aimlessly', 'canines', 'resolute', 'Kazan', 'ferment', 'lactating', 'visualized', 'crypt', 'Fincher', 'pawns', 'Altogether', 'glories', 'confrontational', 'Paradiso', 'intrigues', 'antithesis', 'Makin', 'parochial', 'heaped', 'Wyman', 'frayed', 'contraption', 'trickery', 'fiend', 'Risky', 'awkwardness', 'enthralling', 'ably', 'Coarse', 'feasting', 'mythic', 'felons', 'fluttering', 'star-studded', 'Dull', 'spot-on', 'masquerade', 'radiates', 'permeates', 'attentions', 'Directing', 'marvels', 'slacker', 'flatly', 'maddening', 'trifle', 'musty', 'Ratner', 'compels', 'lullaby', 'flip-flop', 'quickie', 'subtleties', 'gloriously', 'heavyweights', 'humping', 'ugliness', 'scrutinize', 'Weimar', 'interstitial', 'modus', 'stepmother', 'twister', 'handsomely', 'ravishing', 'Alternately', 'aspires', 'tirade', 're-create', 'schoolers', 'treading', 'clamoring', 'singularly', 'overrides', 'masterfully', 'Slack', 'masquerading', 'makings', 'venomous', 'Crypt', 'vanishes', 'fluidity', 'yawning', 'kid-friendly', 'Eudora', 'Pauly', 'reinvented', 'reworking', 'viciously', 'limping', 'tidings', 'authentically', 'riddles', 'Grenoble', 'jaunt', 'Represents', 'chuckles', 'Coy', 'muttering', 'treads', 'Ranging', 'scarier', 'softest', 'fixated', 'Takashi', 'cognizant', 'get-go', 'chiller', 'conspicuously', 'Musketeers', 'confrontations', 'sizzle', 'loveable', 'purports', 'diabolical', 'reprehensible', 'caricatures', 'liven', 'deflated', 'Escapes', 'land-based', 'blissfully', 'three-year-old', 'Woodard', 'bushels', 'Argentinian', 'Putty', 'loathing', 'truest', 'flailing', 'resuscitation', 'Ayres', 'confessional', 'Breitbart', 'weaned', 'omnipotent', 'cameos', 'imbued', 'heaving', 'laurels', 'chimpanzees', '6-year-old', 'Hornby', 'innuendo', 'culminates', 'Supremes', 'Kunis', 'enchantment', 'machinations', 'Jean-Claude', 'patchy', 'unearth', 'Chanukah', 'macabre', 'groans', 'quivering', 'Breen', 'old-time', 'metaphorically', 'savour', 'co-written', 'clichs', 'wide-angle', 'judicious', 'gourd', 'virulent', 'shambles', 'throes', 'low-budget', 'annals', 'Humorous', 'Photographed', 'unleashes', 'knee-jerk', 'overheated', 'tie-in', 'gaudy', 'awed', 'undying', 'wintry', 'gibberish', 'permeate', 'Reversal', 'Niels', 'Peralta', 'disrupts', 'prefabricated', 'unnerving', 'withered', 'lurks', 'Schaeffer', 'aggressiveness', 'jabs', 'one-shot', 'uninitiated', 'Graceland', 'pantheon', 'far-fetched', 'collie', 'Limbo', 'Andersson', 'averse', 'word-of-mouth', 'Filmmakers', 'patting', 'Zealanders', 'stoner', 'grown-ups', 'Touched', 'tingle', 'toasts', 'Demonstrates', 'secretions', 'well-made', 'margaritas', 'unsung', 'depravity', 'lavishly', 'indignant', 'ashtray', 'appetizing', 'undetermined', 'ergo', 'malleable', 'Swank', 'evoking', 'confounded', 'McCracken', 'Ahhhh', 'impersonation', 'espoused', 'reshaping', 'distaste', 'drowsy', 'Heidegger', 'Sweetest', 'well-developed', 'purport', 'infatuation', 'Assured', 'Ado', 'obtuse', 'flattening', 'otherworldly', 'symbiotic', 'dubbing', 'irreparable', 'inadvertent', 'exasperated', 'McAdams', 'Elvira', 'purists', 'seduces', 'rigidly', 'stifled', 'rerun', 'combustible', 'concocted', 'teenaged', 'inexcusable', 'conduits', 'redeemable', 'underlines', 'coincidences', '8-year-old', 'resentful', 'inextricably', 'gander', 'suffocating', 'intractable', 'Costner', 'Shafer', 'humanist', 'overflows', 'letdown', 'arousing', 'melodramatic', 'plucking', 'subtitled', 'comprehensible', 'DOA', 'Uncertain', 'winded', 'ces', 'Disgusting', 'butchered', 'trippy', 'underwhelming', 'anti-', 'Angelique', 'preemptive', 'mistaking', 'Sy', 'bod', 'unfathomable', 'Leavitt', 'Oscar-winning', 'amicable', 'Reiner', 'slog', 'Heist', 'unremarkable', 'balding', 'science-fiction', 'unadulterated', 'exude', 'implausible', 'matinee', 'ponders', 'Imogen', 'deja', 'Miyazaki', 'intelligible', 'ruse', '90-minute', 'self-image', 'zany', 'Neeson', 'transparently', 'contemplates', 'Lacking', 'Calculated', 'Celebrated', 'Adrien', 'diversions', 'Vonnegut', 'ooze', 'weirdly', 'Ia', 'methodically', 'backdrops', 'gargantuan', 'wide-eyed', 'rsum', 'thunderous', 'burlap', 'ravages', 'dispatching', 'box-office', 'beckons', 'progenitor', 'blockbusters', 'reclaiming', 'Neverland', 'Renoir', 'chatty', 'ruthlessly', 'tots', 'snazzy', 'stylishly', 'personified', 'Hutchins', 'upping', 'monologues', 'wail', 'bonanza', 'Deliverance', 'dynamism', 'flat-out', 'Kool-Aid', 'incendiary', 'gunning', 'punchy', 'stabs', 'downer', 'Bittersweet', 'Caulfield', 'conjured', 'deathly', 'skateboards', 'fast-moving', 'claustrophobic', 'Cherish', 'monotony', 'docile', 'Sterile', 'Translating', 'Kaufmann', 'multi-dimensional', 'empathetic', 'grumbling', 'endgame', 'reparations', 'crazier', 'tux', 'squirm', 'hovers', 'Joo', 'Energetic', 'Exposing', 'Qualities', 'jaw-dropping', 'oddities', 'Sylvie', 'predisposed', 'Goya', 'infantile', 'humanism', 'multi-layered', 'Croc', 'crackle', 'brats', 'aggrieved', 'Thi', 'emblematic', 'cruelly', 'incisive', 'imitations', 'FISHER', 'Sisterhood', 'Explosions', 'magnificently', 'writhing', 'Pinocchio', 'sniping', '2-day', 'precocious', 'mores', 'fun-loving', 'grumble', 'gobble', 'rehash', 'zap', 'barbs', 'hyperbolic', 'Marinated', 'euphemism', 'fancies', 'parables', 'sharpener', 'stupendous', 'Crispin', 'desolation', 'unwieldy', 'Bicentennial', 'unease', 'lazily', 'obscenity', 'sappy', 'Aesop', 'underpinnings', 'facades', 'heaviness', 'conceit', 'Shakespearean', 'refreshes', 'allusions', 'cogent', 'daydreaming', 'arty', 'adrenalin', 'forthright', 'subcontinent', 'Ravel', 'Quirky', 'irritates', 'fatherhood', 'bewildering', 'Sharpie', 'Suggests', 'climactic', 'bumbling', 'Ecclesiastes', 'ripoff', 'co-director', 'thud', '40-year-old', 'revolting', 'gutsy', 'recompense', 'hitman', 'connoisseurs', 'boarders', 'ripening', 'circumstantial', 'candor', 'larceny', 'burly', 'spurts', 'Tsai', 'imperfection', 'Leppard', 'pathos', 'point-to-point', 'costumed', 'cinematographer', 'Milla', 'wane', 'belittle', 'Dazzling', 'microcosm', 'didactic', 'Cleaver', 'Baz', 'gasps', 'disconnection', 'clashing', 'welt', 'afresh', 'gyro', 'headlong', 'poof', 'specter', 'repugnant', 'Jie', 'Blob', 'moat', 'Orc', 'haphazard', 'evasive', 'downing', 'Purgatory', 'coughed', 'inkling', 'permutations', 'ferocity', 'obsessively', '20th-century', 'stoic', 'customarily', 'oozes', 'irrevocably', 'slasher', 'mending', 'copycat', 'McCulloch', 'handiwork', 'seeping', 'Grisham', 'discourages', 'omnibus', 'four-year-old', 'faceoff', 'Busby', 'subtext', 'kickass', 'whirling', 'Shag', 'amok', 'Disappointing', 'Goodall', 'jumble', 'humanistic', 'Mulholland', 'infomercial', 'tripe', 'dredge', 'narcissism', 'vim', 'hard-pressed', 'scooping', 'competently', 'heralds', 'self-made', 'devilish', 'Schrader', 'underdogs', 'slapstick', 'overwritten', 'disoriented', 'delirium', 'honeys', 'togetherness', 'preteens', 'bleu', 'ethnicities', 'Memento', 'Dogma', 'SECRETARY', 'tenet', 'lunacy', 'bluffs', 'Curiously', 'muddle', 'squirts', 'denizens', 'intensifying', 'self-expression', 'Kathie', 'Spacey', 'unflattering', 'elude', 'Woodman', 'oddity', 'spunky', 'pained', 'viewings', 'self-promotion', 'bravado', 'hunky', 'dystopian', 'easygoing', 'impressionable', 'disagreeable', 'splatter', 'A.I.', 'fleshed', 'Clueless', 'provokes', 'lo-fi', 'cloaked', 'patronizing', 'captors', 'evaporates', 'mannerisms', 'terrors', 'Nouvelle', 'faltering', 'Quitting', 'CHARLIE', 'placid', 'tearful', 'ribbing', 'self-destructive', 'recessive', 'good-natured', 'outtakes', 'Latifah', 'wistful', 'womanhood', 'snore', 'Speck', 'S&M', 'candidly', 'wickedly', 'antagonism', 'consigned', 'Aimed', 'easy-going', 'egotistical', 'Consistently', 'cuss', 'surefire', 'nary', 'posthumously', 'sub-par', 'unwrapped', 'devaluation', 'misfits', 'interdependence', 'inflection', 'befall', 'hairdo', 'newness', 'A-Team', 'Myer', 'wimp', 'grisly', 'Cotswolds', 'Raimi', 'family-oriented', 'malnourished', 'Bernal', 'Brosnan', 'ineffectual', 'Quaid', 'Rosenbaum', 'Tyco', 'epilogue', 'dupe', 'made-up', 'deploys', 'assembles', 'Vittorio', 'Bruin', 'underlies', 'ethnographic', 'Tobey', 'storytellers', 'plaguing', 'coulda', 'characteristically', 'Godard', '18th-century', 'caretakers', 'unpretentious', 'Statham', 'capricious', 'oodles', 'Rothman', 'astringent', 'jumbled', 'nightmarish', 'wallow', 'epics', 'workman', 'campy', 'unreachable', 'rote', 'entwined', 'Disturbing', 'ushers', 'fortify', 'crescendo', 'Dubya', 'Zucker', 'time-honored', 'Belongs', 'twirling', 'garde', 'Wittgenstein', 'Chicago-based', 'kitsch', 'facile', 'rancid', 'Gabriele', 'fascinate', 'Dunst', 'Lauper', 'Sleepless', 'above-average', 'Banderas', 'bemused', 'outrun', 'Wim', 'Abandon', 'hoopla', 'Purely', 'schoolboy', 'misogyny', 'Stockwell', 'dampened', 'well-intentioned', 'environmentalism', 'chuckling', 'dank', 'one-liners', 'Obstacles', 'luckiest', 'celibacy', 'partisans', 'chimps', 'Butterworth', 'Gere', 'merrily', 'eavesdropping', 'Puccini', 'Jean-Luc', 'McConaughey', 'strays', 'paradoxically', 'drive-by', 'marveled', 'operandi', 'entranced', 'iota', \"''\", 'Bolero', 'formulaic', 'Sia', 'diction', 'snags', 'introverted', 'travail', 'dissing', 'pageants', 'Dripping', 'Warmth', 'disappoints', 'whimper', 'flinch', 'roars', 'Dey', 'Buttercup', 'run-of-the-mill', 'ambivalence', 'Proves', 'blasphemous', 'underarm', 'frisky', 'monotone', 'kaleidoscope', 'straddle', 'self-inflicted', 'notations', 'arthritic', 'twisty', 'asinine', 'big-name', 'life-size', 'scruffy', 'Blanchett', 'Chaotic', 'iconography', 'islanders', 'deconstruction', 'impartiality', 'war-torn', 'cynic', 'moldy', 'mannered', 'superficially', 'pimps', 'rekindle', 'entre', 'weirder', 'frolic', 'sedate', 'heavy-handed', 'Hunk', 'laudable', 'Herrmann', 'diatribe', 'negated', 'plundered', 'peculiarly', 'portrayals', 'snide', 'panache', 'subservient', 'mull', 'Kilmer', 'trotting', 'unsteady', 'Freudian', 'sorrowful', 'revisionist', 'screenplays', 'Cimarron', 'tropes', 'annoyances', 'resolutely', 'breath-taking', 'implosion', 'injects', 'Ratliff', 'delineate', 'nouvelle', 'Thoughtful', 'Manages', 'soulless', 'freighter', 'crummy', 'thoughtfulness', 'minefield', 'Probes', 'decorum', 'Sascha', 'foreboding', 'saddens', 'suburbia', 'critiquing', 'computer-generated', 'cutesy', 'throwaway', 'affable', 'backwater', 'meander', 'eye-popping', 'coinage', 'Enriched', 'ornamentation', 'schoolhouse', 'degrades', 'disarming', 'Yuen', 'insular', 'lark', 'Astonishing', 'uninhibited', 'scintillating', 'Gooding', 'all-powerful', 'winks', 'cathartic', '33-year-old', 'conspire', 'modicum', 'pointedly', 'Touches', 'defensible', 'flopping', 'oeuvre', 'Epps', 'Spirited', 'Diggs', 'pasty', 'Unsurprisingly', 'unabashed', 'playfulness', 'escapades', 'omniscient', 'watchable', 'Gordy', 'helpfully', 'vastness', 'unmistakably', 'Vile', 'immaculately', 'royals', 'stylings', 'vowing', 'corkscrew', 'Bellini', 'affluence', 'missteps', 'chump', 'Remembers', 'aplenty', 'uncluttered', 'Dumbo', 'frightful', 'Quietly', 'whet', 'damsel', 'Helga', 'unsavory', 'ballpoint', 'unconnected', 'fil', 'languishing', 'caper', 'Danilo', 'lurid', 'Nesbitt', 'embers', 'veered', 'long-lived', 'duds', 'earplugs', 'drug-related', 'propels', 'EEE', 'Scrappy', 'distracts', 'thoroughfare', 'Wasabi', 'well-crafted', 'transcendence', 'glitzy', 'Whaley', 'melancholic', 'annoyingly', 'germinate', 'screenwriting', 'buoys', 'prim', 'mournful', 'ignites', 'threefold', 're-do', 'Chilling', 'heft', 'teetering', 'Paulette', 'majorly', 'nakedness', 'warlord', 'soured', 'after-hours', 'Entirely', 'brainless', 'litmus', 'Tense', 'shrapnel', 'midlife', 'parlance', 'Gerardo', 'infatuated', 'abhorrent', 'underbelly', 'delirious', 'heartening', 'brotherly', 'grooved', 'Derrida', 'tacks', 'Ze', 'Dense', 'Quelle', 'squirming', 'harshness', 'instilling', 'exhilaration', 'Tok', 'Chekhov', 'Tout', 'cleverness', 'midsection', 'tickles', 'Highlighted', 'oomph', 'succumbing', 'caffeinated', 'revives', 'Mindless', 'dead-end', 'pared', 'Compelling', 'Walken', 'riled', 'Remarkably', 'J.R.R.', 'slowness', 'Everyman', 'toothless', 'sociopath', 'self-absorbed', 'sideshow', 'forewarned', 'hand-drawn', 'dissecting', 'eclipses', 'suspecting', 'punchline', 'disconnects', 'mail-order', 'unappealing', 'cacophony', 'conspirators', 'textural', 'big-screen', 'slop', 'Soderbergh', 'Winger', 'villainous', 'characterizations', 'vivacious', 'underwritten', 'Dwarfs', 'redemptive', 'rackets', 'raison', 'Sits', 'withering', 'bogs', 'assailants', 'reeks', 'immaturity', 'honorably', 'Scooby-Doo', 'morphs', 'shallower', 'hooters', 'Parris', 'uglier', 'gorgeously', 'ill-advised', 'Handled', 'Lilo', 'entrapment', 'Levant', 'buoyed', 'camouflaged', 'confining', 'hustler', 'Arquette', 'jovial', 'coming-of-age', 'squint', 'vaudeville', 'four-legged', 'royally', 'Sounding', 'overture', 'far-flung', 'allegorical', 'flabby', 'paperbacks', 'sexiness', 'mettle', 'grouper', 'amateurish', 'distorts', 'foreshadowing', 'savored', 'loony', 'unconcerned', 'alarmingly', 'Unbreakable', 'omits', 'doldrums', 'standouts', 'fragmentary', 'wordy', 'strutting', 'Witty', 'redundancies', 'unabashedly', 'Escaping', 'shriek', 'frustrates', 'Kurosawa', 'Beavis', '45-minute', 'Deuces', 'ditty', 'naturalism', 'ineptitude', 'fascinates', 'wordless', 'aground', 'recreates', 'o.k.', 'myopic', 'verve', 'impersonating', 'pillowcases', 'stasis', 'spliced', 'morsels', 'ame', 'Jiri', 'wince', 'jeweled', 'vaunted', 'theocracy', 'squander', 'quietness', 'rouse', 'self-discovery', 'grimy', 'loopy', 'passe', 'posits', 'harmoniously', 'Seemingly', 'prescient', 'blurs', 'serendipity', 'off-putting', 'knockoff', 'embarrassingly', 'maggots', 'stereotyped', 'arguable', 'milked', 'Screenwriter', 'Closely', 'reconfigured', 'entertainments', 'travelogue', 'Sarandon', 'purr', 'burr', 'triumphantly', 'stupor', 'Spider-man', 'discerned', 'virtuosity', 'Frontal', 'one-night', 'reams', 'regrettably', 'Fiennes', 'pander', 'Antlers', 'creeped', 'unhinged', 'spiffy', 'romanticism', 'sugar-free', 'staid', 'point-and-shoot', 'third-person', 'internalized', 'pent', 'infamy', 'Alternating', 'reinvention', 'hobbled', 'undeserved', 'disillusionment', 'surrenders', 'civics', 'coldly', 'name-calling', 'distancing', 'dabbling', 'ringside', 'mopping', 'expound', 'Babbitt', 'statesmen', 'Campion', 'Argentinean', 'straight-up', 'subplot', 'four-hour', 'clamor', 'Gollum', 'Largely', 'Pinochet', 'post-modern', 'Geddes', 'artistes', 'skids', 'Damme', 'rock-solid', 'well-behaved', 'terrorizing', 'technicality', 'Rho', 'decrepit', 'feature-length', 'cradles', 'Velma', 'Generates', 'uninspiring', 'Prophecies', 'Brecht', 'tinsel', 'pleaser', 'whitewash', 'syrupy', 'flaunting', 'career-best', 'Charly', 'inexorably', 'platitudes', 'sentimentality', 'unturned', 'self-indulgent', 'Boll', 'strangeness', 'stymied', 'drowns', 'glosses', 'Braveheart', 'bucked', '50-year-old', 'silences', 'Consists', 'garish', 'flirts', 'Ignorant', 'unrealized', 'mourns', 'exploitative', 'chafing', 'Argento', 'enveloping', 'mugging', 'reiterates', 'mirth', 'cartoonish', 'patter', 'gimmicky', 'messianic', 'weightless', 'B-12', 'goodly', 'juiced', 'unafraid', 'Dreyfus', 'paucity', 'presumes', 'Undisputed', 'nauseating', 'repelled', 'Jackal', 'cannibal', 'Barbershop', 'indoctrinated', 'payoffs', 'shoestring', 'disturbingly', 'Utterly', 'barbershop', 'spiteful', 'hustling', 'SC2', 'adorns', 'pleasuring', 'weariness', 'zeitgeist', 'story-telling', 'nubile', 'low-grade', 'unfunny', 'Dilbert', 'D.W.', 'infusing', 'interviewees', 'Gryffindor', 'unoriginal', 'Kerrigan', 'well-done', 'deathbed', 'amoral', 'Lise', 'Ambitious', 'redux', 'breathtakingly', 'fussing', 'concoctions', 'plying', 'queasy', 'Snipes', 'Sommers', 'Capra', '2525', 'undercurrent', 'pretence', 'garbled', 'pre-teen', 'uncommonly', 'Middle-earth', 'expository', 'cutthroat', 'Andie', 'creaking', 'larger-than-life', 'in-your-face', 'tics', 'cussing', 'gesturing', 'neglects', 'fizz', 'constricted', 'rumblings', 'uncool', 'charmer', 'splendidly', 'wiggling', 'reptilian', 'reaffirms', 'inequities', 'glib', 'orchestrate', 'undertone', 'sterility', 'snicker', 'H.G.', 'skirmishes', 'Executed', 'retrospectively', 'conned', 'aspired', 'rollicking', 'Cronenberg', 'artiste', 'floundering', 'domineering', 'deep-seated', 'bombastic', 'aimless', 'McMullen', 'celluloid', 'glint', 'mean-spirited', 'savagely', 'glorifying', '4ever', 'Jacobi', 'barf', 'labours', 'locusts', 'merited', 'hos', 'fantasized', 'featherweight', 'long-winded', 'pathetically', 'boatload', 'shifty', 'Maryam', 'generically', 'Trivial', 'spoofing', 'Arty', 'natured', 'inadequately', 'surrealism', 'ambiguities', 'Pogue', 'overcooked', 'bleep', 'unimpressive', 'squeamish', 'screenwriters', 'unrequited', 'Dalrymple', 'aplomb', 'incongruous', 'evaded', 'preachy', 'co-opted', 'crisper', 'Villeneuve', 'cryin', 'Tosca', 'residuals', 'Mulan', 'reversals', 'canter', 'fill-in', 'temperamental', 'shatters', 'self-loathing', 'kooky', 'demeanour', 'flatulence', 'wallowing', 'charmingly', 'surest', 'unexplainable', 'Lacks', 'igloo', 'Tootsie', 'tumult', 'Deserves', 'Dramas', 'Englishmen', 'vulgarity', 'strolls', 'sure-fire', 'insipid', 'brandishing', 'blustery', 'artefact', 'comatose', 'Morph', 'drive-thru', 'drudgery', 'unevenly', 'pretensions', 'school-age', 'parachutes', 'flinging', 'patchouli', 'tortuous', 'straddles', 'unelected', 'McWilliams', 'regurgitation', 'molto', 'mire', 'insufficiently', 'faulted', 'unprovoked', 'cold-blooded', 'Vague', 'winking', 'wizardry', 'Gai', 'hems', 'comebacks', 'misdirected', 'decommissioned', 'overwhelms', 'groggy', 'valiantly', 'platonic', 'conceited', 'motherland', 'interminable', 'hotdog', 'Flatbush', 'dishonor', 'plodding', 'Philbin', 'Intriguing', 'atheistic', 'NC-17', 'Rorschach', 'fictions', 'unseemly', 'Seldom', 'moviegoers', 'intergalactic', 'unflinching', 'coasting', 'groupies', 'revels', 'Utter', 'Hip-hop', 'WEIRD', 'well-worn', 'frighteningly', 'melding', 'unconvincing', 'trifecta', 'elicits', 'pasts', 'escapism', 'frowns', 'Tackles', 'freakish', 'all-American', 'emulates', 'Snuggle', 'Resnick', 'saintly', 'underappreciated', 'leniency', 'Redgrave', 'self-aware', 'socio-political', 'grownups', 'Seagal', 'recklessness', 'Nanette', 'bizarrely', 'regalia', 'touchstone', 'bared', 'Lampoon', 'comically', 'lyricism', 'disintegrating', 'cliched', 'specious', 'zeroes', 'vapid', 'Miramax', 'strangling', 'detracts', 'Patric', 'ascends', 'deteriorates', 'beguiling', 'itinerant', 'boilerplate', 'Malkovich', 'conquers', 'unapologetic', 'wordplay', 'uhhh', 'Bueller', 'sisterhood', 'reverie', 'lumbering', 'putrid', 'unfocused', 'unsatisfying', 'usurp', 'badness', 'screed', 'minty', 'linearity', 'Fatale', 'whacking', 'Exceptionally', 'Firestorm', 'enliven', 'unrepentant', 'ick', 'reenactment', 'aristocrat', 'preschooler', 'worn-out', 'cheapo', 'broadside', 'Infidelity', 'snowballs', 'amusements', 'libretto', 'Spectators', 'Tonto', 'essayist', 'hicks', 'rekindled', 'evacuations', 'dreamlike', 'infuses', 'Believes', 'startle', 'italicized', 'rez', 'Weigel', 'writer/director', 'Jeffs', 'half-hearted', 'Suspend', 'revulsion', 'minutiae', 'tightrope', 'showmanship', 'eccentricity', 'dreadfully', 'jingles', 'jokers', 'vicarious', 'spews', 'Eccentric', 'indomitable', 'kidnappings', 'orchestrating', 'nurtures', 'Contempt', 'squinting', 'Illuminating', 'middling', 'long-suffering', 'pop-culture', 'Moot', 'deadpan', 'suffocate', 'reverent', 'misfit', 'anthropomorphic', 'make-believe', 'retaliatory', 'unsurprising', 'post-9', 'Desperately', 'meanest', 'irresistibly', 'Bewitched', 'long-held', 'treatises', 'spoofs', 'Mobius', 'grungy', 'trifling', 'distill', 'irreconcilable', 'slow-moving', 'pasteurized', 'dicey', 'Beresford', 'coda', 'Lavinia', 'hokey', 'Interacting', 'populating', 'fearlessly', 'despondent', 'dawns', 'thematically', 'insensitivity', 'confection', 'Gellar', 'Vivi', 'underutilized', 'conjuring', 'heart-warming', 'zippy', 'nine-year-old', 'emaciated', 'Iditarod', 'incognito', 'follies', 'Transforms', 'putters', 'gazes', 'gruelling', 'Pabst', 'interludes', 'auteur', 'millisecond', 'marvelously', 'loathsome', 'articulates', 'Thrilling', 'First-time', 'Elysian', 'oddest', 'rapt', 'seductively', 'Dench', 'Swept', 'questing', 'foibles', 'indisputably', 'aristocrats', 'discards', 'Branagh', 'Predictably', 'Sleepers', 'inexorable', 'Cranky', 'WASP', 'spellbinding', 'Jeong', 'unaccountable', 'inventiveness', 'fritters', 'no-frills', 'insufferable', 'pillage', 'eye-opener', 'misogynistic', 'thought-out', 'prepackaged', 'Kinnear', 'Mamet', 'flaccid', 'half-baked', 'expounded', 'deform', 'Theirs', 'anachronistic', 'frailty', 'haywire', 'psychopathic', 'low-tech', 'Achieves', 'solidity', 'torments', 'barbarism', 'antsy', 'sabotaged', 'Amaro', 'unimaginative', 'pyrotechnics', 'unsaid', 'misadventures', 'smorgasbord', 'Gainsbourg', 'disrespected', 'Schiffer', 'hedonistic', 'Shiner', 'Cheech', 'Banger', 'unbearably', 'dour', 'sympathetically', 'crudely', 'recycles', 'artifice', 'restate', 'swooping', 'surrealist', 'heart-wrenching', 'Flawed', 'scrawled', 'thankless', 'old-world', 'relishes', 'transpose', 'Zellweger', 'repackaged', 'doze', 'agitator', 'suffocation', '50-year', 'reassembled', 'unknowable', 'skittish', 'baloney', 'Kaos', 'begrudge', 'rambunctious', 'Harland', 'carnivore', 'edifying', 'SOOOOO', 'genteel', 'obsessive-compulsive', 'unsophisticated', 'jolted', 'Suffers', 'Reggio', 'persecutions', 'bloodbath', 'mikes', 'mucking', 'Moretti', 'Porky', 'startlingly', 'cliques', 'Pryce', 'chins', 'resonances', 'Ararat', 'Shyamalan', 'stilted', 'tried-and-true', 'cookie-cutter', 'Streamlined', 'wispy', 'excruciatingly', 'attics', 'condescension', 'effusion', 'earnestness', 'wonderment', 'Liana', 'cringing', 'wall-to-wall', 'echelons', 'vacuous', 'languid', 'grimace', 'Sparkles', 'sardonic', 'indistinct', 'clumsily', 'flippant', 'professes', 'Dramatically', 'costuming', 'swipes', 'contemptuous', 'shtick', 'mindsets', 'sledgehammer', 'BMWs', 'plucky', 'roller-coaster', 'theatrics', 'schtick', 'smoothes', 'self-styled', 'free-for-all', 'phonograph', 'Petter', 'vexing', 'self-appointed', 'Tweedy', 'despairing', 'loitering', 'half-assed', 'Softener', 'revelatory', 'discordant', 'polemic', 'eludes', 'shamefully', 'nonchalant', 'fallible', 'plotline', 'Grenier', 'stickiness', 'meanders', 'Oscar-nominated', 'Moderately', 'self-preservation', 'commander-in-chief', 'vagueness', 'passivity', 'co-writer', 'Houseboat', 'soiree', 'Turpin', 'grasps', 'ransacked', 'frazzled', 'blithely', 'flurries', 'folksy', 'lulled', 'sleaze', 'stylistically', 'irrepressible', 'Rehearsals', 'Landings', 'self-sacrifice', 'missive', 'commercialism', 'engulfing', 'outshine', 'overstuffed', 'Cagney', 'telemarketers', 'begets', 'amuses', 'Screenwriting', 'girlish', 'castrated', 'Contenders', 'bluster', 'boozy', 'mass-market', 'slow-motion', 'strongman', 'listless', 'Kahlo', 'self-assured', 'improvisations', 'harps', 'dutiful', 'navigates', 'single-minded', 'flog', 'fairy-tale', 'underlay', 'Uplifting', 'stanzas', 'plausibility', 'Day-Lewis', 'eroticism', 'Behan', 'formalism', 'grimly', 'snobbery', 'Rewarding', 'Janey', 'Carnahan', 'all-night', 'squabbling', 'DeVito', 'deceptions', 'Frei', 'sanguine', 'agape', 'washout', 'hemlock', 'Wiser', 'rehashed', 'undeserving', 'frothing', 'tinny', 'lameness', 'Veronique', 'retold', \"O'Fallon\", 'savagery', 'disinterest', 'ostentatious', 'fireballs', 'rapid-fire', 'tooled', 'ridiculousness', 'self-consciousness', 'hmmmmm', 'farcical', 'Majid', 'mulls', 'Maggio', 'wreaked', 'droning', 'afterschool', 'gravitas', 'Arkin', 'canny', 'ethnography', 'catharsis', 'cloying', 'cesspool', 'ruinous', 'skilfully', 'sensationalism', 'Jovovich', 'sameness', 'work-in-progress', 'misogynist', 'deconstruct', 'thirty-three', 'insinuating', 'comic-book', 'self-deprecating', 'suffices', 'purer', 'unsympathetic', 'hallucinogenic', 'inscrutable', 'empress', 'emphasising', 'doting', 'griping', 'Balzac', 'befuddled', 'Exists', 'unmoved', 'gratify', 'unceasing', 'MapQuest', 'Bardem', 'knock-off', 'noblest', 'matron', 'ill-informed', 'qual', 'meekly', 'talkers', 'Leaping', 'stillborn', 'curio', 'prosaic', 'Straightforward', 'milks', 'dopey', 'P.T.', 'expeditious', 'shoot-out', 'daydreams', 'Friel', 'travails', 'tedium', 'salacious', 'rediscovering', 'inclusiveness', 'laughably', 'nadir', 'neophyte', 'untidy', 'Unforgiven', 'one-room', 'Bursting', 'deconstructed', 'Ghandi', 'Punish', 'dodges', 'Hardman', 'great-grandson', 'obscenely', 'unguarded', 'Gantz', 'grayish', 'vainly', 'neater', 'quintessentially', 'Hickenlooper', 'Gosford', 'contemptible', 'Fanboy', 'tangents', 'marquis', 'Senegalese', 'peeved', 'quadrangle', 'parent-child', 'Belushi', 'minor-league', 'sneering', 'veers', 'hallelujah', 'Twinkie', 'extra-large', 'trickster', 'shrugging', 'masterly', 'shallows', 'slivers', 'undisciplined', 'unravels', 'quashed', 'baboon', 'unmotivated', 'splashy', 'Weissman', 'ineffable', 'misconstrued', 'blooper', 'painterly', 'hideously', 'conniving', 'on-camera', 'levity', 'sleight', 'major-league', 'disguising', 'titillating', 'bounties', 'R-rated', 'bloodless', 'Battista', 'hackneyed', 'dulled', 'mind-numbing', 'brainy', 'swooning', 'subplots', 'underscoring', 'Goths', 'falters', 'millennial', 'Flashy', 'dozing', 'Sendak', 'racehorse', 'Fingered', 'consoled', 'overdoses', 'undoubted', 'Arwen', 'Mediocre', 'uppity', 'epitaph', 'forges', 'homespun', 'engorged', 'Legged', 'veering', 'high-octane', 'fogging', 'Tarantula', 'wistfully', 'grouchy', 'Begley', 'anguished', 'cowering', 'Sidewalks', 'Hawn', 'clichd', 'eke', 'frustratingly', 'resurrecting', 'Lyne', 'persuades', 'quicksand', 'instigator', 'furrow', 'unknowing', 'stand-off', 'non-threatening', 'facetious', 'patronising', 'misfire', 'Afghani', 'drawn-out', 'Carlito', 'dramatist', 'Hideo', 'jostling', 'intelligentsia', 'stomps', 'Laced', 'fluidly', 'reputedly', 'Lillard', 'conciliatory', 'apolitical', 'smugly', 'off-screen', 'Buscemi', 'C.I.', 'indulges', 'alchemical', 'bullseye', 'evocation', 'Uneven', 'jammies', 'Keener', 'umpteenth', 'dispassionate', 'bobbed', 'counterculture', 'redefinition', 'matter-of-fact', 'overdoing', 'Bui', 'strenuously', 'life-altering', 'bawdy', 'thumbing', 'solemnity', 'grosses', 'escapade', 'dithering', 'trudge', 'Hossein', 'hell-bent', 'reassures', 'ponderous', 'vivre', 'whole-heartedly', 'absurdities', 'Miike', 'deliberative', 'Spousal', 'everyman', 'humorously', 'Kapur', 'decrying', 'Worthless', 'fidget', 'stockbroker', 'pastiche', 'Gremlins', 'hermetic', 'downbeat', 'poetics', 'whiney', 'compulsively', 'praiseworthy', 'guises', 'mouthpieces', 'decibel', 'loosens', 'snickers', 'droll', 'closed-door', 'hard-won', 'roundhouse', 'ho-hum', 'Crikey', 'overwrought', 'unadorned', 'Dumber', 'Fairlane', 'blue-chip', 'ill-conceived', 'squandering', 'Salton', 'sanctimonious', 'dustbin', 'sharpens', 'boho', 'stinker', 'marveling', 'stripped-down', 'astoundingly', 'Rollerball', 'Greenlight', 'reductive', 'captivates', 'multilayered', 'thievery', 'Elaborate', 'soul-searching', 'second-rate', 'brazenly', 'gutless', 'reminiscence', 'Bola', 'invulnerable', 'Shimizu', 'Goodfellas', 'Spanish-American', 'Broder', 'nerve-wracking', 'Gallic', 'yuppie', 'intentioned', 'Sever', 'Imax', 'mishandled', 'cynics', 'publicists', 'Overly', 'perfunctory', 'avarice', 'marginalization', 'ironies', 'ostensible', 'gunfight', 'fizzle', 'ghoulish', 'Dangerfield', 'hodgepodge', 'unwary', 'tatters', 'swashbuckling', \"'40s\", 'uncommitted', 'fashioning', 'Undone', 'barbers', 'decorates', 'immortals', 'infirmity', 'Luhrmann', 'oppositions', 'Banzai', 'grizzled', 'Creeps', 'costumer', 're-creation', 'Continually', 'writer-director', 'Sacre', 'overburdened', 'ploughing', 'exterminator', 'generalities', 'conflagration', 'tartness', 'Jez', 'ingeniously', 'dispossessed', 'Minkoff', 'Lugosi', 'hysterics', 'slyly', 'reek', 'Exploits', 'kitschy', 'xXx', 'Pollak', 'slathered', 'bewitched', 'sardine', 'disappointingly', 'hook-ups', 'Delirious', 'Metaphors', 'avalanches', 'Halos', 'subtler', 'Appropriately', 'Harmless', 'Nostra', 'Dafoe', 'Menzel', 'bestowing', 'tamer', 'squalor', 'Reminiscent', 'ramshackle', 'disloyal', 'screwy', 'churns', 'Rife', 'nuked', 'betters', 'Genuinely', 'twitchy', 'nastier', 'lioness', 'repetitious', 'anarchic', 'Mocking', 'action-adventure', 'thrall', 'fantasia', 'pretension', 'stooping', 'Darkly', 'Fai', 'averting', 'point-of-view', 'chomp', 'craven', 'mild-mannered', 'self-knowledge', 'befits', 'gut-wrenching', 'ribcage', 'poignancy', 'convolution', 'creaky', 'grandstanding', 'no-holds-barred', 'plumbed', 'Predictable', '100-year', 'pageantry', 'finery', 'ten-year-old', 'gawk', 'weepy', 'skidding', 'Sontag', 'repetitively', 'polemical', 'disorienting', 'neurosis', 'mimicry', 'lightens', 'winsome', 'reheated', 'big-budget', 'livelier', 'heartstrings', 'valedictorian', 'psychedelia', 'rehashing', 'Iles', 'mystifying', 'sociopaths', 'confusions', 'Tomei', 'Insanely', 'reaffirming', 'impostor', 'dullness', 'happenstance', 'Undiscovered', 'scuttled', 'brawn', 'liveliness', 'miscalculation', 'shriveled', 'endear', 'urbane', 'sturdiness', 'pay-off', 'Morgen', 'shorn', 'gambles', 'crystallize', 'Gaunt', 'well-formed', 'downplaying', 'sadism', 'entrancing', 'Chyna', 'Catch-22', 'amusingly', 'Loosely', 'dishonorable', 'acerbic', 'all-male', 'Eisenstein', 'unforced', 'fast-forward', 'astonish', 'swank', 'sulking', 'Patricio', 'energizes', 'broached', 'resuscitate', 'B-movie', 'unashamedly', 'skateboarder', 'impressionistic', 'ravaging', 'opportunism', 'Originality', 'thinness', 'acrid', 'Gaping', 'playthings', 'Featherweight', 'Laissez', 'limerick', 'Lecter', 'Coven', \"'30s\", 'freewheeling', 'Shekhar', 'theorizing', 'humdrum', 'unaccustomed', 'Polson', 'wryly', 'contrivance', 'homogenized', 'three-minute', 'rapturous', 'conspiratorial', 'self-righteousness', 'cross-dressing', 'Sayles', 'lingual', 'tarantula', 'insignificance', 'sprightly', 'Leoni', 'Indecent', 'rumbles', 'beached', 'round-robin', 'overexposed', 'Welty', 'voyeuristic', 'actuary', 'Pascale', 'Drags', 'off-center', 'Maelstrom', 'Besson', 'Powerpuff', 'staggeringly', 'imperious', 'Delhomme', 'two-lane', 'Cosa', 'Shaky', 'succumbs', 'bastions', 'Tezuka', 'disreputable', 'memorialize', 'masochistic', 'self-reflection', 'muting', 'pursuers', 'shimmers', 'loveless', 'Elegantly', 'unhappily', 'lovebirds', 'fatigues', 'jettisoned', 'Ferrera', 'madcap', 'self-important', 'unpleasantness', 'pandemonium', 'Insurrection', 'morose', 'reinvigorated', 'bratty', 'uncoordinated', 'all-stars', 'inoffensive', 'bare-bones', 'Aiello', 'stupider', 'low-cut', 'tawdry', 'signpost', 'pubescent', 'harmlessly', 'ballsy', 'contentedly', 'picture-perfect', 'Jonze', 'epochs', 'melds', 'perilously', 'gob', 'Ice-T', 'hogwash', 'Rugrats', 'envelops', 'well-executed', 'backstabbing', 'achingly', 'DeNiro', 'cop-out', 'dead-on', 'Culkin', 'drumbeat', 'minutely', 'denouement', 'rivaling', 'airless', 'saccharine', 'talk-show', 'P.O.V.', 'afflicts', 'down-home', 'brawny', 'referential', 'Linklater', 'flattens', 'overplayed', 'waster', 'Manhunter', 'Marmite', 'threadbare', 'nihilistic', 'glum', 'interrelationships', 'embellishing', 'humorless', 'uneasily', 'labyrinthine', 'zealously', 'Vulgar', 'Severely', 'fastballs', 'curlers', 'distended', 'Americanized', 'self-indulgence', 'wallop', 'Muniz', 'Bugsy', 'M-16', 'welled', 'peopled', 'subgenre', 'natter', 'escapist', 'theatrically', 'shapeless', 'Farrelly', 'Afterschool', 'telegrams', 'jolts', 'ardently', 'songbird', 'frankness', 'blips', 'adorably', '40-minute', 'heart-felt', 'well-timed', 'living-room', 'hands-off', 'Sturges', 'curmudgeon', 'fiendish', 'idolized', 'romantics', '70-year-old', 'spooks', 'unhurried', 'uncouth', 'I.Q.', 'expos', 'Retard', 'devastatingly', 'American-style', 'Privates', 'fearlessness', 'love-hate', 'lazier', 'tug-of-war', 'subcultures', 'unapologetically', 'wheezy', 'stodgy', 'off-kilter', 'imbue', 'poetically', 'Cardoso', 'eccentricities', 'insubstantial', 'hums', 'low-down', 'Michell', 'imaginatively', 'regimented', 'pint-sized', 'redeems', 'watered-down', 'crooning', 'extravagantly', '2455', 'precipitously', 'heart-breaking', 'Mullan', 'waterlogged', 'mangle', 'Bruckheimer', 'Patchy', 'ennui', 'self-interested', 'befallen', 'secularists', 'mumbles', 'destin', 'Shreve', 'exasperating', 'self-examination', 'acolytes', 'hijinks', 'Runyon', 'brainpower', 'disquieting', 'elapse', 'herrings', 'colorfully', 'interlocked', 'peddled', 'Tinseltown', 'hippest', 'undercuts', 'half-naked', 'provocations', 'end-of-year', 'taxicab', 'terrifically', 'brassy', 'machismo', 'saps', 'perdition', 'vigils', 'overcook', 'hot-button', 'revisionism', 'beaut', 'crisply', 'wisp', 'dystopia', 'mongrel', 'Ichi', 'temerity', 'serenely', 'wanes', 'goofball', 'Danang', 'demographically', 'drug-induced', 'banality', 'Feelgood', 'true-to-life', 'provocatively', 'b.s.', 'yawns', 'stepmom', 'silliest', '49-year-old', 'mumbo', 'parodied', 'irksome', 'large-format', 'Deepa', 'timelessness', 'Stoppard', 'dazzles', 'cutoffs', 'Schwartzman', 'Absorbing', 'Amari', 'toe-to-toe', 'joie', 'shootouts', 'Buckaroo', 'Steers', 'rambles', 'vaporize', 'overstating', 'clink', 'puzzlement', 'snared', 'dramatized', 'ensnared', 'quaking', 'careening', 'afflicting', 'wincing', 'Wimmer', 'Superbly', 'Manas', 'stepdad', 'Sparse', 'pummel', 'horrendously', 'made-for-TV', 'luminary', 'Trainspotting', 'Butthead', 'Baran', 'sourness', 'unwatchable', 'ill-fitting', 'mishmash', 'gamely', 'screwball', 'mummified', 'subjugate', 'wreaks', 'expressiveness', 'eschews', 'newfangled', 'Brash', 'even-handed', 'pleasingly', 'mother-daughter', 'V.S.', 'melange', 'offal', 'crackles', 'accentuating', 'hoary', 'patronized', 'verging', 'Fuhrman', 'restatement', 'jackasses', 'non-starter', 'ill-equipped', 'mind-bending', 'Hailed', 'dehumanizing', 'Affirms', 'overloads', 'exhibitionism', 'blubber', 'laugh-out-loud', 'ballerinas', 'sun-drenched', 'Emerges', 'romanticized', 'Brilliantly', 'woozy', 'passive-aggressive', 'Binks', 'pre-dawn', 'Adrift', 'Helmer', 'Longley', 'Plympton', 'kung-fu', 'foul-mouthed', 'page-turner', 'Zemeckis', 'credulity', 'Stale', 'nanosecond', 'Abomination', 'Payoff', 'well-oiled', 'Bratt', 'SL2', 'soupy', 'helpings', 'deconstructing', 'hustlers', 'maudlin', 'Qualls', 'piquant', 'off-beat', 'SATs', 'Giannini', 'Boogaloo', 'ruthlessness', 'forgivable', 'trots', 'slather', 'syncopated', 'logistically', 'sidekicks', 'Bullwinkle', 'iconoclastic', 'Anchored', 'plummets', 'weasels', 'toss-up', 'bedfellows', 'broiling', 'moralistic', 'Dass', 'enactments', 'hackles', 'ardor', 'irreparably', 'claustrophobia', 'studiously', 'hamstrung', 'Magnifique', 'exigencies', 'huskies', 'perversely', 'fresh-faced', 'howler', 'puerile', 'existentialism', 'Boasts', 'pallid', 'off-the-wall', 'galvanize', 'plod', 'believability', 'horrifically', 'discontented', 'super-sized', 'clunker', 'vex', 'Brisk', 'Liotta', 'preoccupations', 'derisive', 'clean-cut', 'Lilia', 'signposts', 'Twenty-three', 're-imagining', 'lackadaisical', 'creepiest', 'Carr', 'ballplayer', 'quibbles', \"d'etre\", 'plex', 'madmen', 'Dahmer', 'hammy', 'freshening', 'resents', 'quirkiness', 'quixotic', 'nutjob', 'Venezuelans', 'gnat', '65-year-old', 'pokey', 'borderlands', 'Undressed', 'proficiently', 'byways', 'Rifkin', 'intelligibility', 'clanging', 'has-been', 'perpetrating', 'space-based', 'betrayals', 'high-minded', 'bons', 'Stepford', 'backhanded', 'Zany', 'Mildly', 'jugglers', 'depressingly', 'naptime', 'metaphoric', 'action-oriented', 'smutty', 'life-affirming', 'dualistic', 'devilishly', 'suffused', 'agreeably', 'simple-minded', 'Roussillon', 'spiritualism', 'credulous', 'surrealistic', 'stifles', 'unerring', 'typifies', 'Switchblade', 'what-if', 'overshadows', 'obnoxiously', '4W', 'Benjamins', 'personifies', 'roiling', 'insinuation', 'fryers', 'regurgitated', 'anti-Catholic', 'inarticulate', 'boorish', 'Truffaut', 'oversimplification', 'Plimpton', 'leaden', 'redolent', 'sunnier', 'virtuosic', 'Deliciously', 'nail-biting', 'damsels', 'dunce', 'tactfully', 'sodden', 'nosedive', 'consumerist', 'do-over', 'Fiorentino', 'Involves', 'self-consciously', 'affectation', 'preprogrammed', 'unexamined', 'clotted', 'oration', 'reassuringly', 'dexterous', 'Kumble', 'buffeted', 'latrine', 'clumsiness', 'rawness', 'all-ages', 'Tso', 'improbably', 'doppelganger', 'Unfaithful', 'wishy-washy', 'sloppiness', 'otherness', 'propulsive', 'retooling', 'Blimp', 'Homeric', 'Uneasy', 'Gondry', 'upstage', 'unmolested', 'bonehead', 'Clumsy', 'disquiet', 'post-Soviet', 'bestial', 'doofus', 'smallness', 'MacDowell', 'villians', 'imminently', 'Oleander', 'staggers', 'dilutes', 'stammering', 'vies', 'swill', 'Massoud', 'diatribes', 'self-evaluation', 'Charade', 'amble', 'giggly', 'diffuses', 'self-importance', 'crowd-pleasing', 'prissy', 'gaiety', 'Troubling', 'percolating', 'sniffle', 'Rohypnol', 'capitalizes', 'grotesquely', 'child-rearing', 'Collapses', 'roughshod', 'looseness', 'troubadour', 'Inventive', 'Granddad', 'retooled', 'Perdition', 'whodunit', 'exhilarated', 'unpleasantly', 'pitifully', 'Homeboy', 'alacrity', 'leafing', 'Corny', 'provocateur', 'trenchant', 'freshened', 'Boldly', 'materializes', 'spry', 'Hayao', 'lulls', 'shockwaves', 'sainthood', 'well-structured', 'deliriously', 'Tunney', 'florid', 'heart-stopping', 'Conforms', 'binging', 'guilt-free', 'gulps', 'highbrow', 'murder-suicide', 'profundity', 'T&A', 'firebrand', 'Burdette', 'Plutonium', 'split-screen', 'Intermezzo', 'laconic', 'Zaza', 'Cassavetes', 'Wildly', 'partway', 'filmic', 'yearnings', 'Kurupt', 'unobtrusively', 'time-travel', 'lecherous', 'swathe', 'heedless', 'reductionist', 'fencer', 'upstaged', 'impish', 'leavened', 'Lightness', 'verges', 'Dolman', 'irreverence', 'insistently', 'Haneke', 'adroit', 'ungainly', 'dramatization', 'sophomoric', 'good-hearted', 'calcified', 'Campanella', 'Stinks', 'auto-pilot', 'M.I.T.', 'fetishism', 'Brainy', 'Bettany', 'Brutally', 'go-round', 'hallucinatory', 'gestalt', 'Hlne', 'not-too-distant', 'compassionately', 'incarnated', 'presuppose', 'Arteta', 'astronomically', 'recognizably', 'Grating', 'slobbering', 'last-second', 'wide-screen', 'grandly', 'self-referential', 'teary-eyed', 'naturalness', 'Heathers', 'Boni', 'turgid', 'Pompeo', 'transmute', 'witless', 'scriptwriter', 'Foxworthy', 'Shinya', 'spindly', 'perversity', 'drop-dead', 'laughingly', 'flagrantly', 'wedgie', 'Black-and-white', 'Rocawear', 'Meticulously', 'elbowed', 'shuns', 'crematorium', 'sags', 'paean', 'Dreary', 'comeuppance', 'Sandrine', 'trifles', 'Goyer', 'Propelled', 'hang-ups', 'sleep-deprived', 'Sacrifices', 'quizzical', 'reeked', 'retread', 'swaggering', 'unrecoverable', 'elegy', 'byzantine', 'porthole', 'folktales', 'off-the-cuff', 'six-time', 'Lohman', 'entendre', 'run-on', 'ebullient', 'bungling', 'Asquith', 'Extremities', 'hooting', 'unreality', 'mid-section', 'Chomp', 'Wachowski', 'misperception', 'wallflower', 'irk', 'Dogtown', 'reeking', 'savaged', 'coeducational', 'character-driven', 'Barris', 'second-guess', 'creepiness', 'well-thought', 'fetid', 'dramatize', 'Gidget', 'bloodletting', 'Tremors', 'All-in-all', 'scummy', 'britches', 'waltzes', 'camerawork', 'hippopotamus', 'psyches', 'dramatics', 'rumination', 'wonderous', 'dreck', 'downplays', 'Raccoons', 'lobotomy', 'astounds', 'Lovingly', 'top-heavy', 'souffl', 'Wenders', 'well-constructed', 'abhors', 'unspeakably', 'anti-establishment', 'Doorstep', 'unmentioned', 'Ryoko', 'neuroses', 'humanize', 'cadavers', 'seacoast', 'Malle', 'Sugarman', 'paper-thin', 'Richly', 'demigod', 'Devotees', 'airhead', 'cruelties', 'Dormer', 'Heartwarming', 'Friggin', 'Showgirls', 'Aranda', 'Lasker', 'segues', 'Suge', 'flotsam', 'Diop', 'capably', 'bludgeon', 'absurdist', 'desiccated', 'masochism', 'soberly', 'non-believer', 'dramedy', 'unflappable', 'Subversive', 'populates', 'animatronic', 'Zoolander', 'telegraphed', 'inauspicious', 'moodiness', 'Samira', 'earthbound', 'mesmerize', 'incoherence', 'Mazel', 'sillier', 'claptrap', 'Hanna-Barbera', 'back-story', 'Mason-Dixon', 'raindrop', 'Uzumaki', 'undemanding', 'SOLELY', 'Deliberately', 'cavorting', 'Gibney', 'hard-edged', 'flinching', 'sulky', 'indecipherable', 'shape-shifting', 'middle-of-the-road', 'self-satisfied', 'Shamu', 'bowser', 'treacle', 'Unbearable', 'satisfyingly', 'Ving', 'wearisome', 'Horrid', 'cockney', 'bravura', 'Noyce', 'jaundiced', 'nonbelievers', 'glinting', 'satiric', 'first-timer', 'hairpiece', 'repartee', 'single-handed', 'scarily', 'withholds', 'thuggery', 'Manipulative', 'let-down', 'Spader', 'manifestos', 'chicanery', 'profanities', 'Enticing', '21/2', 'Almodovar', 'antidotes', 'histrionics', 'mockumentary', 'show-off', 'serviceability', 'rose-colored', 'pulpy', 'straight-ahead', 'shrewdly', 'untalented', 'unemotional', 'Rodan', 'superficiality', 'Conceptually', 'besotted', 'philandering', 'cleaving', 'Screenwriters', 'disintegrates', '60-second', 'warthog', 'undernourished', 'topnotch', 'frailties', 'ratchets', 'Sunk', 'impudent', 'Drumline', 'egregiously', 'epiphanies', 'feminized', 'flava', 'bogging', 'Zwick', 'prepubescent', 'Admirable', 'elegiac', 'Jaunty', 'indescribably', 'cadences', 'densest', 'weightlessness', 'pizazz', 'willies', 'Depression-era', 'Demme', 'Swims', 'Tends', 'arcana', 'compellingly', 'Greengrass', 'middle-age', 'costars', 'rom-com', 'evanescent', 'inauthentic', 'playwriting', 'Balk', 'knock-offs', 'Sinise', 'Painfully', 'despising', 'sloppily', 'Tadpole', 'Rambles', 'valueless', 'Gauls', 'intriguingly', 'jolting', 'Tarkovsky', 'Saldanha', 'gauzy', 'Marries', 'juxtapose', 'twirls', 'brogue', 'coos', 'bodacious', 'voice-overs', 'enthrall', 'obviousness', 'cheapen', 'slavishly', 'sublimely', 'pessimists', 'buffoons', '28K', 'unlikable', 'Ostensibly', 'lushly', 'frisk', 'huggers', 'Brooms', 'touchy-feely', 'intermingling', 'uselessly', 'undiminished', 'wavers', 'mingles', 'fatalism', 'Burstein', 'awfulness', 'impassive', 'oh-so', 'Topkapi', 'nervy', 'fire-breathing', 'moralizing', 'cut-and-paste', 'flavorless', 'imitative', 'pileup', 'nuttiness', 'atmospherics', 'Balto', 'portent', 'spinoffs', 'survivable', 'elan', 'Kjell', 'sneers', 'atonal', 'weirded', 'trivialize', 'heart-pounding', 'submerging', 'worshipful', 'pranksters', 'broken-down', 'Taymor', 'Leguizamo', 'narcissists', 'tov', 'outselling', 'blacken', 'cold-hearted', 'overripe', 'Real-life', 'comedy-drama', 'recapturing', 'lowbrow', 'stringently', 'Phifer', 'repulse', 'pathologically', 'Weaves', 'laziest', 'ensnare', 'messiness', 'Yvan', 'gamesmanship', 'near-future', 'devolves', 'Dodgy', 'basted', 'slo-mo', 'Trekkie', 'erotically', 'tomfoolery', 'surfeit', 'megalomaniac', 'blab', 'Potemkin', 'Old-fashioned', 'Sweetly', 'Bartleby', 'enjoyably', 'workmanlike', 'exalts', 'solipsism', 'Tambor', 'VeggieTales', 'strictness', 'dampens', 'slugfest', 'resoundingly', 'Crudup', 'movingly', 'harks', 'derails', 'undercurrents', 'Imposter', 'Unspeakable', 'diverges', 'overhearing', 'Shattering', 'maddeningly', 'endearingly', 'patrolman', 'unapproachable', 'self-hatred', 'herbivore', 'tormentor', 'plucks', 'viscerally', 'goulash', 'streetwise', 'monstrously', 'Wonton', 'Stirs', 'salesmanship', 'overlong', 'Brockovich', 'blare', 'chirpy', 'heart-rending', 'pitch-perfect', 'Stuffy', 'fudged', 'special-interest', 'Hoult', 'slights', 'anti-human', 'Tavernier', 'Jelinek', 'blundering', 'cleverest', 'Bedouins', 'fisticuffs', '72-year-old', 'quick-witted', 'mumbo-jumbo', 'degenerating', 'fiendishly', 'Sica', 'mimetic', 'joyless', 'Lanie', 'imprimatur', 'Turturro', 'Dickensian', 'antic', 'Wobbly', 'wretchedness', 'classicism', 'Sorvino', 'Chokes', 'cleverer', 'undistinguished', 'Play-Doh', 'arthouse', 'brio', 'deportment', 'romped', 'Adobo', 'Binoche', 'juxtapositions', 'Bravado', 'dogmatism', 'uproarious', 'three-ring', 'misanthropic', 'prurient', 'peppering', 'Engages', 'Congeniality', 'expressively', 'mesmerised', 'churlish', 'Caddyshack', 'miscast', 'Thandie', 'Smothered', 'Exquisitely', 'misfires', 'cat-and-mouse', 'prattle', 'blood-soaked', 'schlock', 'break-ups', 'Bogs', 'Chimpanzees', 'lampoon', 'blithe', 'Shum', 'gunplay', 'Shankman', 'eccentrics', 'high-spirited', 'Cliffhanger', 'punchlines', 'knucklehead', 'self-congratulatory', 'Naipaul', 'denuded', 'Psychologically', 'fifteen-year-old', 'dabbles', 'transgressive', 'libertine', 'chitchat', 'Ozu', 'sisterly', 'indigestible', 'Characterisation', 'Peppered', 'Carvey', 'sixth-grade', 'clear-eyed', 'hothouse', 'slovenly', 'sellouts', 'Murderous', 'Bickle', 'Sluggish', 'ambling', 'Sydow', 'wizened', 'ditties', 'reverberates', 're-enactments', 'telenovela', 'categorisation', 'Lathan', 'grubbing', 'loneliest', 'Musketeer', 'super-cool', 'thoughtlessly', 'Hugely', 'middles', 'fanatically', 'self-actualization', 'hooliganism', 'hide-and-seek', 'give-and-take', 'seaworthy', 'gobbler', 'boldface', 'errs', 'Swiftly', 'histrionic', 'sickeningly', 'inexpressible', 'callow', 'unlikeable', 'unsentimental', 'husband-and-wife', 'Andys', 'wazoo', 'ageism', 'fuzziness', 'Ziyi', 'Beatnik', 'Leys', 'Bjarne', 'bludgeoning', 'gored', 'Slackers', 'Cynics', 'Ronn', 'Maneuvers', 'blandly', 'moviemaking', 'Amlie', 'mlange', 'stoops', 'verisimilitude', 'lolling', 'Flotsam', 'subliminally', 'mutates', 'windup', 'directionless', 'Pasolini', 'guiltless', 'standbys', 'beheadings', 'embalmed', 'Honor', 'Aan', 'trounce', 'badder', 'catsup', 'low-rent', 'grossest', 'hile', 'third-best', 'adroitly', 'documentarian', 'veiling', 'skims', 'digressions', 'hijacks', 'rueful', 'look-see', 'Gulzar', 'dim-witted', 'Farenheit', 'jokey', 'waddling', '3-year-olds', 'ribald', 'scorcher', 'humanizing', 'Mothman', 'low-life', 'Connoisseurs', 'Baio', 'mope', 'last-place', 'Rohmer', 'jangle', 'death-defying', 'Weirdly', 'tweener', 'uncreative', 'dwindles', 'loquacious', 'naivet', 'blandness', 'dimwitted', 'twentysomething', 'purposeless', 'Neuwirth', 'limpid', 'interweaves', 'DePalma', 'chillingly', 'kibosh', 'skewering', 'hopefulness', 'McDormand', 'Celebi', 'Smug', 'egomaniac', 'unimpeachable', 'lurches', 'unblinking', 'whirls', 'Nakata', 'Donati', 'hit-or-miss', 'Fantasma', 'traffics', 'gorgeousness', 'lamentations', 'lensing', 'sympathizing', 'sanitised', 'dilettante', 'Downbeat', 'mordant', 'self-delusion', 'limply', 'meanderings', 'tit-for-tat', 'Akasha', 'Seamstress', 'Muzak', 'orchestrates', 'Melodrama', 'semi-autobiographical', 'lulling', 'rip-roaring', 'self-hating', 'Huppert', 'Invigorating', 'implodes', 'contrivances', 'world-weary', 'subzero', 'Neatly', 'reenacting', 'malarkey', 'wisecracking', 'bungle', 'artless', 'blacklight', 'full-blooded', 'Michle', 'meatier', 'roughage', 'dullest', 'Poignant', 'Fessenden', 'waif', 'doles', 'cresting', 'Passer', 'Perceptive', 'misfiring', 'Galan', 'Resourceful', 'workaday', 'enthuse', 'Elling', 'Guzmn', 'thirteen-year-old', 'divining', 'stoically', 'one-note', 'torpor', 'Katzenberg', 'bleakness', 'large-screen', 'day-old', 'helmer', 'recapitulation', 'Byatt', 'melancholia', 'shambling', 'soppy', 'lamer', 'Hopelessly', 'guffaw', 'blessedly', 'self-satisfaction', 'slam-dunk', 'Bailly', 'time-wasting', 'Flamboyant', 'inchoate', 'comradeship', 'inhalant', 'Skillful', 'curmudgeonly', 'psychopathy', 'zingers', 'send-up', 'hyphenate', 'enlivens', 'inanity', 'sidesteps', 'page-turning', 'fallibility', 'Kirshner', 'exploitive', 'Babak', 'inescapably', 'girl-on-girl', 'SONNY', 'cluelessness', 'preordained', 'whetted', 'b-ball', 'Astonishingly', 'Fassbinder', 'free-wheeling', 'extravaganzas', 'startles', 'Thornberry', 'homages', 'ouija', 'unexceptional', 'one-trick', 'Fubar', 'unfulfilling', 'Danis', 'warm-blooded', 'preciousness', 'crowd-pleaser', 'dumbed-down', 'methamphetamines', 'Estela', 'tediously', 'rag-tag', 'deflates', 'patois', 'over-reliance', 'tour-de-force', 'Intensely', 'twaddle', 'pretentiousness', 'leanest', 'Meandering', 'dulls', 'murk', 'hooey', 'Groen', 'reel-to-reel', 'warpath', 'Razzie', 'ill-timed', 'fleetingly', 'tongue-tied', 're-working', 'slumming', 'set-piece', 'imitator', 'Bogdanovich', 'improbability', 'Top-notch', 'hourlong', 'Collinwood', 'Choppy', 'yammering', 'Rhames', 'self-absorption', 'nonconformity', 'desultory', 'Orlean', 'refracting', 'accomodates', 'chides', 'waltzed', 'slow-paced', 'Tautou', '12-Step', 'unmentionable', 'Liman', 'wades', 'G-rated', 'inelegant', 'hankies', 'near-fatal', 'bona-fide', 'ladles', 'Idiotic', 'junctures', 'dominatrixes', 'fizzles', 'amnesiac', 'self-critical', 'half-a-dozen', 'Dalloway', 'ambitiously', 'shadings', 'Bledel', 'ill-considered', 'Caviezel', 'daringly', 'pomposity', 'playas', 'heartbreakingly', 'art-house', 'recoiling', 'Scarcely', 'Marxian', 'Arthouse', 'fitfully', 'hamming', 'bad-boy', 'turfs', 'goofiness', 'rollerball', 'extols', 'self-aggrandizing', 'dreamscape', 'Flavorful', 'open-mouthed', 'Pretentious', 'unrelentingly', 'coarseness', 'titillation', 'Hilariously', 'Unofficially', 'engagingly', 'Hemmingway', 'geeked', 'mournfully', 'movie-making', 'direct-to-video', 'jags', 'illogic', 'entertainingly', 'Deserving', 'decorous', 'bang-up', 'Rymer', 'situates', 'lived-in', 'artificiality', 'grandiosity', 'marshaled', 'reawaken', 'cultist', 'deriding', 'crooned', 'burnt-out', 'cash-in', 'Rampling', 'talky', 'respectably', 'glibly', 'LaBute', 'doodled', 'largest-ever', 'languidly', 'half-an-hour', 'irritatingly', 'soporific', 'high-strung', 'rough-hewn', 'third-rate', 'allying', 'lynchings', 'engendering', 'Jagjit', 'enthusiasms', 'Skolnick', 'Mattei', 'Wanders', 'Devoid', 'uproariously', 'Quills', 'catapulting', 'whir', 'Cattaneo', 'standard-issue', 'midlevel', 'Fangoria', 'overindulgence', 'Wanker', 'One-of-a-kind', 'wackiness', 'keenest', 'Verbinski', 'Bisset', 'classism', 'seductiveness', 'interminably', 'jell', 'exuberantly', 'outpaces', 'hot-blooded', 'Karmen', 'invitingly', 'fervid', 'conceits', 'satyr', 'dismember', 'heartrending', 'Lends', 'buffoonery', 'Simplistic', 'mixed-up', 'potshots', 'Derailed', 'molehill', 'gross-out', 'stolid', 'fidgeted', 'numbs', 'Shiri', 'Spall', 'Ribisi', 'disarmingly', 'slogged', 'overplay', 'Turnabout', 'fabulousness', 'high-concept', 'duking', 'tonally', 'hitmen', 'flamboyance', 'lower-class', 'sputters', 'mother/daughter', 'dazzlingly', 'tone-deaf', 'exoticism', 'Indian-American', 'odorous', 'anciently', 'Prancing', 'cheapened', 'laugher', 'Aptly', 'appealingly', 'dawdle', 'outdoes', 'Stylistically', 'Soulless', 'miscalculations', 'Sprecher', 'quietude', 'lawmen', 'knickknacks', 'tendentious', 'amiably', 'lector', 'Giggling', 'trumped-up', 'Sonnenfeld', 'Mary-Louise', 'payola', 'Broca', 'Yimou', 'Bladerunner', 'flesh-and-blood', 'Splashes', 'lip-synching', 'hollowness', 'whooshing', 'Nickleby', 'ingratiating', 'Touch', 'big-hearted', 'Downright', 'fulsome', 'slickest', 'nonconformist', 'Defies', 'gang-raped', 'Navajos', 'gushy', 'virulently', 'honks', 'bailiwick', 'circularity', 'scattershot', 'robotically', 'Almodvar', 'in-jokes', 'non-Jew', 'mind-numbingly', 'sugar-coated', 'idiosyncrasy', 'overproduced', 'bait-and-switch', 'rollerblades', 'blazingly', 'oes', 'wondrously', 'beause', 'basest', 'unsubtle', 'brle', 'clownish', 'Dignified', 'flag-waving', 'nonjudgmental', 'reworks', 'Alternates', 'self-involved', 'affronted', 'schmucks', 'cribbing', 'bone-chilling', 'Buuel', 'repugnance', 'slickly', 'toe-tapping', 'sloughs', 'Vaguely', 'slickness', 'shockers', 'road-trip', 'Slapstick', 'Fast-paced', 'globalizing', 'nail-biter', 'Kiarostami', 'Amini', 'newsreels', 'self-mutilation', 'well-paced', 'vibrance', 'haughtiness', 'Narc', 'overachieving', 'Repulsion', 'Tedious', 'winces', 'actioner', 'blaxploitation', 'Benigni', 'hardscrabble', 'hypnotically', 'devotedly', 'Clearasil', 'shallowly', 'crescendos', 'Succeeds', 'vertiginous', 'revelled', 'crawlies', 'underachiever', 'flails', 'varmints', 'Schweiger', 'mends', 'Impostor', 'expressionistic', 'angst-ridden', 'Illiterate', 'likability', 'wilts', 'navel-gazing', 'statecraft', 'companionable', 'Bluto', 'prostituted', 'ham-fisted', 'gentility', 'raptures', 'huckster', 'Scouse', 'Stainton', 'Reeses', 'hit-and-miss', 'stupefying', 'Manoel', 'Meara', 'horrifyingly', 'showboating', 'oddballs', 'anti-feminist', 'truth-telling', 'Petrovich', 'eighth-grader', 'computer-animated', 'underplayed', 'grade-school', 'Desplat', 'sudsy', 'lucks', 'Piccoli', 'Kafkaesque', 'misconceived', 'minute-by-minute', 'disassociation', 'rainwear', 'scriptwriters', 'lushness', 'telegraphs', 'punny', 'triviality', 'well-produced', 'slapdash', 'Inherently', 'Depicts', 'hard-driving', 'standoffish', 'Khouri', 'unforgettably', 'broaches', 'scatological', 'once-over', 'pointlessness', 'gawky', 'hour-and-a-half', 'melodramas', 'schlepping', 'XFL', 'Scorcese', 'movie-going', 'Silberstein', 'peekaboo', 'somebodies', 'self-analysis', 'same-old', 'disgracefully', 'Plunges', 'war-ravaged', 'Drang', 'inter-racial', 'Droll', 'self-possessed', 'Crammed', 'charmers', 'Jeunet', 'dimness', 'Goddammit', 'vulgarities', 'Coughs', 'pixilated', 'Goldmember', 'full-throated', 'good-naturedly', 'psychodrama', 'irrelevancy', 'reedy', 'foundering', 'Terminally', 'monosyllabic', 'peevish', 'Cliches', 'bonbon', 'valedictory', 'Ludicrous', 'low-brow', 'cribbed', 'Dazzles', 'passably', 'atavistic', 'Gayton', 'near-miss', 'schmaltz', 'bedeviled', 'camouflaging', 'bug-eyed', 'Egoyan', 'inquisitiveness', 'Perabo', 'sleight-of-hand', 'Concubine', 'unromantic', 'soft-core', 'Borscht', 'gussied', 'sadists', 'tearjerker', 'cheesiness', 'chiaroscuro', 'rugrats', 'character-based', 'colonics', 'well-acted', 'bespeaks', 'unburdened', 'E.T', 'intercut', 'defoliation', 'Borchardt', 'simpering', 'Nalin', 'quandaries', 'jokester', 'rancorous', 'Yosuke', 'thrillingly', 'moviegoer', 'Masseur', 'Nijinsky', 'distressingly', 'unfurls', 'Shamelessly', 'well-drawn', 'hard-hearted', 'rediscovers', 'suey', 'lapdance', 'Fulfills', 'Grainy', 'picaresque', 'fixating', 'subtlest', 'Exhilarating', 'all-too-familiar', 'Feeble', 'Slob', 'pandered', 'eye-rolling', 'disorientated', 'hedonist', 'hypocrisies', 're-assess', 'in-joke', 'Thewlis', 'sabotages', 'TUCK', 'open-faced', 'staleness', 'keening', 'misbegotten', \"'53\", 'Chabrol', 'unfree', 'unschooled', 'erects', 'Luca', 'sleekly', 'hokum', 'Methodical', 'doe-eyed', 'Flaunts', 'flounders', 'fandango', 'tumbleweeds', 'electrocute', 'martial-arts', 'Widowmaker', 'Christelle', 'one-star', 'oppressively', 'pre-9', 'Affectionately', 'milquetoast', 'bilked', '4Ever', 'ingenue', 'unmemorable', 'counter-cultural', 'doshas', 'Sirk', 'freak-out', 'Chaykin', 'well-conceived', 'slipshod', 'evenhanded', 'concocts', 'unrewarding', 'Draggin', 'bombards', 'street-smart', 'Well-written', 'unlistenable', 'schmaltzy', 'seamy', 'pablum', 'schizo', 'urbanity', 'Tsukamoto', 'convolutions', 'Maik', 'underdone', 'new/old', 'forgoes', 'engross', 'formalist', 'unstinting', 'vocalized', 'self-pitying', 'Broomsticks', 'graceless', 'moviegoing', 'argot', 'slimed', 'Weighty', 'forcefulness', 'urinates', 'Admirers', 'Philosophically', 'contorting', 'salvos', 'unpersuasive', 'Devos', 'constrictive', 'clamorous', 'louts', 'touchingly', 'stylishness', 'phlegmatic', 'Cloaks', 'mid-seventies', 'money-grubbing', 'silver-haired', 'true-blue', 'drive-ins', 'stultifying', 'shoot-em-up', 'freakshow', 'Nanook', 'moralism', 'filmgoers', 'poo-poo', 'sensationalize', 'scorns', 'eldritch', 'maladjusted', 'Reinforces', 'dumbness', 'lovefest', 'Wendigo', 'daredevils', 'halfhearted', 'misanthropy', 'romanced', 'trivializing', 'swashbuckler', 'lethally', 'unrelieved', 'falseness', 'rouses', 'Imamura', 'good-humored', 'Bille', 'Frailty', 'rehashes', '75-minute', 'low-cal', 'Ruggero', 'xtc', 'botching', 'nit-picky', 'cornball', 'Open-ended', 'cinematically', 'languorous', 'Distinctly', 'half-step', 'late-summer', 'hackery', 'movie-goers', 'neverland', 'self-parody', 'cheesiest', 'intermissions', 'Waterboy', 'Overcomes', 'reimagine', 'solipsistic', 'lampoons', 'dramatizing', 'expedience', 'giddily', 'rekindles', 'bromides', 'flakiness', 'infectiously', 'McTiernan', 'slashers', 'depersonalization', 'sleepwalk', 'unmentionables', 'blown-out', 'sexualization', 'lugubrious', 'deckhand', 'imbecilic', 'Eurotrash', 'Escapism', 'navet', 'glumly', 'agitprop', 'Jarecki', 'Cuaron', 'villainess', 'raunch', 'fine-looking', 'dimwits', 'anomie', 'misty-eyed', 'manically', 'Watchable', 'Shandling', 'moviemakers', 'mawkish', 'druggy', 'well-honed', 'cooly', 'rah-rah', 'sleep-inducing', 'Maudlin', 'rose-tinted', 'Apted', 'blisteringly', 'full-frontal', 'preposterously', 'right-thinking', 'Laconic', 'Abandons', 'jaw-droppingly', 'eclair', 'Unfolds', 'open-hearted', 'cackles', 'TAY', '20th-Century', 'rhapsodic', 'bass-heavy', 'blood-curdling', 'commodified', 'overinflated', 'ricocheting', 'Iranian-American', 'cheapening', 'idiotically', 'child-centered', 'screwed-up', 'dramaturgy', 'persnickety', 'humourless', 'styx', 'servicable', 'Koepp', 'souvlaki', 'sorrowfully', 'front-loaded', 'half-asleep', 'atrociously', 'special-effects', 'infuriatingly', 'Lagaan', 'hipness', 'bargain-basement', 'Observant', 'outrunning', 'cockeyed', 'P.O.W.', 'unnerve', 'frittered', 'subdues', 'Skillfully', 'Raimondi', 'Shrewd', 'derring-do', 'Solondz', 'all-too', 'irredeemably', 'Disappointingly', 'sturdiest', 'Iwai', 'well-told', 'avenges', 'ex-Marine', 'amalgamating', 'lan', 'libidinous', 'Grown-up', 'Snide', 'fiascos', 'good-time', 'croaks', 'Entertains', 'plainness', 'accident-prone', 'gender-bending', 'skin-deep', 'Tykwer', 'Willams', 'contriving', '19th-Century', 'Arliss', 'better-than-average', 'risk-takers', 'Ya-Ya', 'Ouzo', 'plods', 'wending', 'Glib', 'Losin', 'curtsy', 'puffery', 'avuncular', 'get-out', 'fetishistic', 'Schnitzler', 'telegraphing', 'palaver', 'Hunnam', 'moviemaker', 'Anne-Sophie', 'snoozer', 'ditsy', 'Kieslowski', 'nine-tenths', 'recasts', 'fresh-squeezed', 'pseudo-intellectual', 'Good-looking', 'co-writers', 'lika', 'insufferably', 'novelistic', 'soul-stirring', 'ZigZag', 'indulgently', 'obscurities', 'muckraking', '50-something', 'kidlets', 'draggy', 'Ontiveros', 'Aurelie', 'thuds', 'beachcombing', 'Floria', 'outshined', 'strong-minded', 'verite', 'McCoist', 'numbingly', 'clobbering', 'rampantly', 'by-the-numbers', 'Romanek', 'two-fifths', 'well-characterized', 'promisingly', 'botches', 'near-impossible', 'monsterous', 'howlers', 'Impresses', 'disconcertingly', 'Hennings', 'torpid', 'Byler', 'Bolstered', 're-hash', 'retch', 'atypically', 'sociologically', 'skullduggery', 'Writer-director', 'right-on', 'preciously', 'Dissing', 'small-screen', 'Writer/director', 'pummels', 'mystification', 'Videodrome', 'Auteuil', 'Thoughtless', 'Impeccably', 'back-stabbing', 'tear-jerking', 'completist', 'savvier', 'Kilner', 'deviously', 'late-inning', 'Wickedly', 'squanders', 'single-mindedness', 'crudeness', 'literarily', 'Milder', 'by-the-book', 'wafer-thin', 'sleekness', 'wretchedly', 'fetishized', 'complexly', 'MacNaughton', 'rara', 'septuagenarian', 'ayatollah', 'straight-faced', '80-minute', 'trivializes', 'hidebound', 'commiserating', 'B-grade', 'Philibert', 'edge-of-your-seat', 'nonprofessional', 'plumbs', 'Lavishly', 'Krawczyk', 'frights', 'anim', 'Brainless', 'patrolmen', 'insultingly', 'crushingly', 'temptingly', 'bluescreen', 'insouciance', 'fifteen-minute', 'Preposterous', 'Must-see', 'cringe-inducing', 'unendurable', 'pleasurably', 'obliviousness', 'erotics', 'overstimulated', 'Lothario', 'Morlocks', 'careens', 'Crummy', 'Jewison', 'Tomcats', 'black-owned', 'Gheorghiu', 'ineptly', 'Blaxploitation', 'balletic', 'EVERLASTING', 'Precocious', 'scene-stealing', 'Diverting', 'haranguing', 'Wimps', 'Wannabes', 'pratfalls', 'unindicted', 'Reassuring', 'unconventionally', 'Pompous', 'dingoes', 'stupidities', 'overeager', 'doddering', 'Niccol', 'Dowse', 'Piscopo', 'actor/director', 'Adroit', 'pured', 'shoot-outs', 'upends', 'Symbolically', 'Toback', 'affability', 'LaPaglia', 'razzle-dazzle', 'incurably', 'amorality', 'action-comedy', 'icily', 'forbearing', 'disrobed', 'fleshed-out', 'inquisitions', 'sturm', 'kilted', 'venality', 'vainglorious', 'savviest', 'completists', 'soliloquies', 'self-flagellation', 'double-barreled', 'insightfully', 'nincompoop', 'bloodsucker', 'slopped', 'Sandeman', 'turntablism', 'boogaloo', 'tarted', 'super-powers', 'cross-shaped', 'fistfights', 'wide-awake', 'Molested', 'Alagna', 'cheap-looking', 'foot-dragging', 'well-shaped', 'psychoanalytical', 'percolates', 'grandness', 'Predecessors', 'Vardalos', 'copout', 'cloyingly', 'stammers', 'soap-opera', 'double-cross', 'potboiler', 'horrifies', 'personifying', 'Parmentier', 'Crackerjack', 'Avary', 'whippersnappers', 'bone-dry', 'blarney', 'enthronement', 'Ringu', 'good-for-you', 'Breckin', 'knowingness', 'six-packs', 'serious-minded', 'screamingly', 'disquisition', 'dullard', 'Banal', 'half-wit', 'pinheads', 're-creations', 'sanctimony', 'Assayas', 'tap-dancing', 'poky', 'action-filled', 'So-so', 'Wladyslaw', 'scuzzy', 'Treebeard', 'kvetch', 'career-defining', 'ricochets', 'becalmed', 'self-congratulation', 'cockamamie', 'emigre', 'breezily', 'Resembles', 'hard-bitten', 'zags', 'deceivingly', 'Thought-provoking', 'Oscar-worthy', 'single-mindedly', 'lobotomized', 'flashbulbs', 'earthshaking', 'inventively', 'Scoob', 'sophisticates', 'inanities', 'scathingly', 'hoo-ha', 'pretentiously', 'somnolent', 'bling-bling', 'highfalutin', 'love-struck', 'Upsetting', 'must-own', 'clambake', 'Antwone', 'dyspeptic', 'Wiel', 'cross-promotion', 'narratively', 'hairier', 'relentlessness', 'sociopathy', 'blobby', 'self-reflexive', 'sexploitation', 'enervating', 'proctologist', 'perceptively', 'Dogme', 'CHiPs', 'war-weary', 'Consummate', 'CliffsNotes', 'perspicacious', 'Ryosuke', 'neo-noir', 'characterisations', 'punchier', 'implausibility', 'Spectacularly', 'cinephile', 'unremittingly', 'head-banging', 'preciseness', 'bleakly', 'perceptiveness', 'hoofing', 'stomach-churning', 'Westfeldt', 'fly-on-the-wall', 'Hitchcockian', 'Nonsensical', 'glacially', 'smack-dab', 'Koyaanisqatsi', 'scotches', 'Cardellini', 'Morrissette', 'extemporaneously', '25-cent', 'decrepitude', 'Kidnapper', 'ultra-violent', 'outrageousness', 'demonizes', 'super-simple', 'Abagnale', 'Garbus', 'paint-by-numbers', 'teenybopper', 'innuendoes', 'screwups', 'Witless', 'hews', 'drawling', 'movie-star', 'galled', 'Cuarn', 'huggy', 'greatest-hits', 'paunchy', 'modernizes', 'charmless', 'Freundlich', 'Pyromania', 'sweet-natured', 'hanky-panky', 'enervated', 'piffle', 'divertissement', 'nonthreatening', 'bellyaching', 'pop-music', 'unforgivably', 'befuddling', 'spookily', 'Muddled', 'sledgehammers', '3/4th', 'Elfriede', 'pre-WWII', 'Shindler', 'nauseatingly', 'Implicitly', 'fleet-footed', 'amped-up', 'scalds', 'hubristic', 'tick-tock', 'Dadaist', 'chomps', 'goofiest', 'upper-crust', 'Frenetic', 'dilithium', 'Mam', 'grimness', 'star-making', 'lumpen', 'consciousness-raising', 'Disturbingly', 'GoodFellas', 'post-September', 'Girardot', 'decipherable', 'raucously', 'outer-space', 'lisping', 'hardass', 'true-crime', 'force-feed', 'Reeks', 'schlocky', 'spiffing', 'Audiard', 'Galinsky', 'PG-rated', 'off-the-rack', 'substitutable', 'TV-movie', 'Koury', 'slipperiness', 'Rinzler', 'befuddlement', 'headbanger', 'regurgitates', 'cheesier', 'venturesome', 'wittier', 'comedy/drama', 'Borrows', 'Saeko', 'Sustains', 'sterotypes', 'down-and-dirty', 'Buries', 'tuneless', 'docu-drama', 'documentarians', 'One-sided', 'Discursive', 'despairingly', 'evincing', 'fantasti', 'doggedness', 'co-winner', 'Jacquot', 'closed-off', 'oafish', 'Playfully', 'Preaches', 'Staggers', 'outre', 'paeans', 'show-biz', 'video/DVD', 'amiability', 'tear-stained', 'well-edited', 'Chaiken', 'empathizes', 'DeMeo', 'no-budget', 'fish-out-of-water', 'Aloof', 'stagings', 'kegger', 'Graced', 'well-directed', 'Serviceable', 'lacerating', 'moodily', 'assaultive', 'half-formed', 'phoniness', 'Greek-American', 'Eerily', 'incarnates', 'wolfish', 'bone-crushing', 'subtexts', 'hoity-toity', 'gambol', 'Confounding', 'hack-and-slash', 'Thornberrys', 'treacly', 'inter-species', 'fusty', 'crudity', 'boffo', 'Ecks', 'inanely', 'Blethyn', 'Kitschy', 'Jar-Jar', 'incurious', 'plotless', 'spiritless', 'Greenfingers', 'elliptically', 'Anemic', 'Papin', 'hard-sell', 'squareness', 'hobnail', 'junk-food', 'Puritanical', 'Aggravating', 'halfwit', 'WWII-era', 'mood-altering', 'mtier', 'pell-mell', 'manchild', 'choppiness', 'simpleminded', 'unessential', 'Shohei', 'spookiness', 'amours', 'hard-partying', 'comic-strip', 'Punch-Drunk', 'Gloriously', 'sweet-and-sour', 'Etoiles', '50-million', 'hideousness', 'backrooms', 'stubbly', 'larded', 'Tambin', 'Dankworth', 'winningly', 'Berling', 'apptit', 'Confuses', 'Enormously', 'Tartakovsky', 'Saddled', 'straight-to-video', 'self-promoter', 'Molony', 'blighter', 'KOK', 'ensnaring', 'jazzman', 'Lurid', 'dullards', 'Vividly', 'Israeli/Palestinian', 'ankle-deep', 'mishandle', 'warmed-over', 'slogs', 'father-and-son', 'too-long', 'Twohy', 'outr', 'megaplex', 'parsec', 'Engrossing', 'Evokes', 'gut-busting', 'archly', 'unselfconscious', 'funniness', 'lighter-than-air', 'moldering', 'crassly', 'gasbag', 'jacked-up', 'drek', 'mateys', 'Deflated', 'whip-smart', 'odoriferous', 'horror-comedy', 'careworn', 'Borstal', 'Krige', 'Uncommonly', 'hyper-realistic', 'fuddy-duddy', 'Shadyac', 'bang-bang', 'Frustratingly', 'Smoochy', 'Falters', 'Solomonic', 'Flounders', 'fudges', 'boosterism', 'Breillat', 'Trite', 'latently', 'cowrote', 'smarty-pants', 'Hampered', 'Nachtwey', 'harangues', 'Highbrow', 'Unintelligible', 'wind-tunnel', 'RINGING', 'crassness', 'Leontine', 'yawner', 'lovably', 'assuredness', 'not-quite', 's/m', 'dreaminess', 'drang', 'Clayburgh', 'Hard-core', 'sermonizing', 'Kirkegaard', 'bathos', 'phantasms', 'knockabout', 'head-turner', 'tartly', 'sytle', 'white-knuckled', 'consigliere', 'Viveka', 'meddles', 'cutes', 'ponderously', 'Hudlin', 'all-too-human', 'Kouyate', 'Dooby', 'hit-man', 'over-dramatic', 'half-bad', 'mopes', 'K-19', 'pretention', 'Succumbs', 'pabulum', 'Pootie', 'Contradicts', 'geniality', 'non-techies', 'Enthusiastically', 'Schnieder', 'real-live', 'bio-pic', 'white-trash', 'Purposefully', 'leers', 'sad-sack', 'sugar-coating', 'pretty-boy', 'junior-high', 'defuses', 'Writhing', 'wheedling', 'dynamited', 'Contrived', '90-plus', 'Israeli-occupied', 'Accorsi', 'flim-flam', 'codswallop', 'swaggers', 'meat-and-potatoes', 'less-is-more', 'transferral', 'gullets', 'unfilmable', 'be-bop', 'Boisterous', 'grandiloquent', 'Scarpia', 'Igby', 'pulchritude', 'SWEPT', 'fatalist', 'bewilderingly', 'Yasujiro', 'Missteps', 'Szpilman', 'Makhmalbaf', 'frat-boy', 'Abbass', 'masterpeice', 'irrigates', 'Presson', 'Musker', 'wankery', 'bagatelle', 'sorriest', 'Over-the-top', 'Tornatore', 'Open-minded', 'scarifying', 'Headly', 'untuned', 'aimlessness', 'Bedknobs', 'Forages', 'spangle', 'Carlen', 'aggrandizing', 'one-sidedness', 'post-feminist', 'early-on', 'skippable', 'Talky', 'bracingly', 'MacGraw', 'fluxing', 'Crimen', 'tantalizes', 'non-narrative', 'bizzarre', 'Flaccid', 'double-crosses', 'leonine', 'Ratcatcher', 'Well-done', 'unpicked', 'self-regarding', 'heart-tugging', 'sentimentalist', 'rotoscope', 'yuks', 'hyper-real', 'scripters', 'Unfunny', 'super-wealthy', 'transmogrification', 'bullfighters', 'exhilarate', 'flashbulb', 'ill-starred', 'dizzily', 'kitchen-sink', 'Sabara', 'Elizabethans', 'Veers', 'Windtalkers', 'grand-scale', 'gelati', 'debases', 'tinseltown', 'horror/thriller', 'neurotics', 'Passable', 'postapocalyptic', 'Keenly', 'zings', 'Immersing', 'sugarcoated', 'romantic-comedy', 'smart-aleck', 'Herek', 'stress-reducing', 'sinuously', 'Musset', 'Kaige', 'chortles', 'Friday-night', 'pencil-thin', 'Cremaster', 'Forgettable', 'sheerly', 'Prurient', 'exhuming', 'slam-bang', 'Jam-packed', 'raffish', 'blood-drenched', 'Iben', 'pressure-cooker', 'flabbergasting', 'dull-witted', 'Morvern', 'candy-like', 'Attal', 'Gadzooks', 'nailbiter', 'smeary', 'unsatisfactorily', 'Shunji', 'south-of-the-border', 'dolorous', 'overstays', 'chick-flick', 'Earnhart', 'lip-gloss', 'stomach-turning', 'chateaus', 'disposible', 'Tashlin', 'martinet', 'sleaziness', 'nymphette', 'glad-handing', 'Affable', 'Well-made', 'brush-up', 'Disjointed', 'zigs', 'malapropisms', 'Pimental', 'Desplechin', 'Bluer', 'three-to-one', 'Gaghan', 'Transcends', 'Falsehoods', 'all-woman', 'off-handed', 'underplays', 'flatfooted', 'heavy-handedness', 'pyschological', 'sober-minded', 'Rubenesque', 'light-heartedness', 'didacticism', 'SANDLER', 'unsettlingly', 'Tufano', 'unrepentantly', 'intoxicatingly', 'multi-character', 'swashbucklers', 'Well-meaning', 'low-heat', 'even-handedness', 'Csokas', 'Ballhaus', 'non-actors', 'windbags', 'improbabilities', 'non-fan', 'human-scale', 'deliberateness', 'amuse-bouche', 'grittiest', 'hamfisted', 'undramatic', 'Scherfig', 'even-toned', 'Burkinabe', 'hagiographic', 'Laurice', 'sun-splashed', 'self-mocking', 'big-wave', 'Incoherence', 'connect-the-dots', 'kittenish', 'retitle', 'worldly-wise', 'Z-Boys', 'malediction', 'low-wattage', 'parapsychological', 'plot-wise', 'Baader-Meinhof', 'setpieces', 'ultraviolent', 'flimsier', 'coheres', 'Petroni', 'played-out', 'hopped-up', 'uncommercial', 'perkiness', 'Holofcener', 'idealistically', 'teeny-bopper', 'Tatou', 'white-on-black', 'belly-dancing', 'talking-head', 'action-movie', '10th-grade', 'new-agey', 'non-Disney', 'unhidden', 'fourteen-year', 'comedically', 'Formulaic', 'frissons', 'unoriginality', 'Ho-Tep', 'overacted', 'corniest', 'fastidiousness', 'loosey-goosey', 'Leery', 'Gaitskill', 'straight-shooting', 'Ahola', 'excrescence', 'quirkily', 'suavity', 'Cavaradossi', 'ages-old', 'high-wattage', 'stagey', 'Uruk-Hai', 'Blackboards', 'respites', 'dead-eyed', 'creepy-crawly', 'nonagenarian', 'Francophiles', 'dime-store', 'Belinsky', 'captivatingly', 'Jaglom', 'Seigner', 'Bloodwork', 'Decter', 'mass-murdering', 'swoony', 'mythmaking', 'mystery/thriller', 'unimaginatively', 'jettisons', 'lionize', 'thornier', 'salt-of-the-earth', 'Splendidly', 'unblinkingly', 'Kuras', 'spell-casting', 'bigger-than-life', 'tastelessness', 'defecates', 'post-breakup', 'sprinklings', 'congeals', 'Dolgin', 'corniness', 'anthropologically', 'uninvolving', 'sidesplitting', 'leatherbound', 'bedevils', 'ever-escalating', 'Kozmo', 'rockumentary', 'light-footed', 'Verdu', 'substance-free', 'Brimful', 'Christmas-tree', 'Majidi', 'overreaches', 'mirthless', 'jazzes', 'Silberling', 'Muccino', 'gratingly', 'sappiness', 'freak-outs', 'Savoca', 'jostles', 'luridly', 'mean-spiritedness', 'dictums', 'goofily', 'politesse', 'poster-boy', 'Pellington', 'Sidey', 'mythologizing', 'through-line', 'made-for-cable', 'pep-talk', 'Innocuous', 'doltish', 'Oozes', 'tradition-bound', 'sparklingly', 'Brims', 'lip-reading', 'Serry', 'byplay', 'Sexpot', 'Admirably', 'tiresomely', 'co-dependence', 'inexpressive', 'Medem', 'drabness', 'Rabbit-Proof', 'Paymer', 'Disreputable', 'Woefully', 'Vibrantly', 'Mnch', 'Fresnadillo', 'sword-and-sorcery', 'well-detailed', 'fabuleux', 'timewaster', 'Cletis', 'Schepisi', '100-minute', 'Blutarsky', 'u-boat', 'dewy-eyed', 'Befuddled', 'unspool', 'open-endedness', 'bottom-feeder', 'groaner', 'Absurdities', 'swordfights', 'semi-coherent', 'neorealism', 'amateurishly', 'Partway', 'homiletic', 'Marvelously', 'sweet-tempered', 'Suffocated', 'Opportunists', 'Aspires', 'date-night', 'Hundert', 'eroticized', 'pile-ups', 'Jir', 'soft-porn', 'Thulani', 'espite', 'Hatosy', 'Cantet', 'award-worthy', 'prep-school', 'Plodding', 'rootlessness', 'Degenerates', 'boorishness', 'Laggard', 'incinerates', 'Bleibtreu', 'gorefest', 'Oliviera', 'teen-oriented', 'cutting-room', 'fiascoes', 're-invents', 'fictionalize', 'guilt-trip', 'enrapturing', 'age-wise', 'stagy', 'repulsively', 'slummy', 'twenty-some', 'lucratively', 'well-wrought', 'fear-inducing', '800-page', 'self-glorification', 'clunkiness', 'snoozy', 'lethargically', 'Soapdish', 'unengaging', 'well-observed', 'Strident', 'fantasy-adventure', 'star-power', 'zzzzzzzzz', 'shellshock', 'turntablists', 'uncharismatic', 'exhaustingly', 'Bearable', 'Christian-themed', 'stiflingly', 'girl-power', 'Croze', 'under-12', 'wiseacre', 'Gutterball', 'verismo', 'dead-center', 'Rubbo', 'genre-busting', 'crosscuts', 'Besco', 'romancer', 'Flatman', 'TV-style', 'festooning', 'way-cool', 'Oversexed', 'guessable', 'over-blown', 'scorchingly', 'hardhearted', 'walled-off', 'Otar', 'Rusi', 'old-hat', 'triteness', 'top-billed', 'sucker-punch', 'paint-by-number', 'not-so-hot', 'woe-is-me', 'setpiece', 'go-for-broke', 'Breheny', 'Swimfan', 'rhapsodize', 'Pythonesque', 'well-shot', 'beer-soaked', 'laddish', 'laser-beam', 'deep-sixed', 'seesawing', 'fuddled', 'Meeropol', 'Pretension', 'luvvies', 'affectingly', 'Depressingly', 'dry-eyed', 'multi-layers', 'out-sized', 'sunbaked', 'Lazily', 'funnybone', 'undeterminable', 'Schwentke', 'Laughably', 'not-so-small', 'Squandering', 'showiness', 'Inconsequential', 'Festers', 'Zaidan', 'amusedly', 'exhilaratingly', 'crappola', 'profundities', 'naf', 'Made-Up', \"L'Avventura\", 'self-empowering', 'bigotries', 'Callar', 'self-serious', 'over-indulgent', 'McCrudden', 'be-all-end-all', 'boy-meets-girl', 'Freudianism', 'Peploe', 'emptily', 'meanspirited', 'Janszen', 'one-joke', 'flower-power', 'pushiness', 'Fleder', 'well-trod', 'pillages', 'Marivaux', 'Merchant-Ivory', 'sogginess', 'Fairy-tale', 'sketchiest', 'Nolden', 'blank-faced', 'greasiest', 'action/comedy', 'Schweig', 'Ponderous', 'yawp', 'rap-metal', 'skeeved', 'thesps', 'small-budget', 'ragbag', 'slap-happy', 'groaners', 'time-killer', 'Bttner', 'exasperatingly', 'snail-like', 'encumbers', 'B-minus', 'Oedekerk', 'action/thriller', 'TelePrompTer', 'highly-praised', 'self-revealing', 't-tell', 'Knockaround', 'Exudes', 'jived', 'infantilized', 'adrenalized', 'port-of-call', 'cheap-shot', 'Yakusho', 'squaddie', 'rough-around-the-edges', 'Captivates', 'Anspaugh', 'screenful', 'Grabowsky', 'guilty-pleasure', 'exotic-looking', 'coltish', 'Giler', 'you-are-there', 'documentary-like', 'directress', 'heartstring', 'Romijn-Stamos', 'thrown-together', 'Deblois', 'uninflected', 'spates', 'not-so-bright', 'community-college', 'neo-realist', 'writer/director/producer', 'post-adolescent', 'Believability', 'high-adrenaline', 'superheroics', 'Godawful', '4/5ths', 'sermonize', 'so-bad-it', '7th-century', 'spring-break', 'somnambulant', 'costumey', 'camera-work', 'teeth-gnashing', 'slaloming', 'Irwins', 'Excruciatingly', 'Compulsively', 'Traffics', 'Well-intentioned', 'dysfunctionally', 'impetuousness', 'rusted-out', 'romanticization', 'uninventive', 'salaciously', 'ever-watchful', 'Italian-language', 'sentimentalized', 'rat-a-tat', 'prankish', 'pre-credit', 'seesawed', 'seriocomic', 'self-destructiveness', 'dislikable', 'oh-so-important', 'boom-box', 'beseechingly', 'not-so-big', 'sticky-sweet', 'Elicits', 'moppets', 'vengefulness', 'crudities', 'Besotted', 'non-porn', 'side-splittingly', 'Abderrahmane', 'not-so-funny', 'Phocion', 'cliche-ridden', 'breeziness', 'poignance', 'Competently', '125-year', 'nature/nurture', 'restage', 'Schtte', 'mind-bender', 'plot-lines', 'stop-and-start', 'Shainberg', 'gal-pal', 'Exploitative', 'Bart', 'Bolado', 'writer/directors', 'neo-Nazism', 'courageousness', 'rejiggering', 'stop-go', 'well-realized', 'mordantly', 'cornpone', 'amateurishness', 'spycraft', 'strip-mined', 'holiday-season', 'reconceptualize', 'delibrately', 'transfigures', 'rejigger', 'Melds', 'Nohe', 'bow-wow', 'actioners', 'trailer-trash', 'overladen', 'niftiest', 'third-act', 'Payami', 'ugly-looking', 'near-disaster', 'buddy-cop', 'odd-couple', 'Cockettes', 'all-enveloping', 'teen-pop', 'Testud', 'live-wire', 'crossing-over', 'Overwrought', '26-episode', 'Staggeringly', 'Guei', 'overpraised', 'outlandishness', 'self-mutilating', 'heart-breakingly', 'preposterousness', 'Gulpilil', 'laugh-a-minute', 'farcically', 'groan-inducing', 'soullessness', 'soul-killing', 'shoot-em-ups', 'good-bad', 'wish-fulfilling', 'Sodden', 'dead-eye', 'Filipino-Americans', 'howlingly', 'middle-America', 'brusqueness', 'flame-like', 'lumpish', 'Far-fetched', 'backmasking', 'dreadfulness', 'ho-ho-ho', 'Nss', 'haunted-house', 'desecrations', 'Deutchland', 'actorly', 'unentertaining', 'Nervy', 'Euro-trash', 'summer-camp', 'fiftysomething', 'Hashiguchi', 'Intacto', 'Russos', 'silly-looking', 'under-10', 'whimsicality', 'super-serious', 'Rechy', 'Carion', 'cremaster', 'Nonchalantly', '10-course', 'uncompelling', 'neurasthenic', 'Kazmierski', 'Tanovic', '65-minute', 'untidily', 'not-being', 'Ming-liang', 'Joff', 'unforgivingly', 'First-timer', 'coma-like', 'Hirosue', 'well-put-together', 'Sensitively', 'murkiest', 'spoofy', 'Juergensen', 'been-there', 'zero-dimensional', 'Humorless', 'ransacks', 'meet-cute', 'embroils', 'Corcuera', 'Disney-style', 'tissue-thin', 'Clockstoppers', 'Ruh-roh', 'melodramatics', 'Daringly', 'Juwanna', 'all-French', 'bodice-ripper', 'hypermasculine', 'elaborateness', 'director/co-writer', 'Wertmuller', 'middle-brow', '15-cent', 'Ellefsen', 'Snoots', '85-minute', 'filmgoing', 'Disney-fied', 'Fudges', 'Nenette', 'sugar-sweet', 'Powaqqatsi', 'roundelay', 'Limps', 'Gedeck', 'silent-movie', 'bad-luck', 'ear-pleasing', 'psychologizing', 'gooeyness', 'unreligious', 'half-lit', 'Caton-Jones', 'throat-singing', 'party-hearty', 'half-sleep', 'art-directed', 'GUNFIRE', 'Mouglalis', 'fourth-rate', 'shrieky', 'giant-screen', 'Bible-study', 'Spielbergian', 'movie-of-the-week', 'fire-red', 'quick-cut', 'Frankenstein-like', 'Naqoyqatsi', 'long-faced', 'nowheresville', 'miscalculates', 'co-writer/director', 'world-at-large', 'ball-and-chain', 'work-hours', 'rhapsodizes', 'unamusing', 'hard-to-believe', 'semi-improvised', 'Blithely', 'not-great', 'undogmatic', 'sanctimoniousness', 'uni-dimensional', 'Chou-Chou', 'ticket-buyers', 'Louiso', 'eyeball-to-eyeball', 'Serrault', 'Heavy-handed', '37-minute', 'crapulence', 'character-oriented', 'unshapely', 'mortarboards', 'Ferzan', 'Magimel', 'flibbertigibbet', 'creature-feature', 'thriller/horror', 'loopiness', 'Reigen', 'semi-stable', 'dorkier', 'cartoonlike', 'transfixes', 'Ohlinger', 'medium-grade', 'grittily', 'crime-busting', '95-minute', 'irony-free', 'haplessness', 'political-action', 'Deconstructionist', 'Gymkata', 'Wildean', 'Sorimachi', 'Ya-Yas', 'VelJohnson', 'Cacoyannis', 'CHASES', 'shmear', 'ploddingly', 'done-that', 'Liyan', 'shoplifts', 'Steadfastly', 'pentacostal', 'soaper', 'inter-family', 'Compassionately', 'chick-flicks', 'Kosminsky', 'Baby-faced', 'fillm', 'pop-influenced', 'laugh-filled', 'Hjejle', 'Unwieldy', 'truth-in-advertising', 'stiff-upper-lip', 'morning-glory', 'peerlessly', 'Ballesta', 'a-knocking', 'unflaggingly', 'extreme-sports', 'period-piece', 'under-rehearsed', 'Jolting', 'Everlyn', 'kid-pleasing', 'neo-fascism', 'focus-grouped', 'moviehouse', 'narcotized', 'extra-dry', 'wham-bam', 'haphazardness', 'disquietingly', 'Narratively', 'drably', 'culture-clash', 'Pollyana', 'by-the-books', 'food-for-thought', 'stumblings', 'gang-infested', 'hard-eyed', 'shticks', 'unimpressively', 'Kalesniko', 'Spinotti', 'Bogdanich', 'spy-thriller', 'fun-seeking', 'anteing', 'Oscar-caliber', 'perfervid', 'sentimentalizing', 'megaplexes', 'American-Russian', 'Hypnotically', 'squirm-inducing', 'over-familiarity', 'uplifter', 'middle-agers', 'quasi-documentary', 'handsome-looking', 'Shakesperean', 'Ruzowitzky', 'Enjoyably', 'feel-bad', 'Gussied', 'Laugh-out-loud', 'saucer-eyed', 'sex-reassignment', 'Harks', 'mid-to-low', 'Sampi', 'thrift-shop', 'Engagingly', 'Nettelbeck', 'water-born', 'denouements', 'wised-up', 'money-oriented', 'Meyjes', 'Wolodarsky', 'Oscar-winners', 'joylessly', 'Dogwalker', 'baseball-playing', 'spirit-crushing', 'Ozpetek', 'kilt-wearing', 'Crummles', 'critic-proof', 'water-bound', 'common-man', 'powerment', 'stultifyingly', 'Control-Alt-Delete', 'niblet', 'migraine-inducing', 'suspenser', 'siuation', 'sepia-tinted', 'bigger-name', 'patient/doctor', 'actory', 'pussy-ass', 'Peppering', 'rise-and-fall', 'head-trip', 'Sha-Na-Na', 'juiceless', 'claustrophic', 'dog-tag', 'gasp-inducing', 'sub-aquatic', 'cannier', 'indomitability', 'Hanussen', 'updatings', 'stumblebum', 'sob-story', 'Ah-nuld', 'mistaken-identity', 'exporing', 'larky', 'Sissako', 'Decasia', 'out-of-field', 'skid-row', 'goombah', 'PG-13-rated', 'Two-bit', 'beer-fueled', 'Kurys', 'no-bull', 'wincingly', 'hat-in-hand', 'giggle-inducing', 'B-film', 'pin-like', 'campaign-trail', 'frightfest', 'opera-ish', 'hard-to-swallow', 'done-to-death', 'toolbags', 'aboul', 'ivans', 'semi-humorous', 'glass-shattering', 'string-pulling', 'anti-Hollywood', 'Dridi', 'Picpus', 'witlessness', 'loose-jointed', '72-minute', 'laboriousness', 'crowdpleaser', 'pan-American', 'heartwarmingly', 'ineptitudes', 'Horrendously', 'riot-control', 'Qatsi', 'encomia', 'bedevilling', 'decent-enough', 'sweaty-palmed', 'horror/action', 'likably', 'Hungry-Man', '20-car', 'Soliah', 'Famuyiwa', 'poorly-constructed', 'Purports', 'plaintiveness', 'Scherick', 'studio-produced', 'datedness', 'large-frame', 'Satisfyingly', 'Portentous', 'gut-bustingly', 'platinum-blonde', 'bug-eye', 'well-mounted', 'off-puttingly', 'obviation', 'Xiaoshuai', 'fever-pitched', 'body-switching', 'candy-coat', 'car-wreck', 'Compellingly', 'Sillier', 'self-dramatizing', 'rosily', 'manipulativeness', 'Coriat', 'animaton', 'sappier', 'stuffiest', 'Hermocrates', 'field-sized', 'Romoli', 'shapable', 'drung', 'copyof', 'little-remembered', 'Chouraqui', 'fountainheads', 'decades-spanning', 'inconsequentiality', 'Bollywood/Hollywood', 'homo-eroticism', 'hard-to-predict', 'narcotizing', 'schticky', 'Moll', 'Viterelli', 'A.E.W.', 'Execrable', 'Schmaltzy', 'testosterone-charged', '88-minute', 'anti-Darwinian', 'Zhuangzhuang', 'action-fantasy', 'Wishy-washy', 'video-viewing', 'Bielinsky', 'humbuggery', 'pouty-lipped', 'Bleakly', 'Pratfalls', 'ventually', 'shiver-inducing', 'Runteldat', 'near-masterpiece', 'Workmanlike', 'book-on-tape', 'ultra-low-budget', 'Light-years', 'wise-beyond-her-years', 'loosely-connected', 'IHOPs', 'post-camp', 'McKlusky', 're-voiced', 'romance-novel', 'mid-film', 'miscasts', 'Blandly', 'eye-boggling', 'swimfan', '77-minute', 'soaringly', 'period-perfect', 'Chardonne', 'Maelstrm', 'poor-me', 'eye-filling', 'Plods', 'forgettably', 'vidgame', 'CleanFlicks', 'Direct-to-Video', 'dearly-loved', 'self-caricature', 'Blisteringly', 'snazzy-looking', 'quick-buck', 'Benshan', 'shapelessly', 'Allodi', 'whip-crack', 'snore-fest', 'underconfident', 'Dismally', 'girl-woman', 'gut-buster', 'ugly-duckling', 'mind-destroying', 'Iosseliani', 'teendom', 'Vietnamese-born', 'C.H.O.', 'precollegiate', 'manga-like', 'hotdogging', 'gang-member', 'softheaded', '93-minute', 'Goldbacher', 'strung-together', 'out-of-kilter', 'self-infatuated', 'italicizes', 'derivativeness', 'documentary-making', 'nerve-rattling', 'Ns', 'give-a-damn', 'shimmeringly', 'writer-producer-director', 'post-colonialist', '87-minute', 'non-God', 'twinkly-eyed', 'show-tunes', '76-minute', 'beat-the-clock', 'movie-esque', 'Art-house', 'Well-acted', 'self-exploitation', 'good-naturedness', 'the-night', '99-minute', 'Unflinchingly', 'clutchy', 'heart-string', 'overplotted', 'teen-targeted', 'Jaw-droppingly', 'Kalvert', 'Plotless', 'laugh-free', '94-minute', 'nonjudgmentally', 'Hollywood-ized', 'comedy/thriller', 'peace-and-love', 'bohos', 'chopsocky', 'laugther', 'gorefests', 'teeth-clenching', 'Addessi', 'Montias', 'Bergmanesque', 'slam-bam', 'RINGU', 'hypertime', 'all-over-the-map', 'walking-dead', 'Nuttgens', 'Penotti', 'police-procedural', 'writer-actor', 'less-than-thrilling', 'thumpingly', 'greaseballs', 'movie-industry', 'ill-constructed', 'celebrityhood', 'Abysmally', 'baaaaaaaaad', 'scene-chewing', 'future-world', 'sixties-style', 'hole-ridden', \"hors-d'oeuvre\", 'AM-radio', 'Cirulnick', 'Piesiewicz', 'BarberShop'])\n"
     ]
    }
   ],
   "source": [
    "print(v.w2i.keys())\n",
    "# print(v.w2i['of'])\n",
    "# print(v.w2i['.'])\n",
    "# v.i2w[:10]\n",
    "# print(vectors[0])\n",
    "# print(vectors[1])\n",
    "# print(vectors[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20727, 300)\n"
     ]
    }
   ],
   "source": [
    "# print(vectors[2, :])\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC-7mRyYNG9b"
   },
   "source": [
    "#### Exercise: words not in our pre-trained set\n",
    "\n",
    "How many words in the training, dev, and test set are also in your vector set?\n",
    "How many words are not there?\n",
    "\n",
    "Store the words that are not in the word vector set in the set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "K6MA3-wF_X5M",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parka-wrapped', 'splendid-looking', 'too-extreme-for-TV', 'phoney-feeling', 'hack-artist', 'art-conscious', 'Birot', 'off-Hollywood', 'Playboy-mansion', 'docu-Dogma', 'not-quite-suburban', 'Truckzilla', 'too-tepid', 'disease-of', 'techno-tripe', 'bliss-less', 'gender-provoking', 'unfakable', 'not-exactly', 'actress-producer', 'thousand-times', 'semi-throwback', 'Seldahl', 'ANTWONE', 'Apallingly', 'Univac-like', 'Eckstraordinarily', 'crummy-looking', 'Makmalbaf', 'crime-film', 'near-hypnotic', 'pseudo-philosophic', 'talking-animal', 'Bottom-rung', 'smashups', 'war-movie', 'kinetically-charged', 'tough-man', 'cheese-laced', 'Nickelodeon-esque', 'techno-horror', 'wonder-what', 'Co-writer/director', 'grunge-pirate', 'pro-Serb', 'Thing-type', 'doing-it-for', 'pigeonhole-resisting', 'messing-about', 'too-conscientious', 'Waydowntown', 'artnering', 'truck-loving', 'tub-thumpingly', 'corruscating', 'Mushes', 'quasi-original', 'guilt-suffused', 'Splat-Man', 'fluff-ball', 'media-soaked', 'unrecommendable', 'vintage-TV', 'conflict-powered', 'overemphatic', 'stable-full', 'Hopkins/Rock', 'Gator-bashing', 'French-produced', 'best-sustained', 'preachy-keen', 'Schneidermeister', 'raw-nerved', 'run-of-the-filth', 'Gerbosi', 'Live-honed', 'boundary-hopping', 'tolerable-to-adults', 'memory-as-identity', 'Vulakoro', 'Oscar-sweeping', 'Pie-like', 'bottomlessly', 'Interminably', 'Fulford-Wierzbicki', 'big-bug', 'videologue', 'humor-seeking', 'fang-baring', 'cat-and-mouser', 'Glizty', 'Puportedly', 'Djeinaba', 'ultra-loud', 'provocatuers', 'Armenian-Canadian', 'still-contemporary', 'portent-heavy', 'often-funny', 'push-the-limits', 'style-free', 'girl-meets-girl', 'love-jealousy', 'Shagster', 'Auteil', 'consciousness-raiser', 'lame-old', 'hayseeds-vs', 'action/effects', 'Georgian-Israeli', 'pun-laden', '-RRB-', 'headline-fresh', 'cliche-bound', 'tardier', 'Laissez-Passer', 'Dialogue-heavy', 'so-bad-they', 'unconned', 'zinger-filled', 'gay-niche', 'badly-rendered', 'Waters-like', 'message-mongering', 'she-cute', 'sitcom-worthy', 'song-and-dance-man', 'huge-screen', 'monster-in-the', 'kid-movie', 'movie-movie', 'girl-buddy', 'yawn-provoking', 'pre-shooting', 'dungpile', 'long-on-the-shelf', 'stalk-and-slash', 'botch-jobs', 'Debrauwer', 'drippiness', 'direct-to-void', 'chloroform-soaked', 'fustily', 'clung-to', 'genre-curling', 'trouble-in-the-ghetto', 'Digital-video', 'pseudo-educational', 'every-joke-has', 'mini-mod-madness', 'less-than-objective', 'Norrington-directed', 'semimusical', 'unslick', 'police-oriented', 'spy-movie', 'sleep-inducingly', 'mock-Tarantino', 'schlock-filled', 'unplundered', 'late-twenty-somethings', 'life-at-arm', 'Hollywood-predictable', 'community-therapy', 'fast-edit', 'skate/surf', 'beast-within', 'monster/science', 'hit-to-miss', 'Dognini', 'Brit-com', 'queasy-stomached', 'Punitively', 'spy-savvy', 'Marine/legal', 'half-dimensional', 'Dogma-like', 'scary-funny', 'often-mined', 'kiddie-oriented', 'Feardotcom', 'dudsville', 'Bjorkness', 'Watstein', 'young-guns', 'sequel-itis', 'family-film', 'brain-slappingly', 'oh-those-wacky-Brits', 'outgag', 'something-borrowed', 'cold-fish', 'too-frosty', 'pokepie', 'Punch-and-Judy', 'pro-wildlife', 'over-romanticize', 'LePlouff', 'II-Birkenau', 'pseudo-sophisticated', 'based-on-truth', 'wise-cracker', 'neo-Hitchcockianism', 'of-the-week', 'Travil', 'dateflick', 'cable-sports', 'Jean-Claud', 'goose-pimple', 'Idemoto', 'surface-obsession', 'step-printing', 'redneck-versus-blueblood', 'Singer/composer', 'show-don', 'anti-erotic', 'Bond-inspired', 'Heremakono', 'kids-in-peril', 'big-budget/all-star', 'auto-critique', 'romantic/comedy', 'hyper-artificiality', 'fully-written', 'super-dooper-adorability', 'already-shallow', 'travel-agency', 'thinly-conceived', 'accordion/harmonica/banjo', 'collage-form', 'turkey-on-rolls', 'boom-bam', 'Achero', 'slash-and-hack', 'for-fans', 'Otto-Sallies', 'lack-of-attention', 'Piercingly', 'Jaglomized', 'actorish', 'sense-spinning', 'McBeal-style', 'best-foreign-film', 'MIBII', 'marveilleux', 'montied', 'Unspools', 'Munchausen-by-proxy', 'technology-of-the-moment', 'TV-to-movie', 'Phoned-in', 'Stortelling', 'beyond-lame', 'surehanded', 'hit-hungry', 'drenched-in-the', 'green-guts', 'nonethnic', 'Clarke-Williams', 'celeb-strewn', 'revenge-of-the-nerds', 'psychodramatics', 'spook-a-rama', 'sex-in-the-city', 'the-week', 'stomach-knotting', 'ill-wrought', 'Je-Gyu', 'Ryanovich', 'Verete', 'even-flowing', 'comedy-deficient', 'Hellstenius', 'moral-condundrum', 'Clericks', 'uncinematic', 'feardotcom.com', 'consumer-advice', 'teen-gang', 'inside-show-biz', 'independent-community', 'college-spawned', 'blood-splattering', 'truncheoning', 'cutesy-pie', 'digital-effects-heavy', 'yahoo-ing', 'cipherlike', 'slasher-movie', 'lascivious-minded', 'dullingly', 'grade-grubbers', 'overmanipulative', 'snazziness', 'funny/gritty', 'slummer', 'lip-non-synching', 'kid-empowerment', 'and-miss', 'non-reactionary', 'video-shot', 'kiddie-flick', 'Petin', 'goth-vampire', 'so-five-minutes-ago', 'flck', 'lump-in-the-throat', 'nerve-raked', 'water-camera', 'Sychowski', 'trance-noir', 'uberviolence', 'Unambitious', 'spy-action', 'Talancn', 'reel/real', 'Holofcenter', 'bait-and-tackle', 'trend-hoppy', 'all-wise-guys-all-the-time', 'the-blanks', 'stiletto-stomps', 'murder-on-campus', 'disease-of-the-week', 'debuter', 'clunk-on-the-head', 'title-bout', 'bar-scrapping', 'unsalvageability', 'Lynch-like', 'ultra-provincial', 'headbangingly', 'Malfitano-Domingo', 'out-stealth', 'disappearing/reappearing', 'TV-cops', 'animated-movie', 'sock-you-in-the-eye', 'cinema-besotted', 'out-depress', 'semi-amusing', 'foot-age', 'sub-formulaic', 'media-constructed', 'shuck-and-jive', 'Japanimator', 'Altman-esque', '79-minute', 'Wewannour', 'Zishe', 'blade-thin', 'big-fisted', 'cameo-packed', 'creepy-scary', 'Re-Fried', 'Director-writer', 'fuhgeddaboutit', 'mothball-y', 'non-mystery', 'Warmed-over', 'meets-new', 'power-lunchers', 'Audacious-impossible', 'Hawk-style', 'social/economic/urban', 'pathos-filled', 'the-cash', 'surfer-girl', 'alientation', 'surface-effect', 'bio-drama', 'made-for-movie', 'teen-speak', 'dialed-up', 'anti-Kieslowski', 'monkeyfun', 'shock-you-into-laughter', 'majority-oriented', 'near-xenophobic', 'Spy-vs', 'better-focused', 'stunt-hungry', 'as-it', 'Conan-esque', 'old-fashioned-movie', 'gunfest', 'pop-cyber', 'caper-comedy', 'out-to-change-the-world', 'cliche-riddled', 'oft-brilliant', 'again-courage', 'non-Shakespeare', 'self-defeatingly', 'snap-crackle', 'waydowntown', 'stand-up-comedy', 'underdramatized', 'Oprahfication', 'quick-cuts', 'overstylized', 'yarn-spinner', 'estrogen-free', 'lovable-loser', 'at-a-frat-party', 'S1M0NE', 'Slowtime', 'killer-thrillers', 'kids-and-family-oriented', 'none-too-original', 'Ill-considered', 'two-drink-minimum', 'save-the-planet', 'been-told-a', 'Cliff-Notes', 'wide-smiling', 'comic/thriller', 'Director-chef', 'smarter-than-thou', 'roisterous', 'prechewed', 'meets-John', 'B-flick', 'Koshashvili', 'Underachieves', 'feardotcom', '103-minute', 'Well-meant', 'interspliced', 'lovers-on-the-run', 'cyber-horror', 'cop-flick', 'Witch-style', 'damaged-goods', 'derisions', 'docu-makers', 'still-inestimable', 'zombie-land', 'Pie-type', 'triple-espresso', 'autocritique', 'repellantly', 'not-so-Divine', 'less-compelling', 'dolphin-gasm', 'bottom-of-the-bill', 'teen-sleaze', 'spy-on-the-run', 'industrial-model', 'movie-specific', 'Nietzsche-referencing', 'gabbiest', 'drama/character', 'prewarned', 'action-and-popcorn', 'ultra-manipulative', 'fizzability', 'German-Expressionist', '112-minute', 'show-stoppingly', 'cinemantic', 'underrehearsed', 'ultra-cheesy', 'rubber-face', 'give-me-an-Oscar', 'jazz-playing', 'tear-drenched', 'pooper-scoopers', 'faux-contemporary', 'groan-to-guffaw', 'well-lensed', 'as-nasty', 'female-bonding', 'triple-crosses', 'Destinees', 'pee-related', 'Bettany/McDowell', 'techno-sex', 'over-amorous', 'pasta-fagioli', 'bang-the-drum', 'teen-Catholic-movie', 'Weapon-derived', 'so-inept', 'gone-to-pot', 'film-culture', 'stage-trained', 'wind-in-the-hair', 'two-actor', 'not-so-stock', 'ink-and-paint', 'corporate-sports', 'cinema-and-self', 'cadness', 'thinks-it-is', 'well-contructed', 'pseudo-witty', 'Journalistically', 'sports-movie', 'Nebrida', 'propriety-obsessed', 'tries-so-hard-to-be-cool', 'banter-filled', 'gore-free', 'identity-seeking', 'kibbitzes', 'seventy-minute', 'pantomimesque', 'bare-midriff', 'Baca-Asay', 'By-the-numbers', 'Oft-described', 'gender-war', 'ballistic-pyrotechnic', 'rock-n-rolling', 'razor-sided', 'good-deed/bad-deed', 'Sluggishly', 'shoe-loving', 'anti-Harry', 'FearDotCom', 'foul-natured', 'cellophane-pop', 'glacier-paced', 'super-stupid', 'snake-down-the-throat', 'direct-to-video/DVD', 'wifty', 'Steinis', 'too-hot-for-TV', 'egocentricities', 'grace-in-rebellion', 'dirty-joke', 'Minac', 'democracie', 'time-switching', 'unembarrassing', 'Cineasts', 'unfussily', 'genial-rogue', 'flick-knife', 'tech-geeks', 'road-and-buddy', 'Collosum', 'Devolves', 'Self-congratulatory', 'not-quite-dead', 'Potty-mouthed', 'musclefest', 'Talkiness', 'Much-anticipated', 'inhospitability', 'landbound', 'Higuchinsky', 'cat-and-cat', 'Solondzian', 'Whiffle-Ball', 'clich-riddled', 'rape-payback', 'smile-button', 'Globetrotters-Generals', 'life-embracing', 'ennui-hobbled', 'kid-vid', 'Felinni', 'downy-cheeked', 'unemotive', 'skyscraper-trapeze', 'sailboaters', 'kiddie-friendly', 'Rintar', 'underventilated', 'Age-inspired', '102-minute', 'makeup-deep', 'Kafka-inspired', 'unlaughable', 'faux-urban', 'of-a-sequel', 'natural-seeming', 'than-likely', 'anti-date', 'Scene-by-scene', 'time-vaulting', 'mush-hearted', 'likableness', 'spiritual-uplift', 'message-movie', 'Ear-splitting', 'Juliet/West', 'tryingly', 'image-mongering', 'soon-to-be-forgettable', 'teen-exploitation', 'dark-as-pitch', 'pseudo-hip', 'moldy-oldie', 'close-to-solid', 'Rocky-like', 'Asiaphiles', 'finding-herself', 'Sandlerian', 'sitcom-cute', 'curse-free', 'Kosashvili', 'quasi-improvised', 'Chabrolian', 'Stevenon', 'telanovela', 'flakeball', '179-minute', 'male-ridden', 'Hitler-study', 'Phonce', 'self-amused', 'merchandised-to-the-max', 'modern-office', 'aborbing', 'ga-zillionth', 'dictator-madman', 'idoosyncratic', 'ooky-spookies', 'wound-licking', 'splatterfests', 'happily-ever', 'movie-starring', 'farewell-to-innocence', 'Gilliam-esque', 'semi-surrealist', 'like-themed', 'Mullinski', 'I-2-spoofing', 'clich-laden', 'soul-stripping', 'not-at-all-good', 'action-thriller/dark', 'old-movie', 'stadium-seat', 'retro-refitting', 'superficiale', 'racial-issues', 'Channel-style', 'from-television', 'coming-of-age/coming-out', 'stuporously', 'dead-undead', 'code-talk', 'teen-driven', 'affirmational', 'Well-nigh', 'artsploitation', 'epic-horror', 'Stultifyingly', 'e-graveyard', 'sequel-for-the-sake', 'unsuspenseful', 'flatula', 'fun-for-fun', 'not-nearly', 'Wollter', 'dirgelike', 'surfacey', 'beat-charged', 'ego-destroying', 'none-too-funny', 'heart-affecting', 'jump-in-your-seat', 'out-outrage', 'Veret', 'Divertingly', 'audience-abuse', 'drama/action', 'bowel-curdling', 'Hollywood-itis', 'Equlibrium', 'Kubrick-meets-Spielberg', 'Annie-Mary', 'hippie-turned-yuppie', 'revigorates', 'Amoses', 'Kasem-furnished', 'bore-athon', 'achronological', 'Imaxy', 'involvingly', 'ricture', 'scuzbag', 'Efteriades', '129-minute', 'Faultlessly', 'Good-naturedly', 'Bierbichler', 'post-Full', 'Copmovieland', 'too-facile', 'thrill-kill', 'melodrama/character', 'Outer-space', 'post-Saving', 'screeching-metal', 'trash-cinema', 'non-firsthand', 'not-quite-urban', 'unclassifiably', 'Hubac', 'TV-insider', 'cor-blimey-luv-a-duck', 'often-cute', 'Cosby-Seinfeld', 'Short-story', 'Silbersteins', 'crime-land', 'indieflick', 'sillified', 'lesser-praised', 'sitcomishly', 'dumbfoundingly', 'Damon/Bourne', 'Dawdles', 'movie-biz', 'long-dreaded', 'well-meaningness', 'fear-reducing', 'Jae-eun', 'non-Britney', 'Kahlories', 'bio-doc', 'Brother-Man', 'Butterfingered', 'adventues', 'bibbidy-bobbidi-bland', 'prefeminist', 'Kaputschnik', 'cliche-drenched', 'over-25s', 'unhibited', 'pro-fat', 'hotsies', 'shlockmeister', 'reality-snubbing', 'bump-in', 'under-inspired', 'Deadeningly', 'under-7', 'buddy-comedy', 'spaniel-eyed', 'achival', '168-minute', 'wannabe-hip', 'Komediant', 'double-pistoled', 'acting-workshop', 'raunch-fests', 'O2-tank', 'Peter/Spider-Man', 'only-in', 'ever-ruminating', 'strafings', 'Wince-inducing', 'singer-turned', 'Pasach', 'blue-light-special', 'dust-caked', 'hyper-time', 'rough-trade', 'nrelentingly', 'challenge-hungry', 'sub-music', 'war/adventure', 'dog-paddle', 'unencouraging', 'Qutting', 'doofus-on', 'bad-movie', 'two-hour-and-fifteen-minute', 'audience-pleaser', 'oh-so-Hollywood', 'middle-fingered', 'toilet-humor', 'less-than-magic', 'neo-Augustinian', 'kids-cute', 'matinee-style', 'pseudo-bio', 'hidden-agenda', 'superlarge', 'follow-your-dream', 'barn-side', 'gender-bender-baller', 'pop-induced', 'made-for-home-video', 'materalism', 'un-bear-able', 'often-deadly', 'qatsi', 'crass-a-thons', 'Star/producer', 'Margolo', 'therapy-dependent', 'Banderas-Lucy', '91-minute', 'snow-and-stuntwork', 'Poke-mania', 'indie-heads', 'laser-projected', 'in-the-ring', 'Driver-esque', 'eroti-comedy', 'brain-deadening', 'hell-jaunt', 'nonchallenging', 'non-Bondish', 'techno-saturation', 'still-raw', 'Priggish', 'girls-behaving-badly', '-LRB-', 'too-spectacular', 'demented-funny', 'warm-milk', 'handbag-clutching', 'skit-com', 'Well-shot', 'Formuliac', 'European-set', 'star-splashed', 'I-heard-a-joke', 'overly-familiar', 'Less-than-compelling', 'pseudo-serious', 'opera-to-film', 'Insufferably', 'wild-and-woolly', 'untugged', 'college-friends', 'quasi-Shakespearean', 'hastier', 'stagecrafts', 'pre-fils', 'Brazil-like', 'Janklowicz-Mann', 'Ourside', 'pro-Serbian', 'affectation-free', 'talk-heavy', 'Reeboir', 'voices-from-the-other-side', 'eardrum-dicing', 'hammily', 'RunTelDat', 'Choquart', 'Old-form', 'wife/colleague', 'overcoming-obstacles', 'Denlopp', 'heartbeat-like', 'barking-mad', 'Brothers/Abrahams', 'crash-and-bash', 'heart-on-its-sleeve', 'diciness', 'white-empowered', 'Bolly-Holly', 'sub-sophomoric', 'actorliness', 'difficult-to-swallow', '170-minute', 'out-bad-act', 'sex-as-war', 'who-wrote-Shakespeare', 'barn-burningly', 'post-Tarantino', 'junk-calorie', 'strainingly', 'Easter-egg-colored', 'Bazadona', 'valley-girl', 'remain-nameless', 'two-wrongs-make-a-right', 'Jeong-Hyang', 'Bruckheimeresque', 'elegiacally', 'screen-eating', 'sub-Tarantino', 'luv-spreading', 'Marcken', '24-and-unders', 'character-who-shall', 'often-hilarious', 'no-surprise', 'Tiresomely', 'hour-and-a-half-long', 'East-vs', 'Have-yourself-a-happy-little-Holocaust', 'non-exploitive', 'B-scene', 'the-loose', 'Romething', 'time-it-is', 'in-jokey', 'groupie/scholar', 'food-spittingly', 'Wisegirls', 'Company-style', 'kill-by-numbers', 'just-above-average', 'video-game-based', 'buzz-obsessed', 'Mad-libs', 'anti-adult', 'slash-dash', 'Ga', 'street-realist', 'a-bornin', 'Brothers-style', 'not-very-funny', 'renegade-cop', 'beloved-major', 'Laissez-passer', 'daytime-drama', 'imponderably', 'clone-gag', 'Recoing', 'thriller-noir', 'Rashomon-for-dipsticks', 'Lifetime-channel', 'guy-in-a-dress', 'Frankenstein-monster', 'pulpiness', 'sense-of-humour', 'slice-of-depression', 'pseudo-rock-video', 'Cool-J', 'mix-and', 'disaffected-indie-film', 'Ki-Deok', 'Shayamalan', 'slash-fest', 'twist-and-turn', 'Oscar-size', 'Live-style', 'socio-histo-political', 'Euro-film', 'hotter-two-years-ago', 'hyper-cliched', 'church-wary', 'self-glorified', 'Venice/Venice', 'out-shock', 'Outrageousness', 'high-buffed', 'Gantzes', 'gangster/crime', 'heart-rate-raising', 'Volletta', 'Mordantly', 'lower-wit', 'Passably', 'Thekids', 'now-cliched', 'Hollywood-action', 'whoopee-cushion', 'Seldhal', 'sex-soaked', 'Indifferently', 'cheatfully', 'deadeningly', 'flex-a-thon', 'tuba-playing', 'gut-clutching', 'uncharismatically', 'Hjelje', 'pyro-correctly', 'special-effects-laden', 'gone-to-seed', 'video-cam'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_not_found = set()\n",
    "\n",
    "for data_set in [train_data, test_data, dev_data]:\n",
    "    \n",
    "    for ex in data_set:\n",
    "            for token in ex.tokens:\n",
    "                \n",
    "                # check if token is not in the vocab\n",
    "                if not token in v.w2i:\n",
    "                    words_not_found.add(token)\n",
    "            \n",
    "            \n",
    "           \n",
    "print(words_not_found)\n",
    "len(words_not_found)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfEd38W0NnAI"
   },
   "source": [
    "#### Exercise: train Deep CBOW with (fixed) pre-trained embeddings\n",
    "\n",
    "Now train Deep CBOW again using the pre-trained word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "z_6ooqgEsB20"
   },
   "outputs": [],
   "source": [
    "class PTDeepCBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab, embedding_w):\n",
    "\n",
    "#         super(PTDeepCBOW, self).__init__(vocab_size, embedding_dim, hidden_dim, output_dim, vocab, embedding_w)\n",
    "        super(PTDeepCBOW, self).__init__()\n",
    "        self.vocab = vocab\n",
    "    \n",
    "        embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        embedding_layer.weights = embedding_w\n",
    "        embedding_layer.weight.requires_grad = False\n",
    "        \n",
    "        hidden_units = hidden_dim\n",
    "        \n",
    "# # disable training the pre-trained embeddings\n",
    "# pt_deep_cbow_model.embed.weight.requires_grad = False\n",
    "\n",
    "        \n",
    "        # this is a trainable look-up table with word embeddings\n",
    "        layers = []\n",
    "        layers.append(embedding_layer)\n",
    "        \n",
    "\n",
    "\n",
    "        layers.append(nn.Linear(embedding_dim,hidden_units))\n",
    "        layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_units,hidden_units))\n",
    "        layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_units,5))\n",
    "        \n",
    "        self.layers=nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # this is the forward pass of the neural network\n",
    "        # it applies a function to the input and returns the output\n",
    "\n",
    "        # this looks up the embeddings for each word ID in inputs\n",
    "        # the result is a sequence of word embeddings\n",
    "        \n",
    "        # word >>> word 2 i \n",
    "        #index >> embeddings\n",
    "        \n",
    "        logits = self.layers(inputs)\n",
    "        logits = logits.sum(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "JfIh4Ni6yuAh",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data\n",
      "Iter 1000: loss=1783.2667, time=2.46s\n",
      "tensor([[   62,    21,     8,  1895,   606,    20,  1895,  3939,    25,   788,\n",
      "             5, 19497,     3]], device='cuda:0')\n",
      "tensor([[  227,    51,   774, 18997,   128,     2,    67,    11,   495,    12,\n",
      "             4,   163,     3]], device='cuda:0')\n",
      "tensor([[ 124,   68,   19,  165,   36, 1312, 1345,    6, 3992,   25,    8,  657,\n",
      "            7, 3099,    2,   19,  171,  207, 1717,  311,    9,   35, 8097,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  77, 1693,    2, 1346,    2, 4826,  606,    3]], device='cuda:0')\n",
      "tensor([[ 7256,  3751,  4255,     5,  4160,    98,   687,  1206,     6,  4652,\n",
      "           595,  3278,     2,  9115, 11439,     2,     4,  4305,     7,  4896,\n",
      "             5,  2150,  3071,     3]], device='cuda:0')\n",
      "tensor([[ 4061, 12118,  7973,     2,  4061,  4815,  2097,     2,   190,     9,\n",
      "            51, 13672,  7699,   606]], device='cuda:0')\n",
      "tensor([[18832,    25,  1169,   108,     2,  6518,   285,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[18510,    20,   102,     0,  1476,     2,   102,  2351, 18659,     0,\n",
      "            67,   243,  5741,     6,  1476,     5,   414,     0,     5,  3773,\n",
      "         19365,    21,   163,  5893,     9,   137,     3]], device='cuda:0')\n",
      "tensor([[13638,  8959,     2, 15884, 12236,     5,  4508,  6132,     2,    22,\n",
      "           719,   136,    18,     8, 16056,  1652,    30,  8690,     6,   330,\n",
      "           242,   148,     8,  5607,     7,    16,  2681,     0, 15474,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 2806,    21,    26,  5829,     2,    73,     8, 15040, 19061,    19,\n",
      "            42,  3566,     8,  3019,   296,   158,   186,     2,     4,  8436,\n",
      "          5848,  6179,    11,  1202,     7,   447,     3]], device='cuda:0')\n",
      "tensor([[ 4742,     4,  8449, 20389,  1022,     7,   118,  2436,  5472,     2,\n",
      "          1138,   549,   158,     5,    50,  4511,     2,  1138,  9472,   158,\n",
      "            11,    26,     4,   633,     7,  2144,    12,  8951,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   91,   197, 10576, 11723,     5,  3021, 10553,     5,  1973,     2,\n",
      "          1331,     4,  9484,     7,     8,  5568,   544,  9931, 14334,  6796,\n",
      "             9,     8,  4734,  1896,     2,  5131,    40, 13720,  5014,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   23,  1037,    21,  5274,     9,     4,   567,     7,  1333, 11044,\n",
      "            11,  5187,     2,   639,    12,  5174, 12460,     0,    28,    51,\n",
      "           867,     0,     3]], device='cuda:0')\n",
      "tensor([[ 1316,  1689,  2582,   149,    36,   233,  5416,     6,   539,    80,\n",
      "          2359,     4,   198,    21,   163,  5693,     2,  6601,  5385,   117,\n",
      "           107, 20204,     8,   841,  3182,    12,  4643,  5347,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77, 15775,  7409,     7, 10519,  1672,     5, 20099,  3651,    30,\n",
      "          2590,     3]], device='cuda:0')\n",
      "tensor([[  211,  9571,    29,     8,  1853,     5,    29,  2104,    29,     8,\n",
      "          4589,     2,    29,  7208, 17187,    29,    22,    11,    26,   304,\n",
      "         16855,  5309,     3]], device='cuda:0')\n",
      "tensor([[    0,  4067, 11655,     0,    44,   106,     4,  1317,     2,  3471,\n",
      "             2,   690,     2,  9144,     2, 15434,    60,   567,     5,  1515,\n",
      "          6036,     4, 10475,  2645,    16,  1114,   154,  5600,     9,     4,\n",
      "           103,   231,     3]], device='cuda:0')\n",
      "tensor([[  23,  606, 3064,   29,    8, 2746,   75, 8328,    6, 4653,  136,    7,\n",
      "            4, 6991, 4467, 3854,   25,   40,  921,   96,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  182,    21,  4676,    52,   808, 15404,    11,    16,  2062,   102,\n",
      "         11704, 13941,     2,   737,     7,     4,   550,   186,  2012,     6,\n",
      "          1150,   465,     4, 15976,     5,  4508,  3900,    19,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 2362,    19,    64,  5730,   313,    33, 12646,    22,     2,    19,\n",
      "           301,    41,  5121,   501,     4,  6633,   956,     7,   131,   544,\n",
      "           518,     9,     4,  3844,     7,    58,  2645,    33,     4,   360,\n",
      "             7,    28,   550,     3]], device='cuda:0')\n",
      "tensor([[   34,    40,  1682,  3925,     2,     0,  7718,    16,  1214,   135,\n",
      "            41,  2312,  3708,   544, 19645,    21,  4329,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  62,  643,    4, 3284,    2, 4248, 2164,    7,    8, 3264, 2067,  709,\n",
      "         2344,   96, 6891,    8, 5483, 3459,    3]], device='cuda:0')\n",
      "tensor([[  189,    21,    34,  4848,   701,    30,     4,  2067,     2,     8,\n",
      "           835,     7, 15143,     5,  4572,    16,   582,  1545,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,  2697,  1002,     2, 17913,     2,     5,   242,  8947,    54,\n",
      "            22,   149,    41,   263,    19,    75,     6,  3368,    18,     4,\n",
      "         18057,   158,     5,     4,  4070,  1140, 20538,   158,     7,   102,\n",
      "          7162,     3]], device='cuda:0')\n",
      "tensor([[  77, 1282,    5, 5088,  606,    3]], device='cuda:0')\n",
      "tensor([[   23,   606,    21,  1620, 20121,     5,    83, 10468,  4991,  2938,\n",
      "           158,   118, 11458,     4, 13226,     7,     4,   606,   658,   158,\n",
      "           107,    22,  1808,   908,     4,  1118,     3]], device='cuda:0')\n",
      "tensor([[   62,    21,     8,  8086,  1771,     5, 11078, 12441,  1496,   427,\n",
      "          1344,     3]], device='cuda:0')\n",
      "tensor([[19370,  4215,    12, 15723,     7,    95,   661,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77, 10175,     2, 14124,  6054,  5947,    18,  6730,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77, 11482,   734,   837,  6198,  2268,    10,    62,    21,     8,\n",
      "          3651,   293,     7,  4690,  4255,    16, 18129,     4, 12276,     7,\n",
      "           102,  9467,    21,  9783,     3]], device='cuda:0')\n",
      "tensor([[  418,    99,    11, 12155,  4470,    29,  1346,     2,    68,    36,\n",
      "            50,    59,     2,    97,   214,  2872,  7117,  2268,     2,    43,\n",
      "            22,    21,    87,    51,     7,     4,  7452,     2, 18928, 15866,\n",
      "             6,   233,   443,     9,    83,    75,     3]], device='cuda:0')\n",
      "tensor([[  88,    8,  125,    2,    4,  606, 2195,   64,    8, 3429,    7, 1328,\n",
      "          662,    2,   43,   99,    6,  172,   16,  897,   22,    9,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 8060,  3143,    21,   845,    11,    40, 17840,     2,  3417,  3651,\n",
      "            52,    44,    81,    11,     6,   177,   158,     5,  1618,   158,\n",
      "            52,     4,   550,  9460,     3]], device='cuda:0')\n",
      "tensor([[   62,    21,     8,  3324, 10113,   130,     7,  4812,  1344,     5,\n",
      "          1501,     3]], device='cuda:0')\n",
      "tensor([[   23,     0,     7,     4,  1033,  9222,     4,  4401,  5685,     7,\n",
      "             4, 10050,     5,  1306,     6,   263,    90,    40, 18512,   360,\n",
      "             2,    29,    68,    49,    85,  6519,    16,    79,    82,  4699,\n",
      "           188,   250,     4,  1239,     7,    75,     3]], device='cuda:0')\n",
      "tensor([[    0, 20661,     0,  2988,     4,  1317,   558,     7, 18021,     2,\n",
      "             5,   682,   895,  3939,    30,    60,   131,  2210,    72,  7749,\n",
      "             4,  1476,    18,  1317,     3]], device='cuda:0')\n",
      "tensor([[   77, 15876,     9,   217,   835,     2,    23, 15446,  1783,  9817,\n",
      "            75,   222,     8, 18447,   810,     7,     4,  1204,  1134,    21,\n",
      "         13026,  7167,   225,     5,     4,  4849,    21, 19714,  6907,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 881, 1357,   11,    8, 2465,   12,  637,    2,    5,   94, 1778,    8,\n",
      "         2465,   12, 5807, 6696, 2458,    2,   72,   42,  113,  517, 1257,    6,\n",
      "            4,  342,    7,  221,   53,  149,  129,    3]], device='cuda:0')\n",
      "tensor([[    0,  1522,     0,    60, 15775,  7829, 12683,     2,   110,   134,\n",
      "           102,   200, 16429,     5,  5771,   201,   606,     2,    46,    83,\n",
      "          5532,   221,     6,   206,    52,    76,    11,   364,     9,   194,\n",
      "             5,   279,     3]], device='cuda:0')\n",
      "tensor([[  425,    21,   386,   236,  2433,  4467,  4455, 16013,    61,   102,\n",
      "          7905,    20, 10944,  4253,  1208,   117,  1525,     9,    58,  1562,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[  167,   608,    29,     8,  4275,     5,  6996, 14457,     2,    23,\n",
      "          2464,     7,     4, 11629,    42,    36,    27,   104,     6,  3603,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[  91,  745,   41,   64, 4124,    2,   43,   19,   39, 1002, 3117,  154,\n",
      "            3]], device='cuda:0')\n",
      "tensor([[ 124,   16,   21,    8,  289,  213,    7,  279,   48,  156,    6,    4,\n",
      "         1257,    3]], device='cuda:0')\n",
      "tensor([[  723,     4, 18025,  2572, 18590,     7,  6446,    21,  1140,    11,\n",
      "           229,   447,     2,    22,    21,   186,     8,  4721,   816,     6,\n",
      "             4,  1369,  7628,     7,     0,  2204,  2247, 12722,     6,     4,\n",
      "          2183,  2090,     3]], device='cuda:0')\n",
      "tensor([[  93,  109,    4, 3459,   21,    8,  201, 1939,    2,   43,   22,  786,\n",
      "           45,  750,   30,  326,    6, 1408,    3]], device='cuda:0')\n",
      "tensor([[   77,  8983,     5,  5613, 11394,     7,     4,  9839,  1022,     7,\n",
      "           956,     3]], device='cuda:0')\n",
      "tensor([[  77,  906,    2, 1346,    2, 1236, 4486,  550,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77,  4034,     7, 17105,     2, 15833,     2,     5,   804,  1506,\n",
      "            55,  6171,     6,   353,    22,    44,  3289,    56,     2,     5,\n",
      "          2984,     4,  3956,     3]], device='cuda:0')\n",
      "tensor([[18918,  1530, 16145,    21, 15375,  2411,  3617,    11,     8,  2929,\n",
      "             7, 11901, 13922,     2,    43,    22,  2697,   906,   346,     6,\n",
      "           805,   102, 12995,  4647,     5, 13690,     9,  3896,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  77, 2344,    2, 8820, 5991,    3]], device='cuda:0')\n",
      "tensor([[  723,     4,   976,    52,     0,    24,   395,    30,  2392,     2,\n",
      "            49,   165,  1753,    20,     8, 13754,  1418,  4255,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  519,  8156,  2533, 12683,    20,     8,  5423,  1807,     7,   268,\n",
      "          3617,     5, 14187,  3651,     3]], device='cuda:0')\n",
      "tensor([[ 1316,  4382,  4367,     6,     4,   312,     7, 10219,     8,  1317,\n",
      "           308,     2,     4,   606,  4343,     8,  1771,  3381,     7,  8660,\n",
      "             5,  6911,     3]], device='cuda:0')\n",
      "tensor([[  62,  643,   40, 2575,  204,   26,    8,  505, 9175,    6, 6222,  948,\n",
      "            9,   92, 4231,    3]], device='cuda:0')\n",
      "tensor([[  711,   386,   236,  3314,     6,     8,  3708,   479,     2, 10759,\n",
      "            21,     8,   108,  1560,     3]], device='cuda:0')\n",
      "tensor([[ 1947,  2776,     5, 11078,     2,     4,   606,     2,  5550,     8,\n",
      "         12117, 13859,     5, 10654,  3388, 15280,     2,  6338,    98,     8,\n",
      "          1359,  1138,   670,    16,    11,   214,  1162,     5,  3838,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 8054,  2195,  2948,     5, 15789,     2,    64,  9808,     8,  9466,\n",
      "          9617,   242,     4,  1643,     3]], device='cuda:0')\n",
      "tensor([[15205,     2, 18535,     2,     5,  7832,  2988,    10,   681, 11691,\n",
      "            21, 17101,     3]], device='cuda:0')\n",
      "tensor([[   77, 16491,  4658,     7,   177,     2, 10373,     2,  6928,     5,\n",
      "           493,    44,     2,  1908,     3]], device='cuda:0')\n",
      "tensor([[ 2550,   315,  1953, 14755,     2,  1560,  5377, 11694, 11135, 12005,\n",
      "            28, 18872,   414,     7, 13097, 16785,     5,  1681,  7722,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,     8, 15312,   414,    48,   171,    44,   454,  4085,\n",
      "             7,     9,    86,  2268,   158,    43,    22,    21,  3827,    82,\n",
      "           546,    20,   159,  5613,  4102,     5,  2533, 11093,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77, 11783,  2476,    16,  7278, 12005,     8, 11504,   559,     9,\n",
      "         10124,    20, 11504,  6613,     9,  1058,     3]], device='cuda:0')\n",
      "tensor([[ 4582,   148,     4, 14635, 11795,  6066,   875,   447,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   74,    11,    29,  8401,     8,   606,    29, 18034,  1208,   117,\n",
      "           488,    12,     2,   325,  4806,     7,     4,   291,   115,  1559,\n",
      "           279,    22,    21,  1078,     3]], device='cuda:0')\n",
      "tensor([[ 272,    7,    4,  163, 2268,    7,    4,  147,   20,  102, 5440,    7,\n",
      "            4, 6872,    6, 4264, 4044,   25,  664, 3024, 1506,   34,    8, 5128,\n",
      "         7642,    3]], device='cuda:0')\n",
      "tensor([[  418,   395,  5066,     4,  1385,     2,    28, 15393,  4658,  3800,\n",
      "          6566,   615,    52,   172,   221,    48,  1270,    30,  1349, 14948,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[  143,     4,   118,   213,  6193, 13221,   151,    18,    83,  1771,\n",
      "          3939,     5,  8820,  4647,     3]], device='cuda:0')\n",
      "tensor([[13044,  2147,    60,  5773,  1095, 17750,    55,   694,    98,   224,\n",
      "            16,   117,   150,   184,   779,    61,     4,   333,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 4210,    44,  4625,     9,  3788,     5,  2033,     2,    22, 15411,\n",
      "            71,    56,    73,   895,     3]], device='cuda:0')\n",
      "tensor([[   62,    21,  5948,     6,   114,     8, 19917,   550,    16,   149,\n",
      "            41,   299,    22,    46,     6,  2674,   428,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,   908,  1397,    73,    18,     4,  1453,     7,     4,\n",
      "          4054,     2,     5,    26,   304,     2,     4, 10150,  8732,     2,\n",
      "             7,     4,   321,     3]], device='cuda:0')\n",
      "tensor([[   77, 12734,  6241,     5, 12556,  6918,  3617,    52,     8,  1262,\n",
      "             5,  1005,  1466,    16,    11,     8,  2981,  2493,  1387,     7,\n",
      "           177,     3]], device='cuda:0')\n",
      "tensor([[   74,  9571,     0,   606,    11,    87,     8,  9736,  4569,     7,\n",
      "         14726,     3]], device='cuda:0')\n",
      "tensor([[   77,  6439,   130,     7,   645,    12,    99,     4,   118,  1659,\n",
      "             5,     0, 15723,     3]], device='cuda:0')\n",
      "tensor([[   77,  2446,  5676,   158,     8,   606,     6,    27, 15329,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  191,    27,   395,    30,     4,   163,     7,     4,   578,     2,\n",
      "            43,    22,    21,  5065,     2,  7278,  8401,     7,   102,   465,\n",
      "             5,  8340,   346,     6,   107,    22, 11917,   779,    16,    28,\n",
      "           550,  5634,    46,   435,   257, 14350,   948,    12,     8,    92,\n",
      "          2090,     3]], device='cuda:0')\n",
      "tensor([[   77,  5483,  2092,   606,    52,     4, 15288,  1088,     7,  9730,\n",
      "             9,     4,   194,     7,     8,   544, 11406,  1339,  2150,  3075,\n",
      "            12,    89,  1419,  1775,    40,  7629,     3]], device='cuda:0')\n",
      "tensor([[12686,  5855,   214,     4, 19634,     5,     4, 10790,   835,     7,\n",
      "          7411,  8044,     3]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  115,    36,    32,  2958,   138,  9613,     2,    43,    20,    60,\n",
      "          7754,    12, 11203,     5,   102,    96,    53,    46,   520,    60,\n",
      "          4658,     8,  1693,  6706,     3]], device='cuda:0')\n",
      "tensor([[   77, 15225,  4034,     7,     4,  1680,  6909,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[17798, 12429,     9,  3768, 15568,     5,  4647,     2,    28,  4767,\n",
      "          3617,    11, 11286,  6168,    25,     4,  4039,  5738,  4874, 10578,\n",
      "             5,     4,  9490,    43,   527,   687, 19367,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,  4483,     6,   114,   101, 17323,     5, 11073,   308,\n",
      "           176,   203,    86,     3]], device='cuda:0')\n",
      "tensor([[   23,   606,    11,  4603,  4238,     2,    43,     2,    50,     6,\n",
      "             4,   312,     2,     4,   647,    24, 11078,  1753,     2,  3598,\n",
      "             6,  1149,     8,   895,   354,    20,  3329,     6,     4,   540,\n",
      "             7,  5707,    21,  7670,     3]], device='cuda:0')\n",
      "tensor([[7889,   21,  606,   11,  293,    7, 5018, 3939,   30,  287,    6, 1335,\n",
      "            3]], device='cuda:0')\n",
      "tensor([[  272,     7,     4,   118,  1359, 18643,  8668,     7,     4,   147,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[ 3859, 17317,  4343,     8,  8929, 13102,   414,  8405,    20, 15474,\n",
      "            43, 13081,    20,  4690,  4255,     2, 11386, 14746,     2,     5,\n",
      "          7314,  2938,    16, 10958,    64, 10729,     7,  1278,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 6121,     2, 20250,    39,  5969,    35, 10911,  8235,    52,   170,\n",
      "             8,   245,   476,     7,    75,  7885,     2,   202,  6029,  3129,\n",
      "          3863,   484,     3]], device='cuda:0')\n",
      "tensor([[  23,  550,   11,  573,    6, 8272,    5, 9561,   51,    9,    8,  835,\n",
      "            7, 4777, 3436,  158, 3160,    5, 4284,  158,   16,   21,   44,  179,\n",
      "         2469,    9, 2436,   21,    0, 7248,    3]], device='cuda:0')\n",
      "tensor([[5975, 4406, 1888, 8956,   11,    8, 2681, 3752,  801,    2,   43,   49,\n",
      "          703,    8, 1328,    2, 8676, 4808,    6,    4, 3753,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,     8,   134,    50,  3013,  2419,    97,    76, 16022,\n",
      "            46,   520,   136,     9,    60,   465,   131,  1257,     2,     5,\n",
      "          5792,     2,  6849,     9,    12,  3445,  7401,     2,    11,     4,\n",
      "           602,  3599,     6,   169,   136,    18,     4,  1118,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  418,    99,    24,     4,   508,  1088,     5,  5470,  1884,   134,\n",
      "          2422,     2,     5,  2444, 11833,    50,  6725, 12340,    28,    75,\n",
      "           196,    29,  2491,     2,    43,     4,   606, 14029,     4,  2855,\n",
      "             7,  1319, 13152, 11687,    21,   753,     3]], device='cuda:0')\n",
      "tensor([[19751,    34,   280,     0,   253,     0,     4,  2020,     9,     4,\n",
      "          8206,   747,     7, 14999,    18,    60,  1476]], device='cuda:0')\n",
      "tensor([[10308,  3722,     2, 12405, 17871, 17673,     3]], device='cuda:0')\n",
      "tensor([[   77, 12711,    12,     4,   681, 15941,  8685,     5,    12,   510,\n",
      "            72, 10807,  4248,     2,  4275, 17888,     3]], device='cuda:0')\n",
      "tensor([[19066,  2164,   158, 11619,  4691,     2,   313,     2,  9223,     5,\n",
      "           710,     3]], device='cuda:0')\n",
      "tensor([[   23,  2933,     2,  5871,     2,   313,     2, 12369,     5,   717,\n",
      "            24,    44,  9312,   520,     4,   999,    21, 13943, 12120,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 5139,    50,  8959,     5,  6783,    97,     4,  8872,     2, 18205,\n",
      "           569, 11824,    46,   164,    30,   102,    86,  5220,   486,   578,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   23,   110,  2551,     7,     4,  1095,   298,    55,   550,     2,\n",
      "            43,    22,    11,     8,   108,  6849,  2664,    12,  1560, 18301,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   77,  9797,     2, 13401,  5071,     7,     8,   767,  2428,   506,\n",
      "             6,   168,    89,   125,   151,   194,     3]], device='cuda:0')\n",
      "tensor([[16008,    30,     4,  1586,     7,     8,  5483,    33, 14543,  5470,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[ 1980,     2,    29,     8,  2854,  2148,     2,     4,   606,    11,\n",
      "           548, 11634,     3]], device='cuda:0')\n",
      "tensor([[  208, 13725,     5,  2817,     7,   633,     2,    19,   301,    41,\n",
      "           184,    43, 10977,  3735,    35,  2287,     5,  9161,  1095,  4417,\n",
      "            37,    55]], device='cuda:0')\n",
      "tensor([[  519,  6851,  9652, 14794,     5, 16692,    16,  4382,    11,    50,\n",
      "           836,     9,  1900,    97,     9,  4756,     3]], device='cuda:0')\n",
      "tensor([[  208, 13752, 14670,    11,  3497,    16,    12,     4,   103,    75,\n",
      "            53,   197,   495,  2857,    50,     6,   738,    97,     6,    58,\n",
      "          8894,    72,  4716,    90,     6,    28,   550,    12,     4,  6578,\n",
      "          2383,     3]], device='cuda:0')\n",
      "tensor([[  341,     8,    75,    80,   671,     4,  4889,   302,  1257,    24,\n",
      "           201,    50,    97, 13094,  7244,     2,    22,    21,  5948,     6,\n",
      "           114,     8,  4912,    16,  1300,    76,    22,    11,     2,     5,\n",
      "          1300,     4,   457,    21,   576,     3]], device='cuda:0')\n",
      "tensor([[   23,  2855,     7,     4,   606,  2974,    36,     9,     4,  5546,\n",
      "          1759,    43,     9,     4, 10104,     7,   102,  3939,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 7883, 13169,     9,     4, 10879,     7,  1120,     2,    43,    60,\n",
      "          2344,     2, 10496,  1139,     5, 17925,  1469,     0,  8432,   128,\n",
      "             0,  5695,     4,  4255,     7,  2966,    21,  9783,     2,     5,\n",
      "            16,  5311,    60, 14746,    30,     0,     3]], device='cuda:0')\n",
      "tensor([[  182,   216,   768,    97,  1095,   177, 13586,    55,   117,   544,\n",
      "           405,     7,    95,   563,   939,    38]], device='cuda:0')\n",
      "tensor([[  23,  318,  597,    7, 2491, 4584,   11,    8,  606,  395, 3264,    6,\n",
      "          102, 8600,    3]], device='cuda:0')\n",
      "tensor([[   77,   550,    16,  3000, 13133,     8,   163,  1640,  2392,    98,\n",
      "             8, 11054,    16, 10202,    16,    19,  1536,     4, 11731,  4754,\n",
      "          6100,     3]], device='cuda:0')\n",
      "tensor([[ 3594,   484,     7,  4065,  2639,   139,  1592,     5, 13994,  4912,\n",
      "          7416,     3]], device='cuda:0')\n",
      "tensor([[  62,   21,   87,    2, 1745,    2,  153,  447,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   34,   719,     4,  1727,     7,  3049,     5, 16750,    22,    20,\n",
      "             8,   414,    16,   117,  1352,   510,  3380,     7,    58,  9989,\n",
      "            20,     4,  2690]], device='cuda:0')\n",
      "tensor([[   0,   21, 3833,   11,    8, 4965,    5, 1162, 5071,    7,  423, 1339,\n",
      "          198,   11, 1082, 6674,  188,    2,  103,   25, 3075,    5,  119,   25,\n",
      "         3994,    3]], device='cuda:0')\n",
      "tensor([[  77, 1693,   43, 4461, 5947,   18, 4896,    2,  268,    5, 7754,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[18906,  4703,    60,  1476,     9,     8,   125,    16, 14278,     5,\n",
      "           148, 15353,   136,     2,     5,    53,   229,  5204,     4,  1166,\n",
      "             6,   644, 13854,     3]], device='cuda:0')\n",
      "tensor([[ 7464,  4019, 13997,  3753,    98,   224,  9367, 13591,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[18115, 19549,    25,  1470,     8, 11119,   266,    98,     4,   973,\n",
      "             7,     4,  3315,    21, 11520,   427,  1593, 15602,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 892, 5709,   26,  102, 3959,    3]], device='cuda:0')\n",
      "tensor([[  23, 3939,  169,    4,  550,    6,    8,  932,  471,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   76,   150,   370,    22,   508,    11,    16,    22,  6077,   136,\n",
      "            98,   102,   198,     2,   994,   136,     8,  3931,  1339,  3199,\n",
      "             5, 12282,    48,    42,   600,     2,  8838,   154,    20,   836,\n",
      "          1476,     5,  5222,   136,    56,     7,     4,  3883,  1179,    48,\n",
      "           171,  2048,     8,   153,  3154,     3]], device='cuda:0')\n",
      "tensor([[15093,     6, 12209,     4,   599,     2,  1848,     5, 20669,  3459,\n",
      "            98,   224,   395,  8786,     3]], device='cuda:0')\n",
      "tensor([[15232,    20,  1476,     5,  1537,    83,  8340,  2938,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[19016,    21,  1068,     6,  6848,  6684,     5,   488,   590,    20,\n",
      "          8435,    21,  3478,  5164,     6,   107,    28,     8,     0,  2115,\n",
      "           595,     3]], device='cuda:0')\n",
      "tensor([[11276,     7,  1886,   574,    29,    68,    22,    31,   164,   242,\n",
      "           134,   371,   158,     5,    11,   163,  2482,    16,   125,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[14274,     2,    28,    11,  1308,    29,     8,   606,     2,   202,\n",
      "            26,     4,   195,    75,   170,     8,   118,  5532, 14237,     7,\n",
      "             4,  2505,  7642,     3]], device='cuda:0')\n",
      "tensor([[  93, 3211,   12,    0, 7178,    5,  871,    0,    2,  148,   64,   90,\n",
      "            2,  325, 1338,   22,   21,   40, 5713, 2472,    6, 7720,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  23,  163,  606,   52, 3049,    6,  756, 7327,  250, 2604,    7, 5395,\n",
      "            3]], device='cuda:0')\n",
      "tensor([[ 2607,     7,     8, 14688,     0,  3377,  3506,     2,    22,    21,\n",
      "            40, 14939,     2, 18911,  3587,  6094,     7,   194,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   4,  606, 6074,   98,    8, 6684,   16,  117,  801,    8,  290,  580,\n",
      "         5009,    3]], device='cuda:0')\n",
      "tensor([[  105,    23,  4955,     7,  7199, 13579,   149,    41,  5474, 18155,\n",
      "            98,     8,   550,  1387,     2,   119,     4,   225,    11,   148,\n",
      "            50, 11374,    97,    22,    31,   131,  5009,   220,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[    0,  1127,     0, 18349,    41,  4480,   170,    29, 16185,    33,\n",
      "         15640,    29,  3419, 10073,  1213,  1257,   158,   795,     4,  8945,\n",
      "           109,    16,     4,   820,  2020,    39,   390,    27,   734,     4,\n",
      "          5827,     3]], device='cuda:0')\n",
      "tensor([[  211,     8, 12310, 10222,     2,     4,   606, 16199,    54,     8,\n",
      "          8987,     2,   728,     2,    22,    11,    36,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  105,  1560,   893, 19006,    99, 15055,  6041,    60,  1476,     2,\n",
      "            53,   149,    41,   941,    90,     9,  8886,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 105,   35, 1898, 1582,    6, 1095, 1039,   55, 2268,   19, 1650,  301,\n",
      "           41, 1584,   22,    3]], device='cuda:0')\n",
      "tensor([[  34,    8,  414,   48,   32,   41,  454,   18,    4,  289, 1046,  174,\n",
      "            2,    5,   22,   21,    8,  414,   16,   48,   29, 1788,    2,    5,\n",
      "          687, 4994,    2,  135,  109,    3]], device='cuda:0')\n",
      "tensor([[  881,  6284,     2,    22,    21,  6014,     5,   186,  9766, 13113,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[  190,     9,    44,     2,    23,  4955,     7,  7199, 13579,    11,\n",
      "          3010,     2,    43,    22,    11,  3627,    94,  1701,     2,    64,\n",
      "             4,  2392,   707,    67,    22,    11,   407,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 105,   19,   42, 3970,    4, 3947,  650,    2,   22,   21,  908, 3094,\n",
      "           56,   12,    4, 3939, 1225,    3]], device='cuda:0')\n",
      "tensor([[ 1918, 13571,     2, 10786,   386, 19411,     9,  3162,  4126,    21,\n",
      "          8033,  5871,     2, 18206,   994,     8,   684,    16,   117,    36,\n",
      "            27,  2422,   707,     3,    55]], device='cuda:0')\n",
      "tensor([[   34,  9005,     2,  5730,  1543,     5,  5471,  7087,  4059,    58,\n",
      "          4297,  2831,     9,     8, 15689,     7,  9652,  2576,    16,   994,\n",
      "          6435,  2103,     6,    40,  4500,     2,    43, 13113,     2,  3931,\n",
      "             3,    55]], device='cuda:0')\n",
      "tensor([[  244,    28,  6805,  1556,     2, 17598,    11,   110,   134,     8,\n",
      "           971,     9,     4,   162,  1673,     2,    20,   102,  3906,     7,\n",
      "         16815,     2, 13269,     5,  6629,     3]], device='cuda:0')\n",
      "tensor([[2859,    2, 5318,    5, 7203, 4164,  421,    7, 9652, 2164,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[20699,    34,  6024,     8,   255,     7,   701,    98,    60,  4451,\n",
      "         12429,  5470,     5,  8838,  1048,    20,     8,  2067,     7,  8676,\n",
      "           158,    43,    36, 15388,   158,  1477,  1476,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[3464, 8812,   98,    4, 1715,    7,  666, 7751, 2859,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  23,  606,   21, 3939,   24, 8340,    3]], device='cuda:0')\n",
      "tensor([[ 1009,     9,   102,   118,  8994,  3099,     2,  2379,  9568,    11,\n",
      "         12464,     3]], device='cuda:0')\n",
      "tensor([[   23,  1782,   108,  5392,   222,  7652,     5,  9921,    11,    76,\n",
      "          2374,    28,  1950,  6618,  6448,   183,     2,    20,   346,  7432,\n",
      "         11562,   158, 18041,     0,   158,     6,   295,   214,   637,     5,\n",
      "           977,  7873,     3]], device='cuda:0')\n",
      "tensor([[16009,    21,  9503,  2854, 11633,    11,  9303,    29,   129,    29,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         11616,     3]], device='cuda:0')\n",
      "tensor([[2701,    2,   22, 1711,   16,    0, 6297,    0, 7982,   21,  406, 2327,\n",
      "           11, 1014,    6,  266,    9,   60, 3745, 3611, 2268,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  62,   44, 2990,   61,    6,  108,  447,    3]], device='cuda:0')\n",
      "tensor([[   77,   289,     2,  2913,     2, 10938, 18687,    16,  4343,   102,\n",
      "         14588,     9,  3284,     2, 11900,  1626,     3]], device='cuda:0')\n",
      "tensor([[   77,  3324,     5,  9971, 14711,   333,    12, 13249,    29,     8,\n",
      "          8465,  1001,  2914,     3]], device='cuda:0')\n",
      "tensor([[13884,    11,    40,  4826,   204,    26,     4,  5326, 13512,     5,\n",
      "         11301, 15717,  3931,     3]], device='cuda:0')\n",
      "tensor([[    8,  2446,     2,  2817,     2, 10828,   606]], device='cuda:0')\n",
      "tensor([[15940,   149,    41,  3354,    12,    70,  7234,     2,    43,   620,\n",
      "          4343,     8,   684,     7,  5099,  3150,     5,  3106,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   23,  5051,  1901,     7,  5233, 18021, 15010,    11,    16,    22,\n",
      "           229,  5712,  1352,    20,     4,  1677,     7,     4,  9567,  1166,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   77,   670,     9,  5330,     7,  4894,     2,  1470,   948,    61,\n",
      "             9,  5051,  3459, 12143,    34]], device='cuda:0')\n",
      "tensor([[   23,  1773,   682,   175,   163,    34,     6,  3140,     4, 13982,\n",
      "          7097,  3391,    25, 13105,     5,  9718,  5838,     2,  1339, 18988,\n",
      "         20649,  4382,  2215,     9, 19311, 19183,     3]], device='cuda:0')\n",
      "tensor([[ 2456,     8,   255,     7,     4,  9604,     7, 10120,    26,    60,\n",
      "           163,     3]], device='cuda:0')\n",
      "tensor([[15205,    43, 19706,   204,    98,     4,   539,     7,  5968, 17101,\n",
      "             2,  3298,  4545,     3]], device='cuda:0')\n",
      "tensor([[ 2824,    22,    21,   438,   548,  8579,     6,   206,    59,     2,\n",
      "           646, 10660,   156,     6,  7889,    12,   377,  5720,    96,    72,\n",
      "           204, 13132,     3]], device='cuda:0')\n",
      "tensor([[   62,  4953,     6,    27,   454,    25,   510,    20,   148,     8,\n",
      "          2858,   750,     9,     4,   754,  7680,     4,   198,  1437,    58,\n",
      "           200, 11150,     3]], device='cuda:0')\n",
      "tensor([[  77,  550,   16, 4490,  136,    7,   73,  101, 2069,    5, 6348,    4,\n",
      "         3709, 5709,   42,   27,   80,   22,   21, 5787,   20, 4741,    5, 8844,\n",
      "            3]], device='cuda:0')\n",
      "tensor([[  500,     6,  1723,    21, 10358,  4124,     5, 14000,    21,  1464,\n",
      "          8119,     2,  4124, 12222,    11,    51,     7,     4,   118,  5483,\n",
      "          5244,    18,    88,     4,   879,     7,  1245,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 5848,  6179,    11,    36,    99,    97,     8, 15733,  2804,  3466,\n",
      "             5,  4579,   982,     2,    43,    87,     8,  8522,     7,     8,\n",
      "          2709,  1025,  1166,    18,     4,  9284,     7,   597,     6,     8,\n",
      "           479,     3]], device='cuda:0')\n",
      "tensor([[   62,    21,     4,  5392,   222,     4,   405,     5,     4, 16429,\n",
      "         18991,  5967,     5, 19310, 14144,     7,  3824,  8415,    16,   370,\n",
      "            28,  5233,   938,  6890,     5,     8,  5056, 15010,   447,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1079, 12450,     5, 10597,  8374,  2441,     9,  5273,  5561,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   91,    47,    41,    32,     6,   109,    52,   313,     6,  2096,\n",
      "             4,   606,    21, 14855,  3906,     7,  3651,     5,  4691,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  77,  606,   52,    8,  544,  290, 1527,  337,   16,   11, 3418,    5,\n",
      "         5532,    6,    4, 9623,    3]], device='cuda:0')\n",
      "tensor([[  143,     4,   103,    75,     9,   137,     2,  2435, 13480,  9895,\n",
      "          1282,  6725,     2,  1338,   140,    53,    21,    82,  9658,    25,\n",
      "             4,  1495,   130,     7,    60, 14174,     3]], device='cuda:0')\n",
      "tensor([[   23,   606,    21, 15015,     5, 20398,    24,    50,    97,  9657,\n",
      "            12,    25,   102, 16675, 13088,  3183,     3]], device='cuda:0')\n",
      "tensor([[3625,    4,  606,   21,    0, 3459,   81,   11,    8, 3025, 3646,  506,\n",
      "            6,   84,   56,    2,   43,   22,   21,   36,   16,    2,   22,   21,\n",
      "            4, 4978,   16, 2374,   19,    9,   35, 1979,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[12043,     5,  1977,    24,   108, 12761,  2559,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   23, 15984,    17,     0,  5674,    11, 17331,     9,     4,   550,\n",
      "            21,  5893,     2,     8,  9505, 16031,     7,  8205,  7317,    16,\n",
      "             2,    25,     4,   267,     7, 13593,  1553,     2,  1030,    45,\n",
      "          2320,  9364,    56,     7,     4,  3883,     3]], device='cuda:0')\n",
      "tensor([[  519,  2864,  7056,     2, 19897,     0,    36,     0,     0,   606,\n",
      "            30,  1726,  1560, 16361, 17965,     2,    72,   719,     4, 13613,\n",
      "          7454,    18,  2499,  2670,     5,   377,  4193,    22,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 2655,    79,     4,   108,   835,     6,  2067,  4295,    72,    24,\n",
      "             2,  1515,  2396,     2, 11444,    25,     4, 18473,   380,     2,\n",
      "         18461,   119,   682,  5680,  3939,    30,    90,    44,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77,  5051,     5, 15191,     0,    12,     4,   118,   213,     0,\n",
      "         14371,     3]], device='cuda:0')\n",
      "tensor([[ 6410,  1394,  1485,     8,   751,  3034,    11,     8,  8676,     5,\n",
      "         10175,  1726,   606,    16,  6810,     4,  4483,  3580,   222,   405,\n",
      "             2,   311,     2,  1206,     2,     5,  7424,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1316, 11319,    20,  4255,     5,     8,   219, 13317,  6135,     2,\n",
      "             4,   606,    11,     8, 13752,  1284,   204,    26,   544,   405,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   23,   163,  6928,   115,    73,    27,   692,   129,   140,    28,\n",
      "           606,     2,  4131,    86, 13536, 10980,     2,    11,   395,    50,\n",
      "         13007,     6,     8,  5676,    97,     8,  6839,  4374,  3523,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77, 11642,     5,  6851,  7718,  1970,    20,  7019,     2,   502,\n",
      "           199,  8998,    11,    40,   660,     3]], device='cuda:0')\n",
      "tensor([[   74,    11,     8,   414,     7,   131, 14902,    72,    47,    41,\n",
      "          1173,     8,   900,  1225,     2,    43,   419,    49,    24,  5360,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[19434, 16065, 11345,     7,  1479,   146,    36,  1362,     2,    43,\n",
      "           172,    72,  1283,  3056,    29,  3044,    32,    26,   181,   238,\n",
      "             8,  4721,  5991,     3]], device='cuda:0')\n",
      "tensor([[  272,     7, 17094,     2, 13371,  1257,     6,   233,   443,     9,\n",
      "             8,   215,     2,   215,    75,     2,   954, 16767,  5655,  7569,\n",
      "            33,    23,  4415,     3]], device='cuda:0')\n",
      "tensor([[13356,     5,  7654,    21,   416,    12,    40, 15994,   465,   370,\n",
      "            12,     8,  9313,  5375,  7904,   414,     2,    43, 18210,  6077,\n",
      "           176,     8, 16258,  4222,     9, 11503,    10,   145,   370,  1059,\n",
      "          1820,     3]], device='cuda:0')\n",
      "tensor([[13896,    11,  3263,    29,     4, 17374, 17454,     2,    60,  2350,\n",
      "         15528,    29,   134, 12092,    29,    22,    11,  1361,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[6147, 5589,   46,   50,   97,  346, 4808,    6,  107,   22, 5018,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[15093,     6,    27,  1464,     5, 14888,  6348,    26,     4,   195,\n",
      "            75,     3]], device='cuda:0')\n",
      "tensor([[   74, 18321,   270,   274,    32,    50,  8256, 14042,    97,    40,\n",
      "          5437,  5122,   606,     3]], device='cuda:0')\n",
      "tensor([[ 124,   73,   80,   19,  133,   22,  301,   41,   84,   95,   50, 1353,\n",
      "            2,    9, 7815, 5653, 3185,    3]], device='cuda:0')\n",
      "tensor([[  227,  7356, 10265,    12,  1560,  2605, 12471,     2,    72, 17460,\n",
      "            30,  4993,  2465,     6, 14560,  1060,    20,     8,  2592, 16297,\n",
      "             7,   483,     3]], device='cuda:0')\n",
      "tensor([[   23,   550,    11,    41,    73,  4991,    10,    62,    21,  8820,\n",
      "             5,  9799,     2,   179,     2,     5,     9, 11176,     2,    22,\n",
      "            11,    41,   148,    44,    16,  4737,     3]], device='cuda:0')\n",
      "tensor([[    0, 17888,    26,   102,   163,     3]], device='cuda:0')\n",
      "tensor([[14492,    34,  6928,    11,  1464,    37]], device='cuda:0')\n",
      "tensor([[20045,     5, 16465,    34,   560,  4826, 15280,     9, 18692,    21,\n",
      "          6518,     5,  4486,  2533,  1716,     3]], device='cuda:0')\n",
      "tensor([[   91,    39,  6687,    20,     8,  8057,   437,     7,   101,     4,\n",
      "          7624,     7,  2946,  7875,    18,     5,     4,   855,   618,   478,\n",
      "             6,   600, 14095,  4654,     4,  2750,   618,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[20422,    46,   608,    28,     0,   726,     5,   164,     8,   620,\n",
      "          6738,     2,  8740,  2164,    56,     7,    22,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   88,  6109,     7,   666, 10859,    21, 15170,  1476,     5, 10458,\n",
      "          7884,     2,    28,   572,  5002,  1140,     7,     4,  6634,    11,\n",
      "             8,  5496,  4826,   606,     3]], device='cuda:0')\n",
      "tensor([[13324,  6870,  1040,  4343,     4,  2463,    12, 10464,  1208,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  189,    46,   243,    82,   224, 13113,    52,     4, 10238,   427,\n",
      "         13884,     3]], device='cuda:0')\n",
      "tensor([[  211,     8,  9757,  1560,     2, 12775,    46,  9147,   224,     9,\n",
      "          1048,    29,    40,  3599,    16,   643, 18695,    20,   102,  1418,\n",
      "          2428,     3]], device='cuda:0')\n",
      "tensor([[  143,     4,   118,   213,     2,  1560, 18860,     0,    21,   103,\n",
      "          1112,    11,     8,  3478,     2, 10146, 18535,  3617,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  205,     4,    75,    48,   632,    16,  2332,    21, 18275,   391,\n",
      "          3137,  1752,    11,   217,   421,    29,  4660,    29, 18541,    21,\n",
      "         16667,     2,    48,    42,   699,    22,  1906,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   74, 11979,   396,  1140,  1051,  3646,  9223,   414,  1792,    20,\n",
      "             4,  4816,   372,     7,   327,  1479,    10,  6058,  8209,     9,\n",
      "           102,  4297,     5,  3573,  1722,     3]], device='cuda:0')\n",
      "tensor([[  77, 5001,    2, 9792,  268, 3617,    3]], device='cuda:0')\n",
      "tensor([[   77,  1039,     2,  9303,   606,    16,  5920,     6,  6848,    50,\n",
      "          4223,  2062,   102, 13136,     5, 12101,    97,    47,   118,  2268,\n",
      "            97,    24,   395,    50,  3587,     5,   779,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  143,     4,   118,   213,     2,    22,    21,     8,   130,     7,\n",
      "         14624,  5243,     2,  5408,   779,     7, 14437,  5164,     5,  1234,\n",
      "          1540,     3]], device='cuda:0')\n",
      "tensor([[   62,    46,     4,  4808,     7,     4,   660,   327,   961,  1257,\n",
      "             2, 14293,    18,     4,  2913,     2, 16547,  3477,     7,     4,\n",
      "          8741,    21, 15709,     3]], device='cuda:0')\n",
      "tensor([[    0, 19291,    21,     0,  2645,  2974,     9,    40, 11679,     2,\n",
      "          2534,  5543,     7,     8,  5773, 13688,     5,     9,     4,  6518,\n",
      "             2,  7050,  4647,   116,  3271,    12,    89,  1476,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   88,    44,     2,    28,    11,     8, 15146,   550,    16,    21,\n",
      "            36,   527,     4,  5018,   330,    22,   274,    32,    82,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[17991,    21, 11286,  3119,  1178,     7, 11504,  7809,  5364,    16,\n",
      "         12478,    46,     8,   110,   687,   567,     3]], device='cuda:0')\n",
      "tensor([[ 199, 1700,   85, 2497,   18,    4, 1476,    3]], device='cuda:0')\n",
      "tensor([[  212,    11,     8,  6844,     6, 19766,     5,  2828,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 105,   13,   79,   82,  883,   52,    4, 2854, 2256,    2,   49,   69,\n",
      "           32,   82,  363,  224,  765,    3]], device='cuda:0')\n",
      "tensor([[ 272,    7,    4,   50, 4248,  321,   21, 1257,    6,  756, 7327,   28,\n",
      "          147,    3]], device='cuda:0')\n",
      "tensor([[ 2300,     4,   476,     7,   550,    48,    85,  2561,  5233, 19323,\n",
      "          1630, 16473, 15010,    33,  5233, 16526, 15010,    31,   183,     6,\n",
      "            27,    38]], device='cuda:0')\n",
      "tensor([[74, 11, 22,  3]], device='cuda:0')\n",
      "tensor([[  418,    12,     4, 17787,    33, 15633,     2,    22,    21,     8,\n",
      "          8775,    68, 17736,  4444,     7,    40, 16826,  1479,     2, 15653,\n",
      "             5,  2150,  2685,     3]], device='cuda:0')\n",
      "tensor([[   77,  4912,    16,    21,  1365,  9652,     9,  3912,     2,     5,\n",
      "             8,   414,    16,    21,  5483,     5,  9792,   158,   148,    68,\n",
      "             4,   633,  5486,     6,     8,   289,     2, 15315, 17726,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   23,   606,    21,   548, 11098,  5071,     7,  7962,     5,  6684,\n",
      "         12626,   102,   916,   414,     6,  1491,     6,     4,   844,     9,\n",
      "            67,   146,     2,   576,     5, 11383, 13014,     2,     5,  1006,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          4069,     2,  1086,  4801,     3]], device='cuda:0')\n",
      "tensor([[ 9845,     8,  4483,  7115,    30,   488,     5, 12955,     6,  1677,\n",
      "             5, 15426,     3]], device='cuda:0')\n",
      "tensor([[  189,    21,   224, 13016,     2,     5,  8775,     2,   179,     2,\n",
      "            52,     4,  4829,  8914,    16,  7876,     8,     0,  1560,    64,\n",
      "          4008, 10579,     6,   953, 14859,    20,    28,  3563,   618,    59,\n",
      "          6270,     6, 18808,     4,  1902,     3]], device='cuda:0')\n",
      "tensor([[  143,   510,  8361,    20, 20359,  2333,     9,   773,     5,  8124,\n",
      "          5634,     7,  4122,  4858,     9,   931,     2,    22,    21,    40,\n",
      "         15936,     3]], device='cuda:0')\n",
      "tensor([[  62,  582,   64,   13,   32,   82, 1413,   45,  494,  194,   12,   28,\n",
      "          550,    5,  113,   13,  301,   41,  898,   12,    4, 6448,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  62,   21,    8,  421, 6618,   16,   22,   99, 5920,    6,   27, 3025,\n",
      "          848,    7, 1411, 3263,    3]], device='cuda:0')\n",
      "tensor([[  519, 14148,     2, 10938,   771,    16,    21, 18190,  6168,     2,\n",
      "         14675,  1021,     5,  9797,   346,     6,  6573,   118,     7,   102,\n",
      "             0,  1628,     3]], device='cuda:0')\n",
      "tensor([[  23,  395,  562,  115,   27, 1210,    6, 1096,    2,   43,   30,  809,\n",
      "         1997,    6, 2671,    7,    4,  633,    2,   28,  606,   11,  118, 9415,\n",
      "           80,   22, 4773,  280,    9,    4,  465,    3]], device='cuda:0')\n",
      "tensor([[   62,  8889,     8,  3369,     5,  4039,  6348,  3304,     7,     4,\n",
      "           163,  1257,    29, 10109,  1095,   771,   673,     3,    55]],\n",
      "       device='cuda:0')\n",
      "tensor([[3766, 5201,    2, 7292, 6490,    2,    5, 4777, 2084, 3099,  517,   61,\n",
      "            6,  236, 1095, 4362, 9484,    3,   55]], device='cuda:0')\n",
      "tensor([[  205, 14887,  7337,     4,  2685,  1036,     9,     4,  2542,     7,\n",
      "            40,  4606,   982,     7,   313,     2,     0,  1850,     0,  1482,\n",
      "            70,  1062,     6,     4,  6933,  3278,   222,  6499,     5,  4570,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   62,    21, 14451,     6,   114,     0,    21, 19551,  2949,     6,\n",
      "            60,  4837,     3]], device='cuda:0')\n",
      "tensor([[  666,   606,     2,    43,   110, 16784,     3]], device='cuda:0')\n",
      "tensor([[ 3397,    29,    48,   274,    27,  1000,     9, 13203, 18734,     2,\n",
      "         17310,    46,     8,   245,  3880,     9,   539,   158,  6042,     4,\n",
      "          5295,     7,    70,  8421,    12,  3468,  7582,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[14753,     9,   102,  5769,     2,  6566,    12,   102,  4606,  4104,\n",
      "             2,    23,  4840,  4228,    11,     8,   606,    16, 12704, 17847,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   62, 12313,     2, 19063,     2, 18075,     5, 15353,    54,    22,\n",
      "            11,  2747,     6,   204,   296,     3]], device='cuda:0')\n",
      "tensor([[4725, 1191,    2,    5,  119,   81,   21,    4,  313,   34]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    46,  4808,     6,  4401,     2,     5,  4131,   138,  3752,\n",
      "         11137,     2,    22,   149,    36, 13581,   501,  4318,     9,     4,\n",
      "          2020,     3]], device='cuda:0')\n",
      "tensor([[ 1316,  1977,    11, 12241, 12733,     4, 10393,   389,    12,    23,\n",
      "          3016,     7,     4,  1066,     2,    48,   215,    12,     8,  1552,\n",
      "           835,     7,  9097,     9,     4,   128,     5,   113,     7,    23,\n",
      "           938,  7738,     3]], device='cuda:0')\n",
      "tensor([[  62,   11,  153,  950,  447,    6,  641, 6000,    5,   60, 5401, 7500,\n",
      "         6768,  176,    8, 8676, 2067,    7, 1476,    3]], device='cuda:0')\n",
      "tensor([[20593,  1346,     2,   102,  1476,    44,     4,    50,  5532,    12,\n",
      "          7894,     6,  7720,    33, 16713,   924,     3]], device='cuda:0')\n",
      "tensor([[   62,    39,    36,  2857,     6,     4, 10130,     2,    43,   172,\n",
      "            72,    64,   215,   753,     5,  1257,    39,  6729,     4,   125,\n",
      "            22, 13821,   360,     5,  3106,     3]], device='cuda:0')\n",
      "tensor([[   74,  7973,    11,    52,    29,   866,     5, 17233,    29,     8,\n",
      "          4513,    42,    84,     3]], device='cuda:0')\n",
      "tensor([[ 5233,    23,  3107, 15010,    11,   450,   134,    40, 12585,  1336,\n",
      "             7,     4,   606,    16,  2719,    22,     2,     5,    22,  4601,\n",
      "             4,   195,  6403,     5,  7339,     3]], device='cuda:0')\n",
      "tensor([[   23,  3057, 19317,  1583,    11,     8, 11673,  2478,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[11757,  6039,   116,    21,    16,  2469, 16964,    72,  5617,  5370,\n",
      "             4,   966,     7,    89,  5102,     3]], device='cuda:0')\n",
      "tensor([[18291,    60,   125,   151,     4, 13404,   213,     7,     8,  1893,\n",
      "         12015,  5854,     4,   267,     7,    60,  9370,     2, 11923,  2382,\n",
      "         15985,   217, 19686,  1028,     3]], device='cuda:0')\n",
      "tensor([[ 3842,   134,     6,   699,    34,     5,     8,   255,     6, 15074,\n",
      "           106,     9,   978,     7,   177,     2,  6213,     5,     4,  1206,\n",
      "             7,  3385,   436,     3]], device='cuda:0')\n",
      "tensor([[ 519,  364,  550,    2,    8, 5503,    7,    4,  360,    7,  606,    6,\n",
      "          649,  136,    5,    6,  107,  136, 4652,   70, 1660,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  391,    28,   177,    33,    11,    22, 17468,    38]],\n",
      "       device='cuda:0')\n",
      "tensor([[17780,   370,    22,   836,   506,     6,   168,    56,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   23, 12464,  3939,     7,     4,  2210,   295,     4,   606,  6295,\n",
      "             5,   295,     4,  2020, 14255,     3]], device='cuda:0')\n",
      "tensor([[ 4350,  1331,    12, 10149, 14768,    21,   684,   158,     5,    12,\n",
      "             4,   125,    22,  2005,     8,  1479,     9,     4, 14431,     7,\n",
      "          4296,   344,     3]], device='cuda:0')\n",
      "tensor([[  938,   389,  2389,    25,   158,  5989,    21,     8,  2632,    80,\n",
      "            19,    47,    41,    32,     6,  7604, 18715,   158,     5,   148,\n",
      "             8,  8444,     6,     4,   457,   478,   296, 17252,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,    51,  6019,     7,     8,  1138,   670,   158,    36,\n",
      "             7, 12272,    33,  7803,    43,     7,     4,   870,  1221,   222,\n",
      "            90,     3]], device='cuda:0')\n",
      "tensor([[13930,  3018,    18, 18088,     3]], device='cuda:0')\n",
      "tensor([[   77,   818,    64,    28,   135,  5649,  2949,     9,   102,  2020,\n",
      "            54,    23, 14157,   149,    36,     3]], device='cuda:0')\n",
      "tensor([[14247,    11,    76,  1176, 11692,   274,    32,  6564,    79,   275,\n",
      "            21, 19343,  1348,  2546,    82, 10411,    25, 13570,     9,  7225,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[12077,     2,  4529,     5,  1418,     3]], device='cuda:0')\n",
      "tensor([[   74,  6566, 19482,    11,   349,     6,  1676,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1794,     5,  3833,    24,  1681,  1476,   158,  1006,  5001,     2,\n",
      "          1006,  3822,   158,     5,     4,  5771,  3939,    25, 12081,     0,\n",
      "             5, 19459,     0,   107,    58,  3318,     5, 10848,  7293,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77, 15040,     2, 11442,  6094,     7,  2237,  8148,    34]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,    59,   108,    16,    19,    42,  5495,   114,     4,\n",
      "          2436,  1095,  4655,    55,   506,     6,   280,   419,     4,  2067,\n",
      "             5, 11117,   310,    12,     4, 18816,    17,  6092,   327,  7838,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   23,   840,     7,     4,   982,     2,     4, 17146,  8278,     7,\n",
      "             4,  9379,   999,     2,     5,     4,  9508,  5418,     9,     4,\n",
      "         15177,  1556,  2674,  5182,   346,     3]], device='cuda:0')\n",
      "tensor([[  519, 17466,  3651,    52, 12653,     2,  5627,     5,   956,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[1095,  192,   55, 2372,    5, 2215,   20, 3099,   59, 8986,   13,  120,\n",
      "          186, 8495,    3]], device='cuda:0')\n",
      "tensor([[124,  13, 171, 970,   6, 759,   8, 480,  18, 217, 347,  30, 113,  18,\n",
      "           3]], device='cuda:0')\n",
      "tensor([[   74,  8588,  1112,    11,     8,   289,   691,     2,  2223,   158,\n",
      "            26,   358,     4, 17912,     2,     5,   795,   148,     8,  7385,\n",
      "           493,     4,  1177, 10718,     2,  5957,  6799,    21,  1186,  4981,\n",
      "          6695,    10,    23, 16616,  1744,     3]], device='cuda:0')\n",
      "tensor([[   34,    20,  5233,    23, 10154,  6813, 15010,    48,   748,     6,\n",
      "             4,    50,  1351,   710,  4579,     3]], device='cuda:0')\n",
      "tensor([[10925,  9558,    11,     8, 13278, 14432,   606,  1339,  5483,  1476,\n",
      "             5,  4248,  2870,    24,  1106,    76,    31,  1854,    30, 19828,\n",
      "          8918,     3]], device='cuda:0')\n",
      "tensor([[   23,   606,    11,     8, 16676,     2,    29,  4828,    29,     4,\n",
      "           252,   463,  1726,  3829,   308,     9,     8,  9862,  3634,     2,\n",
      "            43,    22,    21,    40,  4486,    51,     3]], device='cuda:0')\n",
      "tensor([[14082,    34, 11393,     4,  1138,    20,    40,  9032, 12285, 11403,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[  62,   11, 7432,    2,    5,   16,   21,   44,   22,  413,    6,   27,\n",
      "            3]], device='cuda:0')\n",
      "tensor([[ 3703,     4,   147,    21,   118,  6851, 12908,     7,     0,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77,   293,   198,    46,    82,  1753, 12302,     2,    36,    83,\n",
      "           578,     7,  2976,  6241,  3459,   461,   652,     6,     8,  7326,\n",
      "          2398,     3]], device='cuda:0')\n",
      "tensor([[15722,    46,     8,   550,    59,  3261,  5930,     4,  2320,     7,\n",
      "             8,   290,     5,    60,   130,     3]], device='cuda:0')\n",
      "tensor([[ 8372, 19711,    46,     8, 11288,    12,  4074,  3922,    16, 13961,\n",
      "            89,  8128,  4808,     2,     5,     9,    28, 11783,  1180,  3651,\n",
      "             2,   116,    21,    29, 20373, 13053,    29,   116,    31,     9,\n",
      "         17889,     3]], device='cuda:0')\n",
      "tensor([[   23,   550,    46,    40,  8156, 14035,    16,    39,  3900,   510,\n",
      "            20,     8,  2858,   750,     9,     4,     0,  1479,     2,     4,\n",
      "          5886,  1524,  1554,     5,     4,  8959,     0,     5,  1006,  2833,\n",
      "             0,   844,   637,    42,   107,     8,  7132,    56,     7,     4,\n",
      "          4863,     7,  2441,     3]], device='cuda:0')\n",
      "tensor([[ 1009,    68,    19,    47,    41,   133,     0, 13249,    21,     0,\n",
      "            95,    50,  3612,     7,  2914,  1367,    97,   118,  3024, 15437,\n",
      "             2,    53,   305,   254,   107,     8,  9805,  1944,   153,   447,\n",
      "             6,   641,     3]], device='cuda:0')\n",
      "tensor([[   23,   414,     5,  1590,    24, 18757,     3]], device='cuda:0')\n",
      "tensor([[19834,    21,  1418,     5, 17603,  1007,    32,     8,   125,     7,\n",
      "         15550,    98,    35, 10024,    64,     4,  6623,    19,    79,     8,\n",
      "           276,   220,    16,   745,    41,   156,   296,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,   164,    20, 13653, 11936,  4579,  8844,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    73,   115,  5649,     8,   219,  2903, 15723,     6,   282,\n",
      "          9250,    21,   291,     2,    67,    11,     8,  5676,     9,     5,\n",
      "             7,   948,     3]], device='cuda:0')\n",
      "tensor([[ 3667,    43, 16751,  4478,     3]], device='cuda:0')\n",
      "tensor([[3171,  110,  108,   12,   76,   22,   21,  506,    6,   47,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[19601,  4467,   158,    50, 13168,    97,  5128,   158,    20,     8,\n",
      "          1236,  6348, 13769,     7, 19598,  4572,     5, 14750,   360,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[11557,     2,    29,     9, 19555,     2,  2030,     8,  8741,    20,\n",
      "            40,  2867,  8657,     5,     8,   302,  1182,    12,  9183,  9379,\n",
      "          4332,    56,     7,   973,     5,  2611,    16,   274,  1682,   875,\n",
      "         13339,     5, 13964,     3]], device='cuda:0')\n",
      "tensor([[  62,  115,  875,  215,   26, 3115,  484,   68,   19,  165,   36,    8,\n",
      "         1274,    2,  140,   22,  667, 6047,    7,  234, 1261,   26,    8, 7295,\n",
      "         3004,    3]], device='cuda:0')\n",
      "tensor([[   77,  5507,     2, 13653,  1021,     2, 18535,     2, 14591,  7037,\n",
      "          7718,    16,  7746,     8, 10794,     7, 11665,     5, 11174,     0,\n",
      "          6442,   814,     3]], device='cuda:0')\n",
      "tensor([[   62,  6039,   527,  5483,    29,    40,  3925,     2, 12992,  1138,\n",
      "           670,     3]], device='cuda:0')\n",
      "tensor([[   23,  3650,    21,  1548,    11,     8,  8021,     7, 18421,    16,\n",
      "          8973,   102,  2938,     7, 14746,     3]], device='cuda:0')\n",
      "tensor([[ 8377,  1247,    89,   567,     5,    89,   409,  1059,     6,   703,\n",
      "           136, 19613,    21,  2428,     2,   148,   325,     4,  1138,    11,\n",
      "           548,  1024, 15851,     3]], device='cuda:0')\n",
      "tensor([[  23,  606,  115, 1528, 2985,    9,  102, 5470,  457,   34,   43,   22,\n",
      "          774, 4213,   97,   16,    2,    6, 4153, 2512,   16,  400,    4, 5769,\n",
      "            7,    4, 2565, 6225]], device='cuda:0')\n",
      "tensor([[   77, 11286,  6168,     5,     0, 12683,     7,     4, 17929,     7,\n",
      "            51,   727,    26,     4,  1026,     7,     4,  9672,  2217,     7,\n",
      "          5358,     3]], device='cuda:0')\n",
      "tensor([[  272,     7,     4, 11049,   719,    18,  4114,  1479,    13,   171,\n",
      "           454,     9,     8,   215,    75,     3]], device='cuda:0')\n",
      "tensor([[  189,    11,     8,  2725,     7,  1681,   976,   128,     2,     5,\n",
      "          3046,    16,  9831, 11280,    90,     3]], device='cuda:0')\n",
      "tensor([[13867,    21,  5224,     7,  1994,     5,     4,   177,     7,     0,\n",
      "          3458,   529,   366,    97,     8,    92,  1139,    16,  4953,     6,\n",
      "            27,  1217,    29,     8,   440,  7964,     6,     4,   163,  1310,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          4738,     3]], device='cuda:0')\n",
      "tensor([[  23, 4416,   24, 3172,    5,   39, 3493,    8, 5708,   20,  510,   72,\n",
      "           21,  315,   79,  268, 6522,    3]], device='cuda:0')\n",
      "tensor([[ 2635,  3818, 12900,     9,     8,  4700,     2,     0,    37]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 666, 8740,    0,   11,  139,   37]], device='cuda:0')\n",
      "tensor([[  211, 15721,    29,   102,   986,  3458,     3]], device='cuda:0')\n",
      "tensor([[   23,  1180,    24,   620,   108,    26,    28,   476,     7,   270,\n",
      "             2,  4131,     4,  1788,     2,    72,    32,     8,  3075,    12,\n",
      "         14373,     2,    99,     6, 11454,    90,     3]], device='cuda:0')\n",
      "tensor([[   23, 19161,   981,   175,     6,  1875,  3689,  1180,   307,   194,\n",
      "            11,     8,  5948,  5433,    30,     4,   113,    50,  8228,  2953,\n",
      "             7,     4,     0,   170,     8,  3514,   213,     7,    58,   130,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   62,    21,    40, 13342,  2148,    16, 14066,   447,    26,     4,\n",
      "          5722,  1724,   202,    87,  7373,   102,  5407,    12,   172,    72,\n",
      "           169,   213,     3]], device='cuda:0')\n",
      "tensor([[9172,  136,    6,  488,   16, 8320,   11, 8799,    6, 9844,    8,  646,\n",
      "         1136,   29,    8, 1355,  386, 9799, 8741,    3]], device='cuda:0')\n",
      "tensor([[17886, 12633,   151,     4,  3459,    21,   625, 12491,   158,   332,\n",
      "            22,    21, 11707,    25,     4, 20009,     7,   102,  3024,  1476,\n",
      "             2,     5,  4295,     3]], device='cuda:0')\n",
      "tensor([[   62,    46,     4,  1068,     6,  9876,     5,   280,   176,   522,\n",
      "             2,    43,    22,  2650,    19,    20,   102, 19175,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[17050,    25, 16234,     5,  2014,    21,  4161,  3939,     2,     4,\n",
      "           606,    21,   360,  2974,     9,   102,  5769,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[2806,   11,  490,    5,  572,    3]], device='cuda:0')\n",
      "tensor([[  62,   21,    8, 5050,    5,  429, 5613, 2419,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  227,  1046, 19907,     9,   837,  1278,    46,     4, 15832,     7,\n",
      "         13567,    55,   181,  1637,   484,     3]], device='cuda:0')\n",
      "tensor([[   77, 10175,     5,  5483,   414,    52,  2339,     2,  1240,     7,\n",
      "           624,   719,   136,    18,     8, 11866,    43,  6348,  2419,     7,\n",
      "             4,   633,     3]], device='cuda:0')\n",
      "tensor([[   23,  9711, 11250, 10098,     4, 13368,  3047,   222, 17601,     5,\n",
      "         11819,     3]], device='cuda:0')\n",
      "tensor([[  418,   250,  1539,  4293,     9, 14288,   472,    46,    40,  3599,\n",
      "           164,   159,     8,   854,  4098,     9,    60,  6869,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 7090,    30,  2728, 12904,     2,    28,    11,     4,   195,   550,\n",
      "            19,   495,  1283,     9,  2556,     2,  1473,    16,    22,   574,\n",
      "           148,   216,     3]], device='cuda:0')\n",
      "tensor([[ 7256,   251,  3651,     6, 10935,  4676, 20122,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,  7463, 14272,    21,  1576,    29,     8,   606,  3949,    72,\n",
      "         14124, 10436,  1822,  9998,     6,     4,   260,     7,  5128,  4160,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[ 7879,    21,   261,  4295,   158, 20578,     2,  9349,     5, 16215,\n",
      "           158,  7263,     9,  8202,     2, 14643,  3939,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  77, 9591,   64, 2078,   19,  171,  454,    3]], device='cuda:0')\n",
      "tensor([[  226,  7005,  3138,  3158,     2,  2112, 12430,     5,     8,  3070,\n",
      "          1948,     7,  4978,     2,  2828,    21,   606,    11,    51,     7,\n",
      "          1058,    21,     0,  1728,  7019,     3]], device='cuda:0')\n",
      "tensor([[  764, 14743,    11,     8,   110,   660,  1749,     9,    60,  2256,\n",
      "             2,     5,   182,   570,   391,    62,   189,    38]],\n",
      "       device='cuda:0')\n",
      "tensor([[  135,    27,   454,    26,     4,   110,   358,    12,   102, 12295,\n",
      "             7, 17466,  4255,     3]], device='cuda:0')\n",
      "tensor([[18918,  1145, 18894, 13432,     4,  4947, 11430,   237,     9,   579,\n",
      "             3]], device='cuda:0')\n",
      "tensor([[   62,    21,     8, 13727,  9866,     3]], device='cuda:0')\n",
      "tensor([[  74,   11,    8,  108, 2870,    2,  108, 4647,    2, 1346,  148,   12,\n",
      "         2441,    3]], device='cuda:0')\n",
      "tensor([[   23,  1476,    24,   836,     5,   429,   110, 10447,  4390,    30,\n",
      "          1417,     6, 13070,     3]], device='cuda:0')\n",
      "tensor([[  23,  606,   39,  308, 3784,  129,   18,  214,    4,  966,    5, 3292,\n",
      "         5170,    3]], device='cuda:0')\n",
      "tensor([[ 8393,     2, 10894,     2,     5,  6918,     9,     8,   770,     2,\n",
      "         16643,   694,    16,   370,   102,  1476,   875,  2658,  7050,   148,\n",
      "            80,   102,  2870,    11,    36,     3]], device='cuda:0')\n",
      "tensor([[  418,     8,   606,     6,  6013,   185,  1065,     2,    43,     8,\n",
      "           895,   201, 19676,     6,   295,    35,  6690, 18069,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 2895,  4658,     7,  5274,   158,     5, 13839,   158,    26, 12587,\n",
      "            11,     8, 12727,  3617,    16,  3899,     6,   470,     7,     4,\n",
      "         13742,     3]], device='cuda:0')\n",
      "tensor([[ 7644,    19,     4,  4835,  6059,     7,   194,     9,     8,   573,\n",
      "           446,  3035,   151,     4,   923,     7,     8,  1138,    72,     2,\n",
      "             9,  6109,     7,  6633,   956,     5,  2350, 19172,     2,  1300,\n",
      "             9,    60,  5061,    16,    53,    11,    51,     7,     4, 14992,\n",
      "           518,  2920,     3]], device='cuda:0')\n",
      "tensor([[17216,     2,  1339,     0,  2237,  4808,    46, 10228,  5159,  4743,\n",
      "            34,  2042,    40,  1650, 11979,  6334,     6,    89,   972,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[182, 570, 391,  62, 189,  38]], device='cuda:0')\n",
      "tensor([[ 11,  36, 393,   3]], device='cuda:0')\n",
      "tensor([[   62, 12313,    19,     2,    19,   301,    41,  1676,    22,     2,\n",
      "            19,  6729,   102,  7951,     5,    24,   319,     6,  4750,    83,\n",
      "             7,     4, 16609,    19,    79,   202,  1331,    22,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   68,    19,    24,    40,  3599,    72,    42,  4832,     6,     4,\n",
      "           416,    12,  3160,  1855,    25,  5648,  8946,     4,   973,     7,\n",
      "           426, 10698,     2,   119,  8479,    21,   414,    11,     8,  5483,\n",
      "          4958,    12,  1501,     3]], device='cuda:0')\n",
      "tensor([[ 1316,     4,   471,     7,     4,  3651,  9329,    29,     4,   550,\n",
      "          5727,     2,    81,    21,    94,  7684,     4,   447,     7,  1331,\n",
      "          2435, 13480,     5,  3273,   316,   447,     3]], device='cuda:0')\n",
      "tensor([[ 8518, 18584,    46,   128,     8,  7718,   242, 11597,     2,    43,\n",
      "            16,    21,  3010,     3]], device='cuda:0')\n",
      "tensor([[  143,   550,  4806,    29,   129,    29,  5989,  4806,     2, 15645,\n",
      "            11,     8,   302,  2148,     3]], device='cuda:0')\n",
      "tensor([[20679, 10293,     5,  5252]], device='cuda:0')\n",
      "tensor([[16014,    21,  5483,  8392,     7,  6684,     5,     4,  1039,   412,\n",
      "             7, 10263,     6,   956,     3]], device='cuda:0')\n",
      "tensor([[12307,     2, 13363, 13925,     5, 20377,  1847,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  167,    22,    21,   179,   215,     5,   179, 12265,     5,    22,\n",
      "          2215,     9,     8, 14837,     3]], device='cuda:0')\n",
      "tensor([[  23, 7293,  801, 3939, 6573,  750,    5, 9472,    2,   43,    4, 2419,\n",
      "           11,  395,   50,  836,   97,    4,  901, 3182,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77,  9789,  1346, 13104,     6,   444,  2249,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[1095, 7702,   45,  885,   12,   51,    7,    4,  147,   21,  163, 2268,\n",
      "            3,   55]], device='cuda:0')\n",
      "tensor([[ 1966,    55,  4215,   606, 10723, 11553,    21,   748,     6, 10813,\n",
      "          3651,   144,     4,  4597,     7,    60,  5281, 10734,     2,  4654,\n",
      "         11509,  3700,  3878,    55,  3742,     6,    84,    60,  1021,    26,\n",
      "             4,   289,    75,     3]], device='cuda:0')\n",
      "tensor([[  212, 13906,   108,    39,     7,     4,   977,     5,  1095,  8282,\n",
      "            55, 13808,    21, 18540,     7,  6983,     2,   107,     4,   606,\n",
      "          5532,  2062,    83, 15335,     3]], device='cuda:0')\n",
      "tensor([[   23,   550,    11,    12,  1208,    72,   301,    41,   638,  3157,\n",
      "          4670,     2,     5,     4, 13281,  4141,  1044,    98,    22,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   23,  8554,  6309,     7,  1680,  4896,    11,     4,   818,     7,\n",
      "            28, 16830,     2, 12894,   606,    16,    11,    59,     0,    16,\n",
      "            22,   582,  7000,  1180,     9,   102,  9247,     5,  8755,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   77, 11876,  4390,     2,  1236, 16775,   606,     2,     5,    40,\n",
      "         14108,   748,     6,   457,    16,    42,  6895,  1863,   734, 15001,\n",
      "         14936,    21,  3959,   130,     3]], device='cuda:0')\n",
      "tensor([[ 893,    0,   21, 2870,   11, 7819, 3864,   20, 2188, 3099,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  77, 9367, 5483,    5, 9480, 6168, 5128, 3617,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[13930,     5,  1547,    54,     8,   606,    16, 13653,  8220,   710,\n",
      "             5,  5224,    29,    22,  2695,    19,  6160,     5,   299,     4,\n",
      "          3075,   426,    32,    12,    58,   130,     3]], device='cuda:0')\n",
      "tensor([[19530,   637,   158,   637,    72,   282,     2,   637,    72,  1853,\n",
      "           158,    39,    27, 12718,    25,     4,   125,    22,  1792,    20,\n",
      "           289,   647,    64,   855,     5,  7519,     3]], device='cuda:0')\n",
      "tensor([[ 6226,    21, 14217,   684,  4192,  5511,    50,  1501,    97,    95,\n",
      "          1095,  1677,    55,   266,     2,     5,  3439,  9546,    58,   200,\n",
      "          9196,   194,   827,   135,   641,   552,  2023,   103,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  34,    8,  108,    2,   68,   36, 2658, 1328,    2,  204,   26, 1001,\n",
      "            3]], device='cuda:0')\n",
      "tensor([[  23,  606,   11, 1495,    2, 3418,    5, 1346,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  91,  745,   41, 1584,  102, 2206,    2,   43,   19,  197,   27, 7873,\n",
      "           29,  129,    3]], device='cuda:0')\n",
      "tensor([[  105,    19,  5152,    18,   681, 15941,    21,   539,  4907,    34,\n",
      "          1942,    28,   550,     5,   699,    37]], device='cuda:0')\n",
      "tensor([[   23,  8005,  1344,     7,    28,   606,   582,     6,  5406,    61,\n",
      "            30,     4,  3327,  4844,  1278,     7,     4, 13057,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[  62,   21,   64, 1331,    8, 6623,  164, 5021,    3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    11,     4,  5606,     2,  7485,     2,     0,     2,     0,\n",
      "         19276,     7,  6284,    21,  3436,     6,   567,     5, 17348,    60,\n",
      "          8053,    16,   370,     4,   550,     8, 10203,     5,  5532,  3559,\n",
      "             2,  2062,   102, 14414,  1402,     3]], device='cuda:0')\n",
      "tensor([[   77,  2913,     2, 17813,  2639,    30,   965,    16, 12556,  8144,\n",
      "           313,     2,  1737,     2,   587,     2,     5,   251,  3617,     3]],\n",
      "       device='cuda:0')\n",
      "tensor([[   62,    21,   349,     6,  2126,  3488, 16181,   170,   216,    97,\n",
      "            53,    11,     9,    28,   684,     3]], device='cuda:0')\n",
      "tensor([[ 3135,     4, 14169,  4432,     2,    23,  3057, 19317,  1583,   370,\n",
      "            12,     8,  5496,  9652,   330,     3]], device='cuda:0')\n",
      "tensor([[   34,     8,   447,   201, 19838,     2,  1603,   639,    25,     4,\n",
      "           866,  2134,     7,  4116,  7547,     3]], device='cuda:0')\n",
      "tensor([[    0, 19789,     0,   370,   136,   133,  2168,    52,  5777,    48,\n",
      "           114,   196,   136,   217,   157,     3]], device='cuda:0')\n",
      "tensor([[2824,   99, 1041,  484,  215,    2,    4,  606,   11, 3864,   20,  173,\n",
      "            5, 7600,    3]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-e5391c4d3562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt_deep_cbow_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m PTDbow_losses, PTDbow_accuracies = train_model(\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mpt_deep_cbow_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     print_every=1000, eval_every=1000)\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;31m# evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miter_i\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n\u001b[0m\u001b[0;32m     67\u001b[0m                                          batch_fn=batch_fn, prep_fn=prep_fn)\n\u001b[0;32m     68\u001b[0m                 \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-1c72e7511611>\u001b[0m in \u001b[0;36msimple_evaluate\u001b[1;34m(model, data, prep_fn, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# convert the example input and label to PyTorch tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m# forward pass without backpropagation (no_grad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# get the output from the neural network for input x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m# All strings are unicode in Python 3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    349\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                     \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_dtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mvalue_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a Deep CBOW model with pre-trained embeddings\n",
    "# YOUR CODE HERE\n",
    "# pt_deep_cbow_model = ..\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # copy pre-trained word vectors into embeddings table\n",
    "# pt_deep_cbow_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "\n",
    "\n",
    "embed_w = torch.from_numpy(vectors)\n",
    "\n",
    "\n",
    "\n",
    "pt_deep_cbow_model = PTDeepCBOW( len(v.w2i), 300, hidden_dim=100, output_dim = 5, vocab=v,\n",
    "                                embedding_w = embed_w)\n",
    "\n",
    "# move model to specified device\n",
    "pt_deep_cbow_model = pt_deep_cbow_model.to(device)\n",
    "# train the model\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(pt_deep_cbow_model.parameters(), lr=0.0005)\n",
    "\n",
    "PTDbow_losses, PTDbow_accuracies = train_model(\n",
    "    pt_deep_cbow_model, optimizer, num_iterations=10000, \n",
    "    print_every=1000, eval_every=1000)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ufujv3x31ufD"
   },
   "outputs": [],
   "source": [
    "# plot dev accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTJtKBzd7Qjr"
   },
   "outputs": [],
   "source": [
    "# plot train loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFu8xzCy9XDW"
   },
   "source": [
    "**It looks like we've hit what is possible with just using words.**\n",
    "Let's move on by incorporating word order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g41yW4PL9jG0"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODzXEH0MaGpa"
   },
   "source": [
    "It is time to get more serious. Even with pre-trained word embeddings and multiple layers, we still seem to do pretty badly at sentiment classification. \n",
    "The next step we can take is to introduce word order again, dropping our independence assumptions. In this way, we can get a representation of the sentence as an ordered set of tokens.\n",
    "\n",
    "We will get this representation using a **Long Short-Term Memory** (LSTM). As an exercise, we will code our own LSTM cell, so that we get comfortable with its inner workings.\n",
    "Once we have an LSTM cell, we can call it repeatedly, updating its hidden state one word at a time:\n",
    "\n",
    "```python\n",
    "rnn = MyLSTMCell(input_size, hidden_size)\n",
    "\n",
    "hx = torch.zeros(1, hidden_size)  # initial hidden state\n",
    "cx = torch.zeros(1, hidden_size)  # initial memory cell\n",
    "output = []                       # to save intermediate LSTM states\n",
    "\n",
    "# feed one word at a time\n",
    "for i in range(n_timesteps):\n",
    "  hx, cx = rnn(input[i], (hx, cx))\n",
    "  output.append(hx)\n",
    "```\n",
    "\n",
    "If you need some more help understanding LSTMs, you can check out these resources:\n",
    "- Blog post (highly recommended): http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Paper covering LSTM formulas in detail: https://arxiv.org/abs/1503.04069 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9f4b45BXKFC"
   },
   "source": [
    "#### Exercise: Finish the LSTM cell below. \n",
    "You will need to implement the LSTM formulas:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "        i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
    "        f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
    "        g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
    "        o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
    "        c' = f * c + i * g \\\\\n",
    "        h' = o \\tanh(c') \\\\\n",
    "\\end{array}\n",
    " $$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "*Note that the LSTM formulas can differ slightly between different papers. We use the PyTorch LSTM formulation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "zJ9m5kLMd7-v"
   },
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "  \"\"\"Our own LSTM cell\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "    super(MyLSTMCell, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size    \n",
    "    #print( self.input_size )\n",
    "#print(self.hidden_size)\n",
    "       \n",
    "    self.W_ii = nn.Parameter(torch.empty((self.hidden_size, self.input_size)))\n",
    "    self.W_hi = nn.Parameter(torch.empty((self.hidden_size, self.hidden_size)))\n",
    "    self.B_ii = nn.Parameter(torch.empty(self.hidden_size))\n",
    "    self.B_hi = nn.Parameter(torch.empty(self.hidden_size))\n",
    "    \n",
    "    self.W_if = nn.Parameter(torch.empty((self.hidden_size,self.input_size)))\n",
    "    self.W_hf = nn.Parameter(torch.empty((self.hidden_size, self.hidden_size)))\n",
    "    self.B_if = nn.Parameter(torch.empty(self.hidden_size))\n",
    "    self.B_hf = nn.Parameter(torch.empty(self.hidden_size))\n",
    "\n",
    "    self.W_ig = nn.Parameter(torch.empty((self.hidden_size, self.input_size)))\n",
    "    self.W_hg = nn.Parameter(torch.empty((self.hidden_size, self.hidden_size)))\n",
    "    self.B_ig = nn.Parameter(torch.empty(self.hidden_size))\n",
    "    self.B_hg = nn.Parameter(torch.empty(self.hidden_size))\n",
    "\n",
    "    self.W_io = nn.Parameter(torch.empty((self.hidden_size, self.input_size)))\n",
    "    self.W_ho = nn.Parameter(torch.empty((self.hidden_size, self.hidden_size)))\n",
    "    self.B_io = nn.Parameter(torch.empty(self.hidden_size))\n",
    "    self.B_ho = nn.Parameter(torch.empty(self.hidden_size))\n",
    "\n",
    "\n",
    "    self.reset_parameters()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "    \n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "        weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "  def forward(self, input_, hx, mask=None):\n",
    "    \"\"\"\n",
    "    input is (batch, input_size)\n",
    "    hx is ((batch, hidden_size), (batch, hidden_size))\n",
    "    \"\"\"\n",
    "    \n",
    "    prev_h, prev_c = hx\n",
    "    \n",
    "\n",
    "    # project input and prev state\n",
    "\n",
    "    # main LSTM computation   \n",
    "    \n",
    "    #print(torch.mean(self.W_ii))\n",
    "    i = torch.sigmoid(input_ @ torch.transpose(self.W_ii,0,1) + self.B_ii +prev_h @ self.W_hi + self.B_hi)\n",
    "    f = torch.sigmoid(input_ @ torch.transpose(self.W_if,0,1) + self.B_if + prev_h @ self.W_hf + self.B_hf)\n",
    "    g = torch.tanh(input_ @ torch.transpose(self.W_ig,0,1) + self.B_ig + prev_h@ self.W_hg + self.B_hg)\n",
    "    o = torch.sigmoid(input_@ torch.transpose(self.W_io ,0,1) + self.B_io + prev_h @ self.W_ho + self.B_ho)\n",
    "    #print(i,f,g,o)\n",
    "    \n",
    "  #  print(\"H\", prev_h.size())\n",
    "  #  print(f.size())\n",
    "##  print(\"OLD\",prev_c.size() )\n",
    "   # print(i.size())\n",
    "\n",
    "    c = f * prev_c + i * g\n",
    "   # print(c.size())\n",
    "    h = o * torch.tanh(c)\n",
    "\n",
    "\n",
    "    return h, c\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return \"{}({:d}, {:d})\".format(\n",
    "        self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JM7xPhkQeE5"
   },
   "source": [
    "#### Optional: Efficient Matrix Multiplication\n",
    "\n",
    "It is more efficient to do a few big matrix multiplications than to do many smaller ones. So we will implement the above cell using just **two** linear layers.\n",
    "\n",
    "This is possible because the eight linear transformations contained in one forward pass through an LSTM cell can be reduced to just two:\n",
    "$$W_h h + b_h$$\n",
    "$$W_i x + b_i $$ \n",
    "\n",
    "with $h = $ `prev_h` and $x = $ `input_`.\n",
    "\n",
    "and where: \n",
    "\n",
    "$W_h =  \\begin{pmatrix}\n",
    "W_{hi}\\\\ \n",
    "W_{hf}\\\\ \n",
    "W_{hg}\\\\ \n",
    "W_{ho}\n",
    "\\end{pmatrix}$, $b_h = \\begin{pmatrix}\n",
    "b_{hi}\\\\ \n",
    "b_{hf}\\\\ \n",
    "b_{hg}\\\\ \n",
    "b_{ho}\n",
    "\\end{pmatrix}$,  $W_i = \\begin{pmatrix}\n",
    "W_{ii}\\\\ \n",
    "W_{if}\\\\ \n",
    "W_{ig}\\\\ \n",
    "W_{io}\n",
    "\\end{pmatrix}$ and $b_i = \\begin{pmatrix}\n",
    "b_{ii}\\\\ \n",
    "b_{if}\\\\ \n",
    "b_{ig}\\\\ \n",
    "b_{io}\n",
    "\\end{pmatrix}$.\n",
    "\n",
    "Convince yourself that, after chunking with [torch.chunk](https://pytorch.org/docs/stable/torch.html?highlight=chunk#torch.chunk), the output of those two linear transformations is equivalent to the output of the eight linear transformations in the LSTM cell calculations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9gA-UcqSBe0"
   },
   "source": [
    "#### LSTM Classifier\n",
    "\n",
    "Having an LSTM cell is not enough: we still need some code that calls it repeatedly, and then makes a prediction from the final hidden state. \n",
    "You will find that code below. Make sure that you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "3iuYZm5poEn5"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "  \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "    super(LSTMClassifier, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "    self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
    "    \n",
    "    self.output_layer = nn.Sequential(     \n",
    "        nn.Dropout(p=0.5),  # explained later\n",
    "        nn.Linear(hidden_dim, output_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
    "    T = x.size(1)  # timesteps (the number of words in the sentence)\n",
    "    #print(x)\n",
    "    input_ = self.embed(x)\n",
    "\n",
    "    # here we create initial hidden states containing zeros\n",
    "    # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
    "    hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "    cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "    \n",
    "    # process input sentences one word/timestep at a time\n",
    "    # input is batch-major (i.e., batch size is the first dimension)\n",
    "    # so the first word(s) is (are) input_[:, 0]\n",
    "    outputs = []   \n",
    "    for i in range(T):\n",
    "        \n",
    "        #sys.exit()\n",
    "        hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
    "        outputs.append(hx)\n",
    "    \n",
    "    # if we have a single example, our final LSTM state is the last hx\n",
    "    if B == 1:\n",
    "        final = hx\n",
    "    else:\n",
    "      #\n",
    "      # This part is explained in next section, ignore this else-block for now.\n",
    "      #\n",
    "      # We processed sentences with different lengths, so some of the sentences\n",
    "      # had already finished and we have been adding padding inputs to hx.\n",
    "      # We select the final state based on the length of each sentence.\n",
    "      \n",
    "      # two lines below not needed if using LSTM from pytorch\n",
    "        outputs = torch.stack(outputs, dim=0)           # [T, B, D]\n",
    "        outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
    "\n",
    "        # to be super-sure we're not accidentally indexing the wrong state\n",
    "        # we zero out positions that are invalid\n",
    "        pad_positions = (x == 1).unsqueeze(-1)\n",
    "\n",
    "        outputs = outputs.contiguous()      \n",
    "        outputs = outputs.masked_fill_(pad_positions, 0.)\n",
    "\n",
    "        mask = (x != 1)  # true for valid positions [B, T]\n",
    "        lengths = mask.sum(dim=1)                 # [B, 1]\n",
    "\n",
    "        indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
    "        final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
    "    \n",
    "    # we use the last hidden state to classify the sentence\n",
    "    logits = self.output_layer(final)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxFoVpvMPB6g"
   },
   "source": [
    "#### Dropout\n",
    "\n",
    "Data sparsity and a small data set can cause *overfitting*. This is a phenomenom that is very likely to occur when training strong and expressive models, like LSTMs, on small data. In practice, if your model overfits, this means that it will be very good at predicting (or \"remembering\") the sentiment of the training set, but unable to generalise to new, unseen data in the test set. This is undesirable and one technique to mitigate this issue is *dropout*. \n",
    "\n",
    "A dropout layer is defined by the following formula, which can be applied, for example, to a linear layer:\n",
    "\n",
    "$$\\text{tanh}(W(\\mathbf{h}\\odot \\mathbf{d}) + \\mathbf{b})$$\n",
    "\n",
    "where $\\mathbf{d} \\in \\{0, 1\\}^n$, with $d_j \\sim \\text{Bernoulli}(p)$, \n",
    "\n",
    "These formula simply means that we *drop* certain parameters during training (by setting them to zero). Which parameters we drop is stochastically determined by a Bernoulli distribution and the probability of each parameter being dropped is set to $p = 0.5$ in our experiments (see the previous cell of code where we define our output layer). A dropout layer can be applied at many different places in our models. This technique helps against the undesirable effect that a model relies on single parameters for prediction (e.g. if $h^{\\prime}_j$ is large, always predict positive). If we use dropout, the model needs to learn to rely on different parameters, which is desirable to obtain better generalisation to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQjEjLt9z0XW"
   },
   "source": [
    "**Let's train our LSTM!** Note that is will be a lot slower than previous models because we need to do many more computations per sentence.\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20727\n",
      "20727\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(vectors))\n",
    "print(len(v.w2i))\n",
    "print(len(t2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "LgZoSPD4fsf_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "rnn.W_ii                 [168, 300]   requires_grad=True\n",
      "rnn.W_hi                 [168, 168]   requires_grad=True\n",
      "rnn.B_ii                 [168]        requires_grad=True\n",
      "rnn.B_hi                 [168]        requires_grad=True\n",
      "rnn.W_if                 [168, 300]   requires_grad=True\n",
      "rnn.W_hf                 [168, 168]   requires_grad=True\n",
      "rnn.B_if                 [168]        requires_grad=True\n",
      "rnn.B_hf                 [168]        requires_grad=True\n",
      "rnn.W_ig                 [168, 300]   requires_grad=True\n",
      "rnn.W_hg                 [168, 168]   requires_grad=True\n",
      "rnn.B_ig                 [168]        requires_grad=True\n",
      "rnn.B_hg                 [168]        requires_grad=True\n",
      "rnn.W_io                 [168, 300]   requires_grad=True\n",
      "rnn.W_ho                 [168, 168]   requires_grad=True\n",
      "rnn.B_io                 [168]        requires_grad=True\n",
      "rnn.B_ho                 [168]        requires_grad=True\n",
      "output_layer.1.weight    [5, 168]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6534785\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-8a1f47c0ca8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m lstm_losses, lstm_accuracies = train_model(\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     print_every=250, eval_every=1000)\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "# copy pre-trained word vectors into embeddings table\n",
    "#print(len(vectors))\n",
    "\n",
    "\n",
    "\n",
    "###UNCOMMENT IF PRETRAINED\n",
    "with torch.no_grad():\n",
    "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "    lstm_model.embed.weight.requires_grad = False\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=25000, \n",
    "    print_every=250, eval_every=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BKVnyg0Hq5E"
   },
   "outputs": [],
   "source": [
    "# plot validation accuracy\n",
    "plt.plot(lstm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZowTV0EBTb3z"
   },
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "plt.plot(lstm_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEw6XHQY_AAQ"
   },
   "source": [
    "# Mini-batching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPf96wGzBTQJ"
   },
   "source": [
    "**Why is the LSTM so slow?** Despite our best efforts, we still need to make a lot of matrix multiplications per example (linear in the length of the example) just to get a single classification, and we can only process the 2nd word once we have computed the hidden state for the 1st word (sequential computation).\n",
    "\n",
    "GPUs are more efficient if we do a few big matrix multiplications, rather than lots of small ones. If we could process multiple examples at the same time, then we could exploit that. That is, we could still process the input sequentially, but doing so for multiple sentences at the same time.\n",
    "\n",
    "Up to now our \"mini-batches\" consisted of a single example. This was for a reason: the sentences in our data sets have **different lengths**, and this makes it difficult to process them at the same time.\n",
    "\n",
    "Consider a batch of 2 sentences:\n",
    "\n",
    "```\n",
    "this movie is bad\n",
    "this movie is super cool !\n",
    "```\n",
    "\n",
    "Let's say the IDs for these sentences are:\n",
    "\n",
    "```\n",
    "2 3 4 5\n",
    "2 3 4 6 7 8\n",
    "```\n",
    "\n",
    "We cannot feed PyTorch an object with rows of variable length! We need to turn this into a matrix.\n",
    "\n",
    "The solution is to add **padding values** to our mini-batch:\n",
    "\n",
    "```\n",
    "2 3 4 5 1 1\n",
    "2 3 4 6 7 8\n",
    "```\n",
    "\n",
    "Whenever a sentence is shorter than the longest sentence in a mini-batch, we just use a padding value (here: 1) to fill the matrix.\n",
    "\n",
    "In our computation, we should **ignore** the padding positions (e.g. mask them out) so that paddings do not contribute to the loss.\n",
    "\n",
    "#### Mini-batch feed\n",
    "We will now implement a `get_minibatch` function which will replace `get_example` and returns a mini-batch of the requested size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "IoAE2JBiXJ3P"
   },
   "outputs": [],
   "source": [
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "  \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "  \n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  \n",
    "  batch = []\n",
    "  \n",
    "  # yield minibatches\n",
    "  for example in data:\n",
    "    batch.append(example)\n",
    "    \n",
    "    if len(batch) == batch_size:\n",
    "      yield batch\n",
    "      batch = []\n",
    "      \n",
    "  # in case there is something left\n",
    "  if len(batch) > 0:\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwZM-XYkT8Zx"
   },
   "source": [
    "#### Padding function\n",
    "We will need a function that adds padding 1s to a sequence of IDs so that\n",
    "it becomes as long as the longest sequence in the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "sp0sK1ghw4Ft"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 1, 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad(tokens, length, pad_value=1):\n",
    "  \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
    "  return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "# example\n",
    "tokens = [2, 3, 4]\n",
    "pad(tokens, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL2iixMYUgfh"
   },
   "source": [
    "#### New `prepare` function\n",
    "\n",
    "We will also need a new function that turns a mini-batch into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "ZID0cqozWks8"
   },
   "outputs": [],
   "source": [
    "def prepare_minibatch(mb, vocab, Shuffle = False):\n",
    "  \"\"\"\n",
    "  Minibatch is a list of examples.\n",
    "  This function converts words to IDs and returns\n",
    "  torch tensors to be used as input/targets.\n",
    "  \"\"\"\n",
    "  batch_size = len(mb)\n",
    "  maxlen = max([len(ex.tokens) for ex in mb])\n",
    "  \n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
    "  \n",
    "  x = torch.LongTensor(x)\n",
    "  x = x.to(device)\n",
    "  \n",
    "  y = [ex.label for ex in mb]\n",
    "  y = torch.LongTensor(y)\n",
    "  y = y.to(device)\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "OwDAtCv1x2hB"
   },
   "outputs": [],
   "source": [
    "# Let's test our new function.\n",
    "# This should give us 3 examples.\n",
    "mb = next(get_minibatch(train_data, batch_size=3, shuffle=False))\n",
    "for ex in mb:\n",
    "    None\n",
    "#     print(ex)\n",
    "#     print(ex.tokens)\n",
    "#     print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "dg8zEK8zyUCH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[ 2824,     4,   606,    11,  4367,     2,   102, 16508,    21,   633,\n",
      "            11,     9,     4,   162,   231,     2,    60,  7668,    12,  4516,\n",
      "             5,  7358,   710, 15064,     3,     1,     1,     1,     1,     1],\n",
      "        [   62,    21,  5532,     5,  5001,     5,  6039,    16,   148,     9,\n",
      "          8406,    19,    42,   168,  4255,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [ 2607,     7,   230,  1176,     5,  5753,    21,   118,  2342,  3047,\n",
      "             6, 11716,  9613,     2,  4067,  6796,   574,  3881,     2, 12833,\n",
      "            18,  1176,    21, 20343,     5,  5753,    21,  3424, 15568,     3]],\n",
      "       device='cuda:0')\n",
      "y tensor([3, 3, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# We should find padding 1s at the end\n",
    "x, y = prepare_minibatch(mb, v)\n",
    "print(\"x\", x)\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_prepare_minibatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-05d5880405ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_prepare_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPosVocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_prepare_minibatch' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = pos_prepare_minibatch(mb, PosVocab)\n",
    "print(\"x\", x)\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYBJEoSNUwI0"
   },
   "source": [
    "#### Evaluate (mini-batch version)\n",
    "\n",
    "We can now update our evaluation function to use mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "eiZZpEghzqou"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, \n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "  \"\"\"Accuracy of a model on given data set (using mini-batches)\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout\n",
    "\n",
    "  for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
    "    x, targets = prep_fn(mb, model.vocab)\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "      \n",
    "    predictions = logits.argmax(dim=-1).view(-1)\n",
    "    \n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (predictions == targets.view(-1)).sum().item()\n",
    "    total += targets.size(0)\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23wAZomozh_2"
   },
   "source": [
    "# LSTM (Mini-batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-gkPU7jzBe2"
   },
   "source": [
    "With this, let's run the LSTM again but now using mini-batches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "226Xg9OPzFbA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "rnn.W_ii                 [168, 300]   requires_grad=True\n",
      "rnn.W_hi                 [168, 168]   requires_grad=True\n",
      "rnn.B_ii                 [168]        requires_grad=True\n",
      "rnn.B_hi                 [168]        requires_grad=True\n",
      "rnn.W_if                 [168, 300]   requires_grad=True\n",
      "rnn.W_hf                 [168, 168]   requires_grad=True\n",
      "rnn.B_if                 [168]        requires_grad=True\n",
      "rnn.B_hf                 [168]        requires_grad=True\n",
      "rnn.W_ig                 [168, 300]   requires_grad=True\n",
      "rnn.W_hg                 [168, 168]   requires_grad=True\n",
      "rnn.B_ig                 [168]        requires_grad=True\n",
      "rnn.B_hg                 [168]        requires_grad=True\n",
      "rnn.W_io                 [168, 300]   requires_grad=True\n",
      "rnn.W_ho                 [168, 168]   requires_grad=True\n",
      "rnn.B_io                 [168]        requires_grad=True\n",
      "rnn.B_ho                 [168]        requires_grad=True\n",
      "output_layer.1.weight    [5, 168]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6534785\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-9af479e6f2ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m lstm_losses, lstm_accuracies = train_model(\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# later we will use B examples per update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-28b18ea1d35f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#sys.exit()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-c5d015dbd8f3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_, hx, mask)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#print(torch.mean(self.W_ii))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_ii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_ii\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mprev_h\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_if\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_if\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_h\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_ig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_ig\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMClassifier(\n",
    "    len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "# copy pre-trained vectors into embeddings table\n",
    "with torch.no_grad():\n",
    "  lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "  lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)  \n",
    "  \n",
    "lstm_model = lstm_model.to(device)\n",
    "\n",
    "batch_size = 25\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=30000, \n",
    "    print_every=250, eval_every=250,\n",
    "    batch_size=batch_size,\n",
    "    batch_fn=get_minibatch, \n",
    "    prep_fn=prepare_minibatch,\n",
    "    eval_fn=evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymj1rLDMvyhp"
   },
   "outputs": [],
   "source": [
    "# plot validation accuracy\n",
    "plt.plot(lstm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1je5S1RHVC5R"
   },
   "outputs": [],
   "source": [
    "# plot training loss\n",
    "plt.plot(lstm_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7WjcxXntMi5"
   },
   "source": [
    "# Tree-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyj_UD6GtO5M"
   },
   "source": [
    "In the final part of this lab we will exploit the tree structure of the SST data. \n",
    "Until now we only used the surface tokens, but remember that our data examples include binary trees with a sentiment score at every node.\n",
    "\n",
    "In particular, we will implement **N-ary Tree-LSTMs** which are described in:\n",
    "\n",
    "> Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf) ACL 2015.\n",
    "\n",
    "Since our trees are binary (i.e., N=2), we can refer to these as *Binary Tree-LSTMs*. If you study equations (9) to (14) in the paper, you will find that they are not all too different from the original LSTM that you already have.\n",
    "\n",
    "You should read this paper carefully and make sure that you understand the approach. You will also find our LSTM baseline there.\n",
    "Note however that Tree-LSTMs were proposed around the same time by two other groups:\n",
    "\n",
    "> Phong Le and Willem Zuidema. [Compositional distributional semantics with long short term memory](http://anthology.aclweb.org/S/S15/S15-1002.pdf). *SEM 2015.\n",
    "\n",
    "> Xiaodan Zhu, Parinaz Sobihani,  and Hongyu Guo. [Long short-term memory over recursive structures](http://proceedings.mlr.press/v37/zhub15.pdf). ICML 2015.\n",
    "\n",
    "It is good scientific practice to cite all three papers in your report.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rDzvSos3JFp"
   },
   "source": [
    "## Computation\n",
    "\n",
    "Do you remember the `transitions_from_treestring` function all the way in the beginning of this lab? Every example contains a **transition sequence** produced by this function. Let's look at it again:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "5pg0Xumc3ZUS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              3                                                                     \n",
      "  ____________|____________________                                                  \n",
      " |                                 4                                                \n",
      " |        _________________________|______________________________________________   \n",
      " |       4                                                                        | \n",
      " |    ___|______________                                                          |  \n",
      " |   |                  4                                                         | \n",
      " |   |         _________|__________                                               |  \n",
      " |   |        |                    3                                              | \n",
      " |   |        |               _____|______________________                        |  \n",
      " |   |        |              |                            4                       | \n",
      " |   |        |              |            ________________|_______                |  \n",
      " |   |        |              |           |                        2               | \n",
      " |   |        |              |           |                 _______|___            |  \n",
      " |   |        3              |           |                |           2           | \n",
      " |   |    ____|_____         |           |                |        ___|_____      |  \n",
      " |   |   |          4        |           3                |       2         |     | \n",
      " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
      " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
      " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
      " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
      "\n",
      "Transitions:\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "ex = next(examplereader(\"trees/dev.txt\"))\n",
    "print(TreePrettyPrinter(ex.tree))\n",
    "print(\"Transitions:\")\n",
    "print(ex.transitions)\n",
    "\n",
    "print(len(ex.transitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceBFe9fU4BI_"
   },
   "source": [
    "Note that the tree is **binary**. Every node has two children, except for pre-terminal nodes.\n",
    "\n",
    "A tree like this can be described by a sequence of **SHIFT (0)** and **REDUCE (1)** actions.\n",
    "\n",
    "To construct a tree, we can use the transitions as follows:\n",
    "- **reverse** the sentence (a list of tokens) and call this the **buffer**\n",
    "   - the first word is now on top (last in the list), and we would get it when calling pop() on the buffer\n",
    "- create an empty list and call it the **stack**\n",
    "- iterate through the transition sequence:\n",
    "  - if it says SHIFT(0), we pop a word from the buffer, and push it to the stack\n",
    "  - if it says REDUCE(1), we pop the **top two items** from the stack, and combine them (e.g. with a Tree-LSTM!), creating a new node that we push back on the stack\n",
    "  \n",
    "Convince yourself that going through the transition sequence above will result in the tree that you see.\n",
    "For example, we would start by putting the following words on the stack (by shifting 5 times, starting with `It`):\n",
    "\n",
    "```\n",
    "Top of the stack:\n",
    "-----------------\n",
    "film\n",
    "lovely\n",
    "a \n",
    "'s  \n",
    "It\n",
    "```\n",
    "Now we find a REDUCE in the transition sequence, so we get the top two words (film and lovely), and combine them, so our new stack becomes:\n",
    "```\n",
    "Top of the stack:\n",
    "-----------------\n",
    "lovely film\n",
    "a \n",
    "'s  \n",
    "It\n",
    "```\n",
    "\n",
    "We will use this approach when encoding sentences with our Tree-LSTM.\n",
    "Now, our sentence is a reversed list of word embeddings.\n",
    "When we shift, we move a word embedding to the stack.\n",
    "When we reduce, we apply the Tree-LSTM to the top two vectors, and the result is a single vector that we put back on the stack.\n",
    "After going through the whole transition sequence, we will have the root node on our stack! We can use that to classify the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDWKShm1AfmR"
   },
   "source": [
    "## Obtaining the transition sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO7VKWVpAbWj"
   },
   "source": [
    "\n",
    "So what goes on in the `transitions_from_treestring` function?\n",
    "\n",
    "The idea ([explained in this blog post](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)) is that, if we had a tree, we could traverse through the tree, and every time that we find a node containing only a word, we output a SHIFT.\n",
    "Every time **after** we have finished visiting the children of a node, we output a REDUCE.\n",
    "(What is this tree traversal called?)\n",
    "\n",
    "However, our `transitions_from_treestring` function operates directly on the string representation. It works as follows.\n",
    "\n",
    "We start with the representation:\n",
    "\n",
    "```\n",
    "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n",
    "```\n",
    "\n",
    "First we remove pre-terminal nodes (and add spaces before closing brackets):\n",
    "\n",
    "```\n",
    "(3 It (4 (4 's (4 (3 a (4 lovely film ) ) (3 with (4 (3 lovely performances ) (2 by (2 (2 Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Then we remove node labels:\n",
    "\n",
    "```\n",
    "( It ( ( 's ( ( a ( lovely film ) ) ( with ( ( lovely performances) ( by ( ( Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Then we remove opening brackets:\n",
    "\n",
    "```\n",
    "It 's a lovely film ) ) with lovely performances ) by Buy and ) Accorsi ) ) ) ) ) ) . ) )\n",
    "```\n",
    "\n",
    "Now we replace words by S (for SHIFT), and closing brackets by R (for REDUCE):\n",
    "\n",
    "```\n",
    "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
    "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 \n",
    "```\n",
    "\n",
    "Et voil. We just obtained the transition sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "1y069gM4_v64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
      "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1\n"
     ]
    }
   ],
   "source": [
    "# for comparison\n",
    "seq = ex.transitions\n",
    "s = \" \".join([\"S\" if t == 0 else \"R\" for t in seq])\n",
    "print(s)\n",
    "print(\" \".join(map(str, seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-qOuKbDAiBn"
   },
   "source": [
    "## Coding the Tree-LSTM\n",
    "\n",
    "The code below contains a Binary Tree-LSTM cell.\n",
    "It is used in the TreeLSTM class below it, which in turn is used in the TreeLSTMClassifier.\n",
    "The job of the TreeLSTM class is to encode a complete sentence and return the root node.\n",
    "The job of the TreeLSTMCell is to return a new state when provided with two children (a reduce action). By repeatedly calling the TreeLSTMCell, the TreeLSTM will encode a sentence. This can be done for multiple sentences at the same time.\n",
    "\n",
    "\n",
    "#### Exercise \n",
    "Check the `forward` function and complete the Tree-LSTM formulas.\n",
    "You can see that we defined a large linear layer for you, that projects the *concatenation* of the left and right child into the input gate, left forget gate, right forget gate, candidate, and output gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "J9b9mjMlN7Pb"
   },
   "outputs": [],
   "source": [
    "class TreeLSTMCell(nn.Module):\n",
    "  \"\"\"A Binary Tree LSTM cell\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "    super(TreeLSTMCell, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
    "    self.dropout_layer = nn.Dropout(p=0.25)\n",
    "\n",
    "    self.reset_parameters()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "      weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "  def forward(self, hx_l, hx_r, mask=None):\n",
    "    \"\"\"\n",
    "    hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
    "    hx_r is ((batch, hidden_size), (batch, hidden_size))    \n",
    "    \"\"\"\n",
    "    prev_h_l, prev_c_l = hx_l  # left child\n",
    "    prev_h_r, prev_c_r = hx_r  # right child\n",
    "\n",
    "    B = prev_h_l.size(0)\n",
    "\n",
    "    # we concatenate the left and right children\n",
    "    # you can also project from them separately and then sum\n",
    "    children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
    "    \n",
    "    # project the combined children into a 5D tensor for i,fl,fr,u,o\n",
    "    # this is done for speed, and you could also do it separately\n",
    "    proj = self.reduce_layer(children)  # shape: B x 5D\n",
    "\n",
    "    # each shape: B x D\n",
    "    i, f_l, f_r, u, o = torch.chunk(proj, 5, dim=-1)\n",
    "\n",
    "    # main Tree LSTM computation\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # You only need to complete the commented lines below.\n",
    "#     raise NotImplementedError(\"Implement this.\")\n",
    "\n",
    "    # The shape of each of these is [batch_size, hidden_size]\n",
    "    \n",
    "\n",
    "\n",
    "    i = torch.sigmoid(i)\n",
    "    f_l = torch.sigmoid(f_l)    \n",
    "    f_r = torch.sigmoid(f_r)\n",
    "    u = torch.tanh(u)    \n",
    "    o = torch.sigmoid(o)\n",
    "\n",
    "    # j = hidden\n",
    "    # k = memory\n",
    "    c = i * u + f_l * prev_c_l  + f_r * prev_c_r\n",
    "    h = o * torch.tanh(c)\n",
    "    \n",
    "    return h, c\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return \"{}({:d}, {:d})\".format(\n",
    "        self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj5dYSGh_643"
   },
   "source": [
    "## Explanation of the TreeLSTM class\n",
    "\n",
    "\n",
    "The code below contains the TreeLSTM class, which implements everything we need in order to encode a sentence from word embeddings. The calculations are the same as in the paper, implemented such that the class `TreeLSTMCell` above is as general as possible and only takes two children to reduce them into a parent. \n",
    "\n",
    "\n",
    "**Initialize $\\mathbf{h}$ and $\\mathbf{c}$ outside of the cell for the leaves**\n",
    "\n",
    "At the leaves of each tree the children nodes are **empty**, whereas in higher levels the nodes are binary tree nodes that *do* have a left and right child (but no input $x$). By initializing the leaf nodes outside of the cell class (`TreeLSTMCell`), we avoid if-else statements in the forward pass.\n",
    "\n",
    "The `TreeLSTM` class (among other things) pre-calculates an initial $h$ and $c$ for every word in the sentence. Since the initial left and right child are 0, the only calculations we need to do are based on $x$, and we can drop the forget gate calculation (`prev_c_l` and `prev_c_r` are zero). The calculations we do in order to initalize $h$ and $c$ are then:\n",
    "\n",
    "$$\n",
    "c_1 =  W^{(u)}x_1 \\\\\n",
    "o_1 = \\sigma (W^{(i)}x_1) \\\\\n",
    "h_1 = o_1 \\odot \\text{tanh}(c_1)$$\n",
    "*NB: note that these equations are chosen as initializations of $c$ and $h$, other initializations are possible and might work equally well.*\n",
    "\n",
    "**Sentence Representations**\n",
    "\n",
    "All our leaf nodes are now initialized, so we can start processing the sentence in its tree form. Each sentence is represented by a buffer (initially a list with a concatenation of $[h_1, c_1]$ for every word in the reversed sentence), a stack (initially an empty list) and a transition sequence. To encode our sentence, we construct the tree from its transition sequence as explained earlier. \n",
    "\n",
    "*A short example that constructs a tree:*\n",
    "\n",
    "We loop over the time dimension of the batched transition sequences (i.e. row by row), which contain values of 0's, 1's and 2's (representing SHIFT, REDUCE and padding respectively). If we have a batch of size 2 where the first example has a transition sequence given by [0, 0, 1, 0, 0, 0, 1] and the second by [0, 0, 1, 0, 0, 1], our transition batch will be given by the following two-dimensional numpy array:\n",
    "\n",
    "$$\n",
    "\\text{transitions} = \n",
    "\\begin{pmatrix}\n",
    "0 & 0\\\\ \n",
    "0 & 0\\\\ \n",
    "1 & 1\\\\ \n",
    "0 & 0\\\\ \n",
    "0 & 0\\\\ \n",
    "0 & 1\\\\ \n",
    "1 & 2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The inner loop (`for transition, buffer, stack in zip(t_batch, buffers, stacks)`) goes over each example in the batch and updates its buffer and stack. The nested loop for this example will then do roughy the following:\n",
    "\n",
    "```\n",
    "Time = 0:  t_batch = [0, 0], the inner loop performs 2 SHIFTs. \n",
    "\n",
    "Time = 1:  t_batch = [0, 0], \"..\"\n",
    "\n",
    "Time = 2:  t_batch = [1, 1], causing the inner loop to fill the list child_l and child_r for both examples in the batch. Now the statement if child_l will return True, triggering a REDUCE action to be performed by our Tree LSTM cell with a batch size of 2. \n",
    "\n",
    "Time = 3:  t_batch = [0, 0], \"..\".\n",
    "\n",
    "Time = 4:  t_batch = [0, 0], \"..\"\n",
    "\n",
    "Time = 5:  t_batch = [0, 1], one SHIFT will be done and another REDUCE action will be performed by our Tree LSTM, this time of batch size 1.  \n",
    "\n",
    "Time = 6:  t_batch = [1, 2], triggering another REDUCE action with batch size 1.\n",
    "```\n",
    "*NB: note that this was an artificial example for the purpose of demonstrating parts of the code, the transition sequences do not necessarily represent actual trees.*\n",
    "\n",
    "**Batching and Unbatching**\n",
    "\n",
    "Within the body of the outer loop over time, we use the functions for batching and unbatching. \n",
    "\n",
    "*Batching*\n",
    "\n",
    "Before passing two lists of children to the reduce layer (an instance of `TreeLSTMCell`), we batch the children as they are at this point a list of tensors of variable length based on how many REDUCE actions there are to perform at a certain time step across the batch (let's call the length `L`). To do an efficient forward pass we want to transform the list to a pair of tensors of shape `([L, D], [L, D])`, which the function `batch` achieves. \n",
    "\n",
    "*Unbatching*\n",
    "\n",
    "In the same line where we batched the children, we unbatch the output of the forward pass to become a list of states of length `L` again. We do this because we need to loop over each example's transition at the current time step and push the children that are reduced into a parent to the stack.\n",
    "\n",
    "*The batch and unbatch functions let us switch between the \"PyTorch world\" (Tensors) and the Python world (easy to manipulate lists).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "5PixvTd4AqsQ"
   },
   "outputs": [],
   "source": [
    "# Helper functions for batching and unbatching states\n",
    "# For speed we want to combine computations by batching, but \n",
    "# for processing logic we want to turn the output into lists again\n",
    "# to easily manipulate.\n",
    "\n",
    "def batch(states):\n",
    "  \"\"\"\n",
    "  Turns a list of states into a single tensor for fast processing. \n",
    "  This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
    "  return torch.cat(states, 0).chunk(2, 1)\n",
    "\n",
    "def unbatch(state):\n",
    "  \"\"\"\n",
    "  Turns a tensor back into a list of states.\n",
    "  First, (h, c) are merged into a single state.\n",
    "  Then the result is split into a list of sentences.\n",
    "  \"\"\"\n",
    "  return torch.split(torch.cat(state, 1), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CynltDasaLPt"
   },
   "source": [
    "Take some time to understand the class below, having read the explanation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "rQOqMXG4gX5G"
   },
   "outputs": [],
   "source": [
    "class TreeLSTM(nn.Module):\n",
    "  \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "    super(TreeLSTM, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "    self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
    "\n",
    "    # project word to initial c\n",
    "    self.proj_x = nn.Linear(input_size, hidden_size)\n",
    "    self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
    "    \n",
    "    self.buffers_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, x, transitions):\n",
    "    \"\"\"\n",
    "    WARNING: assuming x is reversed!\n",
    "    :param x: word embeddings [B, T, E]\n",
    "    :param transitions: [2T-1, B]\n",
    "    :return: root states\n",
    "    \"\"\"\n",
    "\n",
    "    B = x.size(0)  # batch size\n",
    "    T = x.size(1)  # time\n",
    "\n",
    "    # compute an initial c and h for each word\n",
    "    # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
    "    # We do not handle input x in the TreeLSTMCell itself.\n",
    "    buffers_c = self.proj_x(x)\n",
    "    buffers_h = buffers_c.tanh()\n",
    "    buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
    "    buffers_h = buffers_h_gate * buffers_h\n",
    "    \n",
    "    # concatenate h and c for each word\n",
    "    buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
    "\n",
    "    D = buffers.size(-1) // 2\n",
    "\n",
    "    # we turn buffers into a list of stacks (1 stack for each sentence)\n",
    "    # first we split buffers so that it is a list of sentences (length B)\n",
    "    # then we split each sentence to be a list of word vectors\n",
    "    buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
    "    buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
    "\n",
    "    # create B empty stacks\n",
    "    stacks = [[] for _ in buffers]\n",
    "\n",
    "    # t_batch holds 1 transition for each sentence\n",
    "    for t_batch in transitions:\n",
    "\n",
    "      child_l = []  # contains the left child for each sentence with reduce action\n",
    "      child_r = []  # contains the corresponding right child\n",
    "\n",
    "      # iterate over sentences in the batch\n",
    "      # each has a transition t, a buffer and a stack\n",
    "      for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
    "        if transition == SHIFT:\n",
    "          stack.append(buffer.pop())\n",
    "        elif transition == REDUCE:\n",
    "          assert len(stack) >= 2, \\\n",
    "            \"Stack too small! Should not happen with valid transition sequences\"\n",
    "          child_r.append(stack.pop())  # right child is on top\n",
    "          child_l.append(stack.pop())\n",
    "\n",
    "      # if there are sentences with reduce transition, perform them batched\n",
    "      if child_l:\n",
    "        reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
    "        for transition, stack in zip(t_batch, stacks):\n",
    "          if transition == REDUCE:\n",
    "            stack.append(next(reduced))\n",
    "\n",
    "    final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
    "    final = torch.cat(final, dim=0)  # tensor [B, D]\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4EzbVzqaXkw"
   },
   "source": [
    "Just like the LSTM before, we will need an extra class that does the classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "nLxpYRvtQKge"
   },
   "outputs": [],
   "source": [
    "class TreeLSTMClassifier(nn.Module):\n",
    "  \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "    super(TreeLSTMClassifier, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "    self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
    "    self.output_layer = nn.Sequential(     \n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    # x is a pair here of words and transitions; we unpack it here.\n",
    "    # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
    "    x, transitions = x\n",
    "    emb = self.embed(x)\n",
    "    \n",
    "    # we use the root/top state of the Tree LSTM to classify the sentence\n",
    "    root_states = self.treelstm(emb, transitions)\n",
    "\n",
    "    # we use the last hidden state to classify the sentence\n",
    "    logits = self.output_layer(root_states)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh9RbhGwaiLg"
   },
   "source": [
    "## Special `prepare` function for Tree-LSTM\n",
    "\n",
    "We need yet another `prepare` function. For our implementation, sentences need to be *reversed*. We will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "DiqH-_2xdm9H"
   },
   "outputs": [],
   "source": [
    "def prepare_treelstm_minibatch(mb, vocab, Shuffle = False):\n",
    "  \"\"\"\n",
    "  Returns sentences reversed (last word first)\n",
    "  Returns transitions together with the sentences.  \n",
    "  \"\"\"\n",
    "  batch_size = len(mb)\n",
    "  maxlen = max([len(ex.tokens) for ex in mb])\n",
    "    \n",
    "  # vocab returns 0 if the word is not there\n",
    "  # NOTE: reversed sequence!\n",
    "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
    "  \n",
    "  x = torch.LongTensor(x)\n",
    "  x = x.to(device)\n",
    "  \n",
    "  y = [ex.label for ex in mb]\n",
    "  y = torch.LongTensor(y)\n",
    "  y = y.to(device)\n",
    "  \n",
    "  maxlen_t = max([len(ex.transitions) for ex in mb])\n",
    "  transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
    "  transitions = np.array(transitions)\n",
    "  transitions = transitions.T  # time-major\n",
    "  \n",
    "  return (x, transitions), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMUsrlL9ayVe"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "IpOYUdg2D3v0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (treelstm): TreeLSTM(\n",
      "    (reduce): TreeLSTMCell(300, 150)\n",
      "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (buffers_dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
      "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
      "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
      "treelstm.proj_x.bias     [150]        requires_grad=True\n",
      "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
      "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
      "output_layer.1.weight    [5, 150]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6534905\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-7993db8e31ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m       batch_size=25, eval_batch_size=25)\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-85-7993db8e31ee>\u001b[0m in \u001b[0;36mdo_train\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m   return train_model(\n\u001b[0m\u001b[0;32m     20\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m       \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# later we will use B examples per update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-16b784492014>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# we use the root/top state of the Tree LSTM to classify the sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mroot_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtreelstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# we use the last hidden state to classify the sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-a4e6303c25d4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, transitions)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# then we split each sentence to be a list of word vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mbuffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Bx[T, 2D]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mbuffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# BxTx[2D]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# create B empty stacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-a4e6303c25d4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# then we split each sentence to be a list of word vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mbuffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Bx[T, 2D]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mbuffers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# BxTx[2D]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# create B empty stacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now let's train the Tree LSTM!\n",
    "\n",
    "tree_model = TreeLSTMClassifier(\n",
    "    len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "with torch.no_grad():\n",
    "  tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "  tree_model.embed.weight.requires_grad = False\n",
    "  \n",
    "def do_train(model):\n",
    "  \n",
    "  print(model)\n",
    "  print_parameters(model)\n",
    "\n",
    "  model = model.to(device)\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "  \n",
    "  return train_model(\n",
    "      model, optimizer, num_iterations=30000, \n",
    "      print_every=250, eval_every=250,\n",
    "      prep_fn=prepare_treelstm_minibatch,\n",
    "      eval_fn=evaluate,\n",
    "      batch_fn=get_minibatch,\n",
    "      batch_size=25, eval_batch_size=25)\n",
    "  \n",
    "results = do_train(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHcHHaLtguUg"
   },
   "outputs": [],
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7QZZH86eHqu"
   },
   "source": [
    "# Further experiments and report\n",
    "\n",
    "For your report, you are expected to answer research questions by doing further experiments.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "Make sure you cover at least the following:\n",
    "\n",
    "- How important is word order for this task?\n",
    "- Does the tree structure help to get a better accuracy?\n",
    "- How does performance depend on the sentence length? Compare the various models. Is there a model that does better on longer sentences? If so, why?\n",
    "- Do you get better performance if you supervise the sentiment **at each node in the tree**? You can extract more training examples by treating every node in each tree as a separate tree. You will need to write a function that extracts all subtrees given a treestring. \n",
    "    - Warning: NLTK's Tree function seems to result in invalid trees in some cases, so be careful if you want to parse the string to a tree structure before extraction the phrases.\n",
    "\n",
    "**To obtain a full grade, you should conduct further investigations.** For example, you can also investigate the following:\n",
    "\n",
    "- When making a wrong prediction, can you figure out at what point in the tree (sentence) the model fails? You can make a prediction at each node to investigate.\n",
    "- How does N-ary Tree LSTM compare to the Child-Sum Tree LSTM? \n",
    "- How do the Tai et al. Tree LSTMs compare to Le & Zuidema's formulation?\n",
    "- Or... your own research question!\n",
    "\n",
    "In general:\n",
    "\n",
    "- ***When you report numbers, please report the mean accuracy across 3 (or more) runs with different random seed, together with the standard deviation.*** This is because the final performance may vary per random seed. \n",
    "More precisely, you should run each model with 3 different seeds, and for each of these 3 runs, evaluate the best model (according to the validation) on the test dataset. The validation dataset is used for finding the best model over iterations, but the accuracy you report should be on the test dataset.\n",
    "\n",
    "## Report instructions\n",
    "\n",
    "Your report needs to be written in LaTeX. You are required to use the ACL 2020 template which you can download from or edit directly on [Overleaf](https://www.overleaf.com/latex/templates/instructions-for-acl-2018-proceedings/xzmhqgnmkppc). Make sure your names and student numbers are visible at the top. (Tip: you need to uncomment `\\aclfinalcopy`).\n",
    "You can find some general tips about writing a research paper [here](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/), but note that you need to make your own judgment about what is appropriate for this project. \n",
    "\n",
    "We expect you to use the following structure:\n",
    "1. Introduction (~1 page) - describe the problem, your research questions and goals, a summary of your findings and contributions. Please cite related work (models, data set) as part of your introduction here, since this is a short paper.\n",
    "    - Introduce the task and the main goal\n",
    "    - Clear research questions\n",
    "    - Motivating the importance of the questions and explaining the expectations\n",
    "    - How are these addressed or not addressed in the literature\n",
    "    - What is your approach\n",
    "    - Short summary of your findings\n",
    "2. Background (~1/2-1 page) -\n",
    "cover the main techniques (\"building blocks\") used in your project (e.g. word embeddings, LSTM, Tree-LSTM) and intuitions behind them. Be accurate and concise.\n",
    "    - How each technique that you use works (don't just copy the formulas)\n",
    "    - The relation between the techniques\n",
    "3. Models (~1/2 page) - Cover the models that you used.\n",
    "    - The architecture of the final models (How do you use LSTM or Tree-LSTM for the sentiment classification task? What layers do you have, how do you do classification? What is your loss function?)\n",
    "4. Experiments (~1/2 page) - Describe your experimental setup. The information here should allow someone else to reproduce your experiments. Describe how you evaluate the models.\n",
    "    - Explain the task and the data\n",
    "    - Training the models (model, data, parameters and hyper parameters if the models, training algorithms, what supervision signals you use, etc.)\n",
    "    - Evaluation (e.g. metrics)\n",
    "5. Results and Analysis (~1 page). Go over the results and analyse your findings.\n",
    "    - Answer each of the research questions you raised in the introduction.\n",
    "    - Plots and figures highlighting interesting patterns\n",
    "    - What are the factors that make model A better than model B in task C? Investigate to prove their effect!\n",
    "6. Conclusion (~1/4 page). The main conclusions of your experiments.\n",
    "    - What have you learned from you experiments? How does it relate to what is already known in the literature?\n",
    "    - Were the results as expected? Any surprising results? Why?\n",
    "    - Based on what you learned what would you suggest doing next?\n",
    "\n",
    "\n",
    "General Tips:\n",
    "\n",
    "- Math notation  define each variable (either in running text, or in a pseudo-legenda after or before the equation).\n",
    "- Define technical terminology you need.\n",
    "- Avoid colloquial language  everything can be said in a scientific-sounding way.\n",
    "- Avoid lengthy sentences, stay to the point.\n",
    "- Do not spend space on \"obvious\" things.\n",
    "- Do not go over the page limit. (We will deduct points for that.)\n",
    "- The page limit is 4 pages excluding references and appendix. There is no strict limit to references and appendix. However, the report needs to remain fully self-contained: the appendix should only include content that is not necessary to understand your work. For example, preprocessing decisions, model parameters, pseudocode, sample system inputs/outputs, and other details that are necessary for the exact replication of your work can be put into the appendix. However, \n",
    "\n",
    "\n",
    "An ideal report:\n",
    "- Precise, scientific-sounding, technical, to the point \n",
    "  - Little general waffle/chit-chat\n",
    "- Not boring  because you dont explain obvious things too much\n",
    "- Efficient delivery of (only) the facts that we need to know to understand/reimplement\n",
    "- Results visually well-presented and described with the correct priority of importance of sub-results\n",
    "- Insightful analysis  speculation should connect to something interesting and not be too much; the reader learns something new\n",
    "- No typos, no colloquialisms  well-considered language\n",
    "- This normally means several re-draftings (re-orderings of information)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does normal LSTM handle word-order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to set seeds:\n",
    "\n",
    "def set_seed(seed=0):\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "\n",
    "  # When running on the CuDNN backend two further options must be set for reproducibility\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "uCINIXV1q1oe"
   },
   "outputs": [],
   "source": [
    "### Normal LSTM RUN\n",
    "def train_LSTM_Noshuffle():\n",
    "    lstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "    print(lstm_model)\n",
    "    print_parameters(lstm_model)\n",
    "\n",
    "    lstm_model = lstm_model.to(device)\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "    acc = train_model(\n",
    "        lstm_model, optimizer, num_iterations=10000, \n",
    "        print_every=250, eval_every=1000, OnlyTest=True)\n",
    "    return  acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shuffled Word-order\n",
    "def train_LSTM_Shuffle():\n",
    "    lstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "    print(lstm_model)\n",
    "    print_parameters(lstm_model)\n",
    "\n",
    "    lstm_model = lstm_model.to(device)\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "    acc = train_model(\n",
    "        lstm_model, optimizer, num_iterations=10000, \n",
    "        print_every=250, eval_every=1000, Shuffle = True, OnlyTest = True)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "rnn.W_ii                 [168, 300]   requires_grad=True\n",
      "rnn.W_hi                 [168, 168]   requires_grad=True\n",
      "rnn.B_ii                 [168]        requires_grad=True\n",
      "rnn.B_hi                 [168]        requires_grad=True\n",
      "rnn.W_if                 [168, 300]   requires_grad=True\n",
      "rnn.W_hf                 [168, 168]   requires_grad=True\n",
      "rnn.B_if                 [168]        requires_grad=True\n",
      "rnn.B_hf                 [168]        requires_grad=True\n",
      "rnn.W_ig                 [168, 300]   requires_grad=True\n",
      "rnn.W_hg                 [168, 168]   requires_grad=True\n",
      "rnn.B_ig                 [168]        requires_grad=True\n",
      "rnn.B_hg                 [168]        requires_grad=True\n",
      "rnn.W_io                 [168, 300]   requires_grad=True\n",
      "rnn.W_ho                 [168, 168]   requires_grad=True\n",
      "rnn.B_io                 [168]        requires_grad=True\n",
      "rnn.B_ho                 [168]        requires_grad=True\n",
      "output_layer.1.weight    [5, 168]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6534785\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-494a91d7b94c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSeeds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mNoShuffleAcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_LSTM_Noshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-37bc7c81c621>\u001b[0m in \u001b[0;36mtrain_LSTM_Noshuffle\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     acc = train_model(\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         print_every=250, eval_every=1000, OnlyTest=True)\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# later we will use B examples per update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-28b18ea1d35f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#sys.exit()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-c5d015dbd8f3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_, hx, mask)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_if\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_if\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_h\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_ig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_ig\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_io\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_io\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_h\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_ho\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_ho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[1;31m#print(i,f,g,o)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Seeds = [8,88,888]\n",
    "NoShuffleAcc= []\n",
    "for seed in Seeds:\n",
    "    set_seed(seed)\n",
    "    NoShuffleAcc.append(train_LSTM_Noshuffle())\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "rnn.W_ii                 [168, 300]   requires_grad=True\n",
      "rnn.W_hi                 [168, 168]   requires_grad=True\n",
      "rnn.B_ii                 [168]        requires_grad=True\n",
      "rnn.B_hi                 [168]        requires_grad=True\n",
      "rnn.W_if                 [168, 300]   requires_grad=True\n",
      "rnn.W_hf                 [168, 168]   requires_grad=True\n",
      "rnn.B_if                 [168]        requires_grad=True\n",
      "rnn.B_hf                 [168]        requires_grad=True\n",
      "rnn.W_ig                 [168, 300]   requires_grad=True\n",
      "rnn.W_hg                 [168, 168]   requires_grad=True\n",
      "rnn.B_ig                 [168]        requires_grad=True\n",
      "rnn.B_hg                 [168]        requires_grad=True\n",
      "rnn.W_io                 [168, 300]   requires_grad=True\n",
      "rnn.W_ho                 [168, 168]   requires_grad=True\n",
      "rnn.B_io                 [168]        requires_grad=True\n",
      "rnn.B_ho                 [168]        requires_grad=True\n",
      "output_layer.1.weight    [5, 168]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6534785\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-33d4539ffd44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSeeds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mShuffleAcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_LSTM_Shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-848953c86630>\u001b[0m in \u001b[0;36mtrain_LSTM_Shuffle\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     acc = train_model(\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         print_every=250, eval_every=1000, Shuffle = True, OnlyTest = True)\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Seeds = [8,88,888]\n",
    "ShuffleAcc= []\n",
    "for seed in Seeds:\n",
    "    set_seed(seed)\n",
    "    ShuffleAcc.append(train_LSTM_Shuffle())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(NoShuffleAcc), np.std(NoShuffleAcc))\n",
    "print(np.average(ShuffleAcc), np.std(ShuffleAcc))\n",
    "\n",
    "print(NoShuffleAcc)\n",
    "print(ShuffleAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT LSTM VS TREE LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TREE LSTM MINIBATCH\n",
    "# Now let's train the Tree LSTM!\n",
    "def tree_lstm(data = train_data):\n",
    "    tree_model = TreeLSTMClassifier(\n",
    "        len(v.w2i), 300, 150, len(t2i), v)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "      tree_model.embed.weight.requires_grad = False\n",
    "\n",
    "    def do_train(model):\n",
    "\n",
    "      print(model)\n",
    "      print_parameters(model)\n",
    "\n",
    "      model = model.to(device)\n",
    "\n",
    "      optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "      return train_model(\n",
    "          model, optimizer, num_iterations=10000, \n",
    "          print_every=250, eval_every=250,\n",
    "          prep_fn=prepare_treelstm_minibatch,\n",
    "          eval_fn=evaluate,\n",
    "          batch_fn=get_minibatch,\n",
    "          batch_size=25, eval_batch_size=25,\n",
    "          OnlyTest=True,\n",
    "          data = data)\n",
    "\n",
    "    return do_train(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM MINIBATCH\n",
    "def lstm_minibatch(data = train_data):\n",
    "    lstm_model = LSTMClassifier(\n",
    "        len(v.w2i), 300, 168, len(t2i), v)\n",
    "\n",
    "    # copy pre-trained vectors into embeddings table\n",
    "    with torch.no_grad():\n",
    "      lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "      lstm_model.embed.weight.requires_grad = False\n",
    "\n",
    "    print(lstm_model)\n",
    "    print_parameters(lstm_model)  \n",
    "\n",
    "    lstm_model = lstm_model.to(device)\n",
    "\n",
    "    batch_size = 25\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
    "\n",
    "    lstm_accuracies = train_model(\n",
    "        lstm_model, optimizer, num_iterations=10000, \n",
    "        print_every=250, eval_every=250,\n",
    "        batch_size=batch_size,\n",
    "        batch_fn=get_minibatch, \n",
    "        prep_fn=prepare_minibatch,\n",
    "        eval_fn=evaluate,\n",
    "        OnlyTest=True, \n",
    "        data = data)\n",
    "    \n",
    "    return lstm_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "rnn.W_ii                 [168, 300]   requires_grad=True\n",
      "rnn.W_hi                 [168, 168]   requires_grad=True\n",
      "rnn.B_ii                 [168]        requires_grad=True\n",
      "rnn.B_hi                 [168]        requires_grad=True\n",
      "rnn.W_if                 [168, 300]   requires_grad=True\n",
      "rnn.W_hf                 [168, 168]   requires_grad=True\n",
      "rnn.B_if                 [168]        requires_grad=True\n",
      "rnn.B_hf                 [168]        requires_grad=True\n",
      "rnn.W_ig                 [168, 300]   requires_grad=True\n",
      "rnn.W_hg                 [168, 168]   requires_grad=True\n",
      "rnn.B_ig                 [168]        requires_grad=True\n",
      "rnn.B_hg                 [168]        requires_grad=True\n",
      "rnn.W_io                 [168, 300]   requires_grad=True\n",
      "rnn.W_ho                 [168, 168]   requires_grad=True\n",
      "rnn.B_io                 [168]        requires_grad=True\n",
      "rnn.B_ho                 [168]        requires_grad=True\n",
      "output_layer.1.weight    [5, 168]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6534785\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-4a7726573f69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSeeds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mminibatch_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mminibatch_tree_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-8d47a753ad13>\u001b[0m in \u001b[0;36mlstm_minibatch\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     lstm_accuracies = train_model(\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Seeds = [8,88,888]\n",
    "minibatch_lstm = []\n",
    "minibatch_tree_lstm = []\n",
    "\n",
    "for seed in Seeds:\n",
    "    set_seed(seed)\n",
    "    minibatch_lstm.append(lstm_minibatch())\n",
    "    minibatch_tree_lstm.append(tree_lstm())\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.average(minibatch_lstm),np.std(minibatch_lstm))\n",
    "print( np.average(minibatch_tree_lstm), np.std(minibatch_tree_lstm))\n",
    "\n",
    "print(minibatch_lstm, minibatch_tree_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "rnn.W_ii                 [168, 300]   requires_grad=True\n",
      "rnn.W_hi                 [168, 168]   requires_grad=True\n",
      "rnn.B_ii                 [168]        requires_grad=True\n",
      "rnn.B_hi                 [168]        requires_grad=True\n",
      "rnn.W_if                 [168, 300]   requires_grad=True\n",
      "rnn.W_hf                 [168, 168]   requires_grad=True\n",
      "rnn.B_if                 [168]        requires_grad=True\n",
      "rnn.B_hf                 [168]        requires_grad=True\n",
      "rnn.W_ig                 [168, 300]   requires_grad=True\n",
      "rnn.W_hg                 [168, 168]   requires_grad=True\n",
      "rnn.B_ig                 [168]        requires_grad=True\n",
      "rnn.B_hg                 [168]        requires_grad=True\n",
      "rnn.W_io                 [168, 300]   requires_grad=True\n",
      "rnn.W_ho                 [168, 168]   requires_grad=True\n",
      "rnn.B_io                 [168]        requires_grad=True\n",
      "rnn.B_ho                 [168]        requires_grad=True\n",
      "output_layer.1.weight    [5, 168]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6534785\n",
      "\n",
      "Shuffling training data\n",
      "Shuffling training data\n",
      "Shuffling training data\n",
      "Shuffling training data\n",
      "Shuffling training data\n",
      "Iter 250: loss=361.3469, time=4.03s\n",
      "iter 250: dev acc=0.3833\n",
      "new highscore\n",
      "Shuffling training data\n",
      "Shuffling training data\n",
      "Shuffling training data\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-4286c0fb3e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSeeds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mminibatch_lstm_short\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshort_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mminibatch_lstm_long\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlong_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-8d47a753ad13>\u001b[0m in \u001b[0;36mlstm_minibatch\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     lstm_accuracies = train_model(\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# later we will use B examples per update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-28b18ea1d35f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#sys.exit()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-c5d015dbd8f3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_, hx, mask)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#print(torch.mean(self.W_ii))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_ii\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_ii\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mprev_h\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_if\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_if\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_h\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_ig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_ig\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB_hg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Seeds = [1, 29, 88]\n",
    "\n",
    "minibatch_lstm_short = []\n",
    "minibatch_lstm_long = []\n",
    "\n",
    "minibatch_tree_lstm_short = []\n",
    "minibatch_tree_lstm_long = []\n",
    "\n",
    "for seed in Seeds:\n",
    "    set_seed(seed)\n",
    "    minibatch_lstm_short.append(lstm_minibatch(short_train))\n",
    "    minibatch_lstm_long.append(lstm_minibatch(long_train))\n",
    "    \n",
    "    minibatch_tree_lstm_short.append(tree_lstm(short_train))\n",
    "    minibatch_tree_lstm_long.append(tree_lstm(long_train))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minibatch_lstm_long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-54d3055438e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_lstm_long\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_lstm_long\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_lstm_long\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_lstm_long\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_lstm_long\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_lstm_short\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_tree_lstm_short\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_tree_lstm_long\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minibatch_lstm_long' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.average(minibatch_lstm_long), np.std(minibatch_lstm_long))\n",
    "print(np.average(minibatch_lstm_short), np.std(minibatch_lstm_short))\n",
    "print(np.average(minibatch_tree_lstm_long), np.std(minibatch_tree_lstm_long))\n",
    "print(np.average(minibatch_tree_lstm_short), np.std(minibatch_tree_lstm_short))\n",
    "\n",
    "print(minibatch_lstm_long, minibatch_lstm_short, minibatch_tree_lstm_short, minibatch_tree_lstm_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-LSTM Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Word,POS-tag pairs as vocab\n",
    " - Alter LSTM model to train embeddings (change dimension)\n",
    " - Train\n",
    " - compare with different configurations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 21871\n",
      "Original Vocabulary size: 20727\n"
     ]
    }
   ],
   "source": [
    "## Vocab\n",
    "\n",
    "sentence_pos_tags = [pos_tag(sentence.tokens) for sentence in train_data]\n",
    "\n",
    "PosVocab = Vocabulary()\n",
    "for ex in sentence_pos_tags:\n",
    "    for token in ex:\n",
    "        PosVocab.count_token(token)\n",
    "\n",
    "PosVocab.build()\n",
    "print(\"Vocabulary size:\", len(PosVocab.w2i))\n",
    "print(\"Original Vocabulary size:\", len(v.w2i))\n",
    "\n",
    "#sentence_pos_tags = [pos_tag(sentence.tokens) for sentence in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18280\n",
      "Pretrained size: 20727\n",
      "Pos vocab size: 21871\n"
     ]
    }
   ],
   "source": [
    "NoPosv = Vocabulary()\n",
    "for data_set in (train_data,):\n",
    "  for ex in data_set:\n",
    "    for token in ex.tokens:\n",
    "      NoPosv.count_token(token)\n",
    "\n",
    "NoPosv.build()\n",
    "print(\"Vocabulary size:\", len(NoPosv.w2i))\n",
    "print(\"Pretrained size:\", len(v.w2i))\n",
    "print(\"Pos vocab size:\", len(PosVocab.w2i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_prepare_example(example, vocab, shuffle = False):\n",
    "    \"\"\"\n",
    "    Map tokens to their IDs for a single example\n",
    "    \"\"\"\n",
    "  \n",
    "      # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
    "    x = [vocab.w2i.get(t, 0) for t in example.tags]\n",
    "  \n",
    "    x = torch.LongTensor([x])\n",
    "    if shuffle:\n",
    "        x =  x[:,torch.randperm(x.size()[1])]\n",
    "    x = x.to(device)\n",
    "  \n",
    "    y = torch.LongTensor([example.label])\n",
    "    y = y.to(device)\n",
    "  \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_prepare_minibatch(mb, vocab, Shuffle = False):\n",
    "  \"\"\"\n",
    "  Minibatch is a list of examples.\n",
    "  This function converts words to IDs and returns\n",
    "  torch tensors to be used as input/targets.\n",
    "  \"\"\"\n",
    "  #print(mb)\n",
    "  batch_size = len(mb)\n",
    "  maxlen = max([len(ex.tokens) for ex in mb])\n",
    "  \n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tags], maxlen) for ex in mb]\n",
    "  \n",
    "  x = torch.LongTensor(x)\n",
    "  x = x.to(device)\n",
    "  \n",
    "  y = [ex.label for ex in mb]\n",
    "  y = torch.LongTensor(y)\n",
    "  y = y.to(device)\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(21871, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [21871, 300] requires_grad=True\n",
      "rnn.W_ii                 [168, 300]   requires_grad=True\n",
      "rnn.W_hi                 [168, 168]   requires_grad=True\n",
      "rnn.B_ii                 [168]        requires_grad=True\n",
      "rnn.B_hi                 [168]        requires_grad=True\n",
      "rnn.W_if                 [168, 300]   requires_grad=True\n",
      "rnn.W_hf                 [168, 168]   requires_grad=True\n",
      "rnn.B_if                 [168]        requires_grad=True\n",
      "rnn.B_hf                 [168]        requires_grad=True\n",
      "rnn.W_ig                 [168, 300]   requires_grad=True\n",
      "rnn.W_hg                 [168, 168]   requires_grad=True\n",
      "rnn.B_ig                 [168]        requires_grad=True\n",
      "rnn.B_hg                 [168]        requires_grad=True\n",
      "rnn.W_io                 [168, 300]   requires_grad=True\n",
      "rnn.W_ho                 [168, 168]   requires_grad=True\n",
      "rnn.B_io                 [168]        requires_grad=True\n",
      "rnn.B_ho                 [168]        requires_grad=True\n",
      "output_layer.1.weight    [5, 168]     requires_grad=True\n",
      "output_layer.1.bias      [5]          requires_grad=True\n",
      "\n",
      "Total number of parameters: 6877985\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-7df07983d66c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPosLstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m PosLstm_losses, PosLstm_accuracies = train_model(\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mPosLstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-80fd001eb57b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size, Shuffle, OnlyTest, data)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp1\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PosLstm_model = LSTMClassifier(\n",
    "    len(PosVocab.w2i), 300, 168, len(t2i), PosVocab)\n",
    "\n",
    "print(PosLstm_model)\n",
    "print_parameters(PosLstm_model)  \n",
    "  \n",
    "PosLstm_model = PosLstm_model.to(device)\n",
    "\n",
    "batch_size = 25\n",
    "optimizer = optim.Adam(PosLstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "PosLstm_losses, PosLstm_accuracies = train_model(\n",
    "    PosLstm_model, optimizer, num_iterations=10000, \n",
    "    print_every=250, eval_every=250,\n",
    "    batch_size=batch_size,\n",
    "    batch_fn=get_minibatch, \n",
    "    prep_fn=prepare_minibatch,\n",
    "    eval_fn=evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_poslstm():\n",
    "    PosLstm_model = LSTMClassifier(\n",
    "        len(PosVocab.w2i), 300, 168, len(t2i), PosVocab)\n",
    "\n",
    "    print(PosLstm_model)\n",
    "    print_parameters(PosLstm_model)  \n",
    "\n",
    "    PosLstm_model = PosLstm_model.to(device)\n",
    "\n",
    "    batch_size = 25\n",
    "    optimizer = optim.Adam(PosLstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "    acc = train_model(\n",
    "        PosLstm_model, optimizer, num_iterations=10000, \n",
    "        print_every=250, eval_every=250,\n",
    "        batch_size=batch_size,\n",
    "        batch_fn=get_minibatch, \n",
    "        prep_fn=pos_prepare_minibatch,\n",
    "        eval_fn=evaluate, OnlyTest=True)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm():\n",
    "    lstm_model = LSTMClassifier(\n",
    "    len(NoPosv.w2i), 300, 168, len(t2i), NoPosv)\n",
    "\n",
    "    print(lstm_model)\n",
    "    print_parameters(lstm_model)  \n",
    "\n",
    "    lstm_model = lstm_model.to(device)\n",
    "\n",
    "    batch_size = 25\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "\n",
    "    acc = train_model(\n",
    "        lstm_model, optimizer, num_iterations=10000, \n",
    "        print_every=250, eval_every=250,\n",
    "        batch_size=batch_size,\n",
    "        batch_fn=get_minibatch, \n",
    "        prep_fn=prepare_minibatch,\n",
    "        eval_fn=evaluate , OnlyTest=True)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_poslstm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seeds = [1, 29, 88]\n",
    "\n",
    "lstm_accs = []\n",
    "poslstm_accs = []\n",
    "for seed in Seeds:\n",
    "    set_seed(seed)\n",
    "    poslstm_accs.append(run_poslstm())\n",
    "    lstm_accs.append(run_lstm())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(lstm_accs), np.std(lstm_accs))\n",
    "print(np.average(poslstm_accs), np.std(poslstm_accs))\n",
    "\n",
    "print(lstm_accs)\n",
    "print(poslstm_accs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP1 2020 Practical 2 (student version)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
